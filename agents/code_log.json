[
  {
    "timestamp": "2025-07-21T18:24:38.118271",
    "iteration": 1,
    "initial_agent_response": {
      "feature_name": "topic_centroid_similarity_score",
      "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
      "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7)\n            if pd.notna(inflow_threshold):\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            print(f\"Could not process keyword '{keyword}': {e}\")\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
    },
    "correction_attempts_made": 10,
    "correction_history": [
      {
        "attempt": 1,
        "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7)\n            if pd.notna(inflow_threshold):\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            print(f\"Could not process keyword '{keyword}': {e}\")\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
        "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices]\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7)\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "topic_centroid_similarity_score",
          "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
          "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7)\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
        }
      },
      {
        "attempt": 2,
        "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7)\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
        "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices]\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "topic_centroid_similarity_score",
          "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
          "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
        }
      },
      {
        "attempt": 3,
        "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
        "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices]\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "topic_centroid_similarity_score",
          "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
          "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
        }
      },
      {
        "attempt": 4,
        "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
        "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "topic_centroid_similarity_score",
          "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
          "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
        }
      },
      {
        "attempt": 5,
        "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
        "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "topic_centroid_similarity_score",
          "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
          "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
        }
      },
      {
        "attempt": 6,
        "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
        "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "topic_centroid_similarity_score",
          "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
          "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
        }
      },
      {
        "attempt": 7,
        "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
        "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "topic_centroid_similarity_score",
          "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
          "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
        }
      },
      {
        "attempt": 8,
        "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
        "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "topic_centroid_similarity_score",
          "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
          "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
        }
      },
      {
        "attempt": 9,
        "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
        "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "topic_centroid_similarity_score",
          "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
          "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
        }
      },
      {
        "attempt": 10,
        "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
        "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "topic_centroid_similarity_score",
          "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
          "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
        }
      }
    ],
    "final_report": {
      "error": "Code failed after all correction attempts.",
      "final_traceback": "Traceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n",
      "correction_history": [
        {
          "attempt": 1,
          "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7)\n            if pd.notna(inflow_threshold):\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            print(f\"Could not process keyword '{keyword}': {e}\")\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
          "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices]\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7)\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
          "response_json": {
            "feature_name": "topic_centroid_similarity_score",
            "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
            "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7)\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
          }
        },
        {
          "attempt": 2,
          "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7)\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
          "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices]\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
          "response_json": {
            "feature_name": "topic_centroid_similarity_score",
            "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
            "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
          }
        },
        {
          "attempt": 3,
          "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
          "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices]\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
          "response_json": {
            "feature_name": "topic_centroid_similarity_score",
            "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
            "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
          }
        },
        {
          "attempt": 4,
          "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices]\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
          "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
          "response_json": {
            "feature_name": "topic_centroid_similarity_score",
            "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
            "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
          }
        },
        {
          "attempt": 5,
          "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
          "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
          "response_json": {
            "feature_name": "topic_centroid_similarity_score",
            "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
            "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
          }
        },
        {
          "attempt": 6,
          "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
          "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
          "response_json": {
            "feature_name": "topic_centroid_similarity_score",
            "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
            "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
          }
        },
        {
          "attempt": 7,
          "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
          "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
          "response_json": {
            "feature_name": "topic_centroid_similarity_score",
            "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
            "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
          }
        },
        {
          "attempt": 8,
          "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
          "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
          "response_json": {
            "feature_name": "topic_centroid_similarity_score",
            "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
            "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
          }
        },
        {
          "attempt": 9,
          "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
          "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
          "response_json": {
            "feature_name": "topic_centroid_similarity_score",
            "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
            "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
          }
        },
        {
          "attempt": 10,
          "prompt": "Your previous code attempt failed. Analyze the root cause and provide a corrected version.\n\n### \uc804\ub7b5 \ube0c\ub9ac\ud551 \ubc0f \ub370\uc774\ud130 \uba85\uc138\uc11c (`master_post_data.csv`)\n\n\ub2f9\uc2e0\uc740 \uc774\uc81c \ub2e8\uc21c\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc, '\uacbd\uc7c1\uc5d0\uc11c \uc2b9\ub9ac\ud558\uae30 \uc704\ud55c \uc804\ub7b5'\uc774 \ub2f4\uae34 \ud2b9\ubcc4 \ub370\uc774\ud130\uc14b\uc744 \ubd84\uc11d\ud558\uac8c \ub429\ub2c8\ub2e4. \ub2f9\uc2e0\uc758 \uc784\ubb34\ub294 \uc774 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc640 \ubaa9\uc801\uc744 \uc644\ubcbd\ud788 \uc774\ud574\ud558\uace0, '\uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc778'\uc744 \ucc3e\uc544\ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 1: \uc804\ub7b5\uc801 \ubaa9\ud45c (The \"Why\")\n\n#### 1. \ub370\uc774\ud130\uc758 \ud575\uc2ec \ubaa9\uc801\n\uc774 \ub370\uc774\ud130\uc14b\uc758 \uc720\uc77c\ud55c \ubaa9\ud45c\ub294 **\"\uc65c \ud2b9\uc815 \uc8fc\uc81c(\ud0a4\uc6cc\ub4dc)\uc5d0\uc11c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\ub294 \uacbd\uc7c1\uc0ac\ubcf4\ub2e4 \uc21c\uc704\uac00 \ub0ae\uc740\uac00?\"** \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 '\ub300\ud45c \uac80\uc0c9\uc5b4'(`representative_keyword`)\ub77c\ub294 \uc804\uc7a5(battlefield)\ub9c8\ub2e4, \uc6b0\ub9ac\uc758 \ud50c\ub808\uc774\uc5b4('ours')\uc640 \uac15\ub825\ud55c \uacbd\uc7c1\uc790('competitor')\ub97c \ud568\uaed8 \ubc30\uce58\ud588\uc2b5\ub2c8\ub2e4.\n\n#### 2. \ub450 \uadf8\ub8f9\uc758 \uc774\ud574: '\uc6b0\ub9ac' vs '\uacbd\uc7c1\uc790'\n\ub2f9\uc2e0\uc774 \ubd84\uc11d\ud560 \ubaa8\ub4e0 \ud589(row)\uc740 `source` \uceec\ub7fc\uc744 \ud1b5\ud574 \ub450 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uc774 \ub450 \uadf8\ub8f9\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \ubd84\uc11d\uc758 \uccab\uac78\uc74c\uc785\ub2c8\ub2e4.\n\n*   **`source` == 'ours' (\uc6b0\ub9ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ub0b4\ubd80 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uace0\uc720\ud55c **\uc22b\uc790 `post_id`**\ub85c \uc2dd\ubcc4\ub429\ub2c8\ub2e4.\n    *   **\uac15\uc810:** **\uc2e4\uc81c \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow`, `non_brand_average_ctr`)**\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc131\uacf5'\ud588\uace0 \uc5b4\ub5a4 \ud3ec\uc2a4\ud2b8\uac00 '\uc2e4\ud328'\ud588\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **`source` == 'competitor' (\uacbd\uc7c1\uc0ac \ud3ec\uc2a4\ud2b8):**\n    *   **\uc2dd\ubcc4\uc790:** \ud3ec\uc2a4\ud2b8\uc758 **URL(`post_url`)** \uc790\uccb4\uac00 \ud575\uc2ec \uc2dd\ubcc4\uc790\uc785\ub2c8\ub2e4.\n    *   **\uac15\uc810:** \uc774\ub4e4\uc740 \uc774\ubbf8 \ud2b9\uc815 '\ub300\ud45c \uac80\uc0c9\uc5b4' \uc804\uc7a5\uc5d0\uc11c **\uc0c1\uc704 \ub7ad\ud0b9\uc744 \ucc28\uc9c0\ud55c, \uc99d\uba85\ub41c \uac15\uc790\ub4e4**\uc785\ub2c8\ub2e4. \uadf8\ub4e4\uc758 \ud2b9\uc9d5\uc740 \uace7 '\uc131\uacf5 \uacf5\uc2dd'\uc758 \ub2e8\uc11c\uac00 \ub429\ub2c8\ub2e4.\n    *   **\ud55c\uacc4:** \uc678\ubd80 \ud3ec\uc2a4\ud2b8\uc774\ubbc0\ub85c, \ub2f9\uc5f0\ud788 \uc6b0\ub9ac\uc758 \ub0b4\ubd80 \uc131\uacfc \ub370\uc774\ud130(`non_brand_inflow` \ub4f1)\ub294 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\uac12\uc774 \ube44\uc5b4\uc788\uc74c).\n\n#### 3. \ub2f9\uc2e0\uc758 \uc9c4\uc9dc \uc784\ubb34: '\uc131\uacf5 \ud328\ud134' \ubc1c\uacac\n\ub2e8\uc21c\ud788 \uceec\ub7fc \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub294 \uac83\uc744 \ub118\uc5b4, \ub2e4\uc74c \uac00\uc124\uc744 \uc99d\uba85\ud560 \ud53c\ucc98\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n> **\"\uc131\uacfc\uac00 \uc88b\uc740 \ud3ec\uc2a4\ud2b8\ub4e4(\uacbd\uc7c1\uc0ac \uc804\uccb4 + \uc6b0\ub9ac \uc911 \uc0c1\uc704 \ud3ec\uc2a4\ud2b8)\uc740 A\ub77c\ub294 \uacf5\ud1b5\uc801\uc778 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uc131\uacfc\uac00 \ub098\uc05c \uc6b0\ub9ac \ud3ec\uc2a4\ud2b8\uc5d0\uc11c\ub294 A\ub77c\ub294 \ud2b9\uc9d5\uc774 \ubd80\uc871\ud558\ub2e4.\"**\n\uc774 \uac00\uc124\uc744 \ub370\uc774\ud130\ub85c \uc99d\uba85\ud558\ub294 \uac83\uc774 \ub2f9\uc2e0\uc758 \ucd5c\uc885 \ubaa9\ud45c\uc785\ub2c8\ub2e4.\n\n---\n\n### Part 2: \uc8fc\uc694 \uceec\ub7fc \uc0c1\uc138 \uba85\uc138 (The \"What\")\n\n*   **`source`**: `string`. \uc774 \ud589\uc758 \ub370\uc774\ud130 \ucd9c\ucc98. 'ours'(\uc6b0\ub9ac) \ub610\ub294 'competitor'(\uacbd\uc7c1\uc0ac) \uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n*   **`representative_keyword`**: `string`. \uac01 \ud3ec\uc2a4\ud2b8\uac00 \uc5b4\ub5a4 \uac80\uc0c9\uc5b4 \ud544\ub4dc\uc5d0\uc11c \uacbd\uc7c1\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 '\ub300\ud45c \uac80\uc0c9\uc5b4'.\n*   **`post_id`**: `int64`. 'ours' \ud3ec\uc2a4\ud2b8\uc758 \uace0\uc720 \uc22b\uc790 ID. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c.\n*   **`post_url`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc804\uccb4 URL. 'competitor' \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \uc2dd\ubcc4\uc790.\n*   **`post_title`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \uc81c\ubaa9.\n*   **`post_body`**: `string`. \ud3ec\uc2a4\ud2b8\uc758 \ud575\uc2ec \ud14d\uc2a4\ud2b8 \ubcf8\ubb38.\n    *   **[\ub9e4\uc6b0 \uc911\uc694!]** \uc774 \ud14d\uc2a4\ud2b8\ub294 \uc6d0\ubb38\uc5d0\uc11c \uc21c\uc218 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\ud55c \uac83\uc73c\ub85c, **\ubb38\ub2e8\uc744 \uad6c\ubd84\ud558\ub294 \uc904\ubc14\uafc8 \ubb38\uc790(\\n)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.** \ubaa8\ub4e0 \ub0b4\uc6a9\uc740 \ud558\ub098\uc758 \uc5f0\uc18d\ub41c \ubb38\uc790\uc5f4\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, **\ubb38\ub2e8 \ubd84\ud560\uc744 \uc804\uc81c\ub85c \ud558\ub294 \ud53c\ucc98\ub97c \uc0dd\uc131\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.**\n*   **`category_keywords` / `morpheme_words`**: `string`. \ud14d\uc2a4\ud2b8 \ubd84\uc11d\uc744 \ud1b5\ud574 \ucd94\ucd9c\ub41c \ud0a4\uc6cc\ub4dc \ubc0f \ud615\ud0dc\uc18c \ubaa9\ub85d.\n*   **\uac01\uc885 `_score` \ubc0f `_count` \uceec\ub7fc**: `float64` / `int64`. `readability_score`, `total_image_count` \ub4f1 \ud3ec\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \uc815\ub7c9\uc801 \ud2b9\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc0ac\uc804 \uacc4\uc0b0\ub41c \ud53c\ucc98.\n*   **`non_brand_inflow` / `non_brand_average_ctr`**: `float64`. 'ours' \ud3ec\uc2a4\ud2b8\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \uc131\uacfc \uc9c0\ud45c. 'competitor'\uc758 \uacbd\uc6b0 \ube44\uc5b4\uc788\uc74c(NaN).\n\n\uc774 \ube0c\ub9ac\ud551\uc744 \uc644\ubcbd\ud788 \uc219\uc9c0\ud558\uace0, \uc774\uc81c \ubd84\uc11d\uc744 \uc2dc\uc791\ud558\uc2ed\uc2dc\uc624.\n\n\n--- PREVIOUS HYPOTHESIS ---\nFor any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\n--- FAILED CODE ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df\n--- ERROR TRACEBACK ---\nTraceback (most recent call last):\n  File \"/Users/min/codes/medilawyer_sales/blog_automation/agents/main.py\", line 151, in execute_feature_code\n    [sys.executable, temp_script_path],\n     ^^^\nNameError: name 'sys' is not defined. Did you forget to import 'sys'\n\n--- NEW JSON WITH CORRECTED CODE ---",
          "response_raw": "```json\n{\n  \"feature_name\": \"topic_centroid_similarity_score\",\n  \"hypothesis\": \"For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nimport torch\\nimport sys\\nfrom sentence_transformers import SentenceTransformer, util\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        # Using a multilingual model as the language of the content is not specified.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the semantic similarity of each post to the \\\"ideal\\\" content for its topic.\\n\\n    The \\\"ideal\\\" content is defined as the semantic centroid of high-performing posts\\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\\n    \\\"\\\"\\\"\\n    # 4. ALWAYS check for empty DataFrames.\\n    if df.empty:\\n        df['topic_centroid_similarity_score'] = np.nan\\n        return df\\n\\n    model = get_model()\\n\\n    # Create a composite text field for more focused semantic analysis,\\n    # combining the title and pre-extracted morphemes.\\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\\n    \\n    # Initialize the new feature column\\n    df['topic_centroid_similarity_score'] = np.nan\\n\\n    # Process each representative_keyword group separately\\n    for keyword in df['representative_keyword'].unique():\\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\\n        \\n        # Isolate the data for the current keyword\\n        keyword_df = df.loc[keyword_df_indices].copy()\\n\\n        # Define the High-Performing Group for this keyword\\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\\n\\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\\n            # Ensure there's enough data to calculate a quantile meaningfully\\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n                if pd.notna(inflow_threshold):\\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\\n            else:\\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\\n\\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\\n\\n        # If no high-performers are found for this keyword, we cannot create a centroid.\\n        # The feature value for these posts will remain NaN, which is appropriate.\\n        if high_perf_group.empty:\\n            continue\\n\\n        # Generate embeddings for the high-performing group and calculate the centroid\\n        try:\\n            # 2. Use efficient batch processing\\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # The centroid represents the \\\"ideal\\\" semantic center of the topic\\n            centroid = torch.mean(high_perf_embeddings, dim=0)\\n\\n            # Calculate similarity of all posts in the keyword group to the centroid\\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\\n            \\n            # util.cos_sim returns a 2D tensor, so we flatten it\\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\\n\\n            # Assign the calculated similarities back to the main DataFrame using original indices\\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\\n\\n        except Exception as e:\\n            # In case of any unexpected errors during embedding/calculation, log and continue\\n            # Using sys.stderr to ensure logs are captured by the execution environment\\n            print(f\\\"Could not process keyword '{keyword}': {e}\\\", file=sys.stderr)\\n            continue\\n\\n    # Clean up the temporary column\\n    df = df.drop(columns=['semantic_text'])\\n    \\n    # 3. Return the full DataFrame with the new column.\\n    return df\"\n}\n```",
          "response_json": {
            "feature_name": "topic_centroid_similarity_score",
            "hypothesis": "For any given 'representative_keyword', the content of high-performing posts (all competitor posts and the top 30% of our own posts) shares a common semantic core. This feature calculates the 'semantic centroid' of this high-performance group and then measures how similar each individual post is to this ideal centroid. A higher score suggests the post's content is more aligned with what is already proven to rank well for that specific topic, and should therefore correlate with better performance.",
            "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the language of the content is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to the \"ideal\" content for its topic.\n\n    The \"ideal\" content is defined as the semantic centroid of high-performing posts\n    (all competitor posts + top 30% of our posts by inflow) for a given representative_keyword.\n    \"\"\"\n    # 4. ALWAYS check for empty DataFrames.\n    if df.empty:\n        df['topic_centroid_similarity_score'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Create a composite text field for more focused semantic analysis,\n    # combining the title and pre-extracted morphemes.\n    df['semantic_text'] = df['post_title'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n    \n    # Initialize the new feature column\n    df['topic_centroid_similarity_score'] = np.nan\n\n    # Process each representative_keyword group separately\n    for keyword in df['representative_keyword'].unique():\n        keyword_df_indices = df[df['representative_keyword'] == keyword].index\n        \n        # Isolate the data for the current keyword\n        keyword_df = df.loc[keyword_df_indices].copy()\n\n        # Define the High-Performing Group for this keyword\n        competitor_posts = keyword_df[keyword_df['source'] == 'competitor']\n        our_posts = keyword_df[keyword_df['source'] == 'ours']\n\n        high_performing_our_posts = pd.DataFrame() # Initialize empty\n        # Check if 'our_posts' exists and has valid inflow data to calculate a quantile\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Ensure there's enough data to calculate a quantile meaningfully\n            if len(our_posts['non_brand_inflow'].dropna()) > 1:\n                inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n                if pd.notna(inflow_threshold):\n                    high_performing_our_posts = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            else:\n                high_performing_our_posts = our_posts[our_posts['non_brand_inflow'].notna()]\n\n        high_perf_group = pd.concat([competitor_posts, high_performing_our_posts])\n\n        # If no high-performers are found for this keyword, we cannot create a centroid.\n        # The feature value for these posts will remain NaN, which is appropriate.\n        if high_perf_group.empty:\n            continue\n\n        # Generate embeddings for the high-performing group and calculate the centroid\n        try:\n            # 2. Use efficient batch processing\n            high_perf_texts = high_perf_group['semantic_text'].tolist()\n            high_perf_embeddings = model.encode(high_perf_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # The centroid represents the \"ideal\" semantic center of the topic\n            centroid = torch.mean(high_perf_embeddings, dim=0)\n\n            # Calculate similarity of all posts in the keyword group to the centroid\n            all_keyword_texts = keyword_df['semantic_text'].tolist()\n            all_keyword_embeddings = model.encode(all_keyword_texts, convert_to_tensor=True, show_progress_bar=False)\n            \n            # util.cos_sim returns a 2D tensor, so we flatten it\n            similarities = util.cos_sim(all_keyword_embeddings, centroid).flatten()\n\n            # Assign the calculated similarities back to the main DataFrame using original indices\n            df.loc[keyword_df_indices, 'topic_centroid_similarity_score'] = similarities.cpu().numpy()\n\n        except Exception as e:\n            # In case of any unexpected errors during embedding/calculation, log and continue\n            # Using sys.stderr to ensure logs are captured by the execution environment\n            print(f\"Could not process keyword '{keyword}': {e}\", file=sys.stderr)\n            continue\n\n    # Clean up the temporary column\n    df = df.drop(columns=['semantic_text'])\n    \n    # 3. Return the full DataFrame with the new column.\n    return df"
          }
        }
      ]
    }
  }
]