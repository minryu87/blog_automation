import os
import sys
import json
import logging
import re
import signal
from dotenv import load_dotenv
import pandas as pd

from scipy.stats import pearsonr
import numpy as np
from datetime import datetime
import traceback
import subprocess

# --- Configure logging ---
logging.basicConfig(level=logging.WARNING)
logging.getLogger("google_genai").setLevel(logging.ERROR)
logging.getLogger("httpx").setLevel(logging.WARNING)

# --- Local Imports ---
from . import config
from .team import feature_engineer_agent, failure_analysis_agent
from .tools import HistoryTool, HumanFeedbackTool, LogFileTool, AGENTS_DIR

# --- 0. Load Environment Variables ---
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '..', '.env'))

# --- 1. Meta-Orchestrator Configuration ---
if not os.getenv("GEMINI_API_KEY"):
    raise ValueError("GEMINI_API_KEY not found in environment variables.")

# --- Constants and Configuration ---
# Load settings from the config file
GOAL = config.GOAL
MAX_ITERATIONS_PER_TASK = config.MAX_ITERATIONS_PER_TASK
MAX_CORRECTION_ATTEMPTS = config.MAX_CORRECTION_ATTEMPTS
EXPERIMENT_PIPELINE = config.EXPERIMENT_PIPELINE
TASK_FILE_NAMES = config.TASK_FILE_NAMES
BASE_DATASET_PATH = config.BASE_DATASET_PATH
FEEDBACK_FILE_PATH = config.FEEDBACK_FILE_PATH

# --- Graceful Shutdown ---
shutdown_flag = False
force_shutdown = False

def signal_handler(sig, frame):
    global shutdown_flag, force_shutdown
    if not shutdown_flag:
        print("\n[Orchestrator] ì¢…ë£Œ ì‹ í˜¸ ê°ì§€. í˜„ì¬ ì‘ì—… ì™„ë£Œ í›„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print("ì¦‰ì‹œ ê°•ì œ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ë‹¤ì‹œ ëˆ„ë¥´ì„¸ìš”.")
        shutdown_flag = True
    else:
        print("\n[Orchestrator] ê°•ì œ ì¢…ë£Œ ì‹ í˜¸ ê°ì§€. ì¦‰ì‹œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        force_shutdown = True
        sys.exit(1)

signal.signal(signal.SIGINT, signal_handler)

def extract_json_from_string(text: str) -> dict | None:
    """ë¬¸ìì—´ì—ì„œ ì²« ë²ˆì§¸ JSON ê°ì²´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not isinstance(text, str):
        return None
    match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
    if match:
        json_str = match.group(1).strip()
    else:
        start_index = text.find('{')
        end_index = text.rfind('}')
        if start_index != -1 and end_index != -1 and start_index < end_index:
            json_str = text[start_index:end_index+1].strip()
        else:
            return None
    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        return None

def _install_and_retry(error_output: str, script_path: str, input_csv: str, output_csv: str) -> tuple[pd.DataFrame | None, str | None]:
    """
    ModuleNotFoundErrorê°€ ë°œìƒí•  ê²½ìš° íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤.
    """
    match = re.search(r"ModuleNotFoundError: No module named '(\w+)'", error_output)
    if not match:
        return None, error_output

    library_name = match.group(1)
    print(f"[Self-Healing] ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°œê²¬: '{library_name}'. ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
    
    try:
        subprocess.run(
            [sys.executable, "-m", "pip", "install", library_name],
            capture_output=True, text=True, check=True, encoding='utf-8'
        )
        print(f"[Self-Healing] '{library_name}' ì„±ê³µì ìœ¼ë¡œ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤...")

        retry_result = subprocess.run(
            [sys.executable, script_path],
            capture_output=True, text=True, encoding='utf-8', check=False
        )
        
        if retry_result.returncode != 0:
            return None, retry_result.stderr

        if os.path.exists(output_csv):
            return pd.read_csv(output_csv), None
        else:
            return None, "ì¬ì‹œë„ í›„ì—ë„ ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    except Exception as e:
        return None, f"'{library_name}' ì„¤ì¹˜ ë˜ëŠ” ì¬ì‹œë„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def execute_feature_code(feature_code: str, base_df: pd.DataFrame) -> tuple[pd.DataFrame | None, str | None]:
    """
    Saves and executes the generated python code in an isolated subprocess.
    """
    temp_script_path = os.path.join(AGENTS_DIR, '_temp_feature_generator.py')
    temp_input_csv_path = os.path.join(AGENTS_DIR, '_temp_input.csv')
    temp_output_csv_path = os.path.join(AGENTS_DIR, '_temp_output.csv')

    script_content = f"""
import pandas as pd
import numpy as np
import sys
import os
from sentence_transformers import SentenceTransformer, util
import torch
import re

# --- Generated Feature Code ---
{feature_code}
# --- Execution Logic ---
def run():
    input_path = '{temp_input_csv_path}'
    output_path = '{temp_output_csv_path}'
    try:
        df = pd.read_csv(input_path)
        func_name = next((name for name in globals() if name.startswith('generate_feature')), None)
        if not func_name:
            raise ValueError("No function starting with 'generate_feature' found.")
        generate_func = globals()[func_name]
        modified_df = generate_func(df)
        modified_df.to_csv(output_path, index=False)
    except Exception as e:
        print(f"Error during script execution: {{e}}", file=sys.stderr)
        sys.exit(1)
if __name__ == '__main__':
    run()
"""
    try:
        with open(temp_script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        base_df.to_csv(temp_input_csv_path, index=False)
        result = subprocess.run(
            ["python", temp_script_path],
            capture_output=True, text=True, encoding='utf-8', check=False
        )
        if result.returncode != 0:
            if "ModuleNotFoundError" in result.stderr:
                return _install_and_retry(result.stderr, temp_script_path, temp_input_csv_path, temp_output_csv_path)
            else:
                return None, result.stderr
        if os.path.exists(temp_output_csv_path):
            modified_df = pd.read_csv(temp_output_csv_path)
            return modified_df, None
        else:
            return None, "Output file was not created by the script."
    except Exception as e:
        return None, traceback.format_exc()
    finally:
        for path in [temp_script_path, temp_input_csv_path, temp_output_csv_path]:
            if os.path.exists(path):
                os.remove(path)

def analyze_correlation(df: pd.DataFrame, feature_name: str, target_metric: str) -> dict:
    """ì§€ì •ëœ ë‹¨ì¼ íƒ€ê²Ÿ ì§€í‘œì— ëŒ€í•´ í”¼ì²˜ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  í•´ì„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if feature_name not in df.columns:
        raise ValueError(f"Feature '{feature_name}' not found in DataFrame.")
    
    df_for_analysis = df.copy()

    # ë°ì´í„° íƒ€ì…ì„ ìˆ«ìë¡œ ê°•ì œ ë³€í™˜í•©ë‹ˆë‹¤. ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê°’ì€ NaNìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    df_for_analysis[feature_name] = pd.to_numeric(df_for_analysis[feature_name], errors='coerce')
    df_for_analysis[target_metric] = pd.to_numeric(df_for_analysis[target_metric], errors='coerce')

    cleaned_df = df_for_analysis[[feature_name, target_metric]].replace([np.inf, -np.inf], np.nan).dropna()
    
    if len(cleaned_df) < 2 or cleaned_df[feature_name].nunique() <= 1:
        corr, p_value = None, None
    else:
        corr, p_value = pearsonr(cleaned_df[feature_name], cleaned_df[target_metric])

    return {"correlation": corr, "p_value": p_value, "interpretation": interpret_correlation(corr, p_value)}

def interpret_correlation(correlation, p_value):
    """ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼ì˜ í†µê³„ì  ìœ ì˜ë¯¸ì„±ê³¼ ì˜ë¯¸ë¥¼ í•´ì„í•©ë‹ˆë‹¤."""
    if correlation is None or p_value is None:
        return "ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì˜ˆ: í”¼ì²˜ì˜ ë¶„ì‚°ì´ 0)."
    alpha = 0.05
    is_significant = p_value < alpha
    abs_corr = abs(correlation)
    strength = "ë§¤ìš° ê°•í•œ" if abs_corr >= 0.7 else "ê°•í•œ" if abs_corr >= 0.5 else "ì¤‘ê°„ ì •ë„ì˜" if abs_corr >= 0.3 else "ì•½í•œ"
    direction = "ì–‘ì˜" if correlation > 0 else "ìŒì˜"
    conclusion = f"{strength} {direction} ìƒê´€ê´€ê³„({correlation:.4f})ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. "
    conclusion += f"ì´ ê²°ê³¼ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•©ë‹ˆë‹¤(p-value: {p_value:.4f})." if is_significant else f"í•˜ì§€ë§Œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤(p-value: {p_value:.4f})."
    return conclusion

def analyze_and_log_failure(error_traceback: str, code: str, hypothesis: str):
    """LLM ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¦¬ì  ì‹¤íŒ¨ì˜ ê·¼ë³¸ ì›ì¸ì„ ë¶„ì„í•˜ê³  ê¸°ë¡í•©ë‹ˆë‹¤."""
    print("[ì‹¤íŒ¨ ë¶„ì„] FailureAnalysisAgentë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¼ë³¸ ì›ì¸ ë¶„ì„ ì¤‘...")
    analysis_prompt = (
        f"ë‹¤ìŒ ì‹¤íŒ¨í•œ ì½”ë“œ ì‹¤í–‰ì„ ë¶„ì„í•˜ì—¬ ê·¼ë³¸ì ì¸ ë…¼ë¦¬ì  ì›ì¸ì„ ì°¾ì•„ë‚´ì„¸ìš”.\n\n"
        f"--- ê°€ì„¤ ---\n{hypothesis}\n\n"
        f"--- ì‹¤íŒ¨í•œ ì½”ë“œ ---\n{code}\n\n"
        f"--- ì—ëŸ¬ íŠ¸ë ˆì´ìŠ¤ë°± ---\n{error_traceback}\n\n"
    )
    try:
        response_raw = failure_analysis_agent.run(analysis_prompt).content
        response_json = extract_json_from_string(response_raw)
        failure_reason = response_json.get("failure_reason", "ì•Œ ìˆ˜ ì—†ëŠ” ë…¼ë¦¬ì  ì˜¤ë¥˜")
    except Exception as e:
        failure_reason = f"ë¶„ì„ ì—ì´ì „íŠ¸ ìì²´ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}"
    
    # LogFileToolì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  ì§ì ‘ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    log_path = os.path.join(AGENTS_DIR, 'logical_failure_history.log')
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(f"[{datetime.now().isoformat()}] {failure_reason}\n")

    print(f"[ì‹¤íŒ¨ ë¶„ì„] ë…¼ë¦¬ì  ì‹¤íŒ¨ ê¸°ë¡ ì™„ë£Œ: {failure_reason}")

def get_task_feedback(task_number: int) -> str:
    """feedback.mdì—ì„œ íŠ¹ì • ì‘ì—… ë²ˆí˜¸ì— ëŒ€í•œ ì§€ì¹¨ê³¼ ê·¸ ìƒìœ„ PART ì„¤ëª…ì„ í•¨ê»˜ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        with open(FEEDBACK_FILE_PATH, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        return f"ì˜¤ë¥˜: í”¼ë“œë°± íŒŒì¼({FEEDBACK_FILE_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ì „ì²´ ë‚´ìš©ì„ PARTë³„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    parts = re.split(r'(?=## \[PART)', content)
    
    for part_content in parts:
        if not part_content.strip():
            continue
            
        # í˜„ì¬ PARTì— ì›í•˜ëŠ” ì‘ì—…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        task_pattern = re.compile(rf"### \[ì‘ì—… {task_number}\]:", re.IGNORECASE)
        if task_pattern.search(part_content):
            # ì‘ì—…ì´ í¬í•¨ëœ ê²½ìš°, í•´ë‹¹ PARTì˜ ì„¤ëª…ê³¼ íŠ¹ì • TASKì˜ ì„¤ëª…ì„ ì¡°í•©í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
            
            # PART ë¸”ë¡ ì „ì²´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            part_match = re.search(r"## \[PART.*?\]:.*?(?=\n## \[PART|\Z)", part_content, re.DOTALL | re.IGNORECASE)
            part_block = part_match.group(0).strip() if part_match else ""

            # íŠ¹ì • TASK ë¸”ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            task_match = re.search(rf"(### \[ì‘ì—… {task_number}\]:.*?)(?=\n### \[ì‘ì—…|\Z)", part_content, re.DOTALL | re.IGNORECASE)
            task_block = task_match.group(1).strip() if task_match else ""
            
            # PART ì„¤ëª…ê³¼ TASK ì„¤ëª…ì„ í•©ì³ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
            if part_block and task_block:
                # PART ì„¤ëª…ì—ì„œ TASK ëª©ë¡ì€ ì œì™¸í•˜ê³  í—¤ë” ë¶€ë¶„ë§Œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ --- ê¸°ì¤€ìœ¼ë¡œ ìë¦…ë‹ˆë‹¤.
                part_header = part_block.split('---')[0].strip()
                return f"{part_header}\n\n{task_block}"

    return f"ì˜¤ë¥˜: í”¼ë“œë°± íŒŒì¼ì—ì„œ [ì‘ì—… {task_number}]ì— ëŒ€í•œ ì§€ì¹¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def get_task_metadata(task_feedback: str) -> dict | None:
    """ì‘ì—… í”¼ë“œë°±ì—ì„œ êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    # ë©”íƒ€ë°ì´í„°ëŠ” ì´ì œ ### [ì‘ì—…] ë¸”ë¡ ì•ˆì—ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.
    task_metadata_pattern = re.compile(r"### \[ì‘ì—… \d+\]:.*?\n<!--(.*?)-->", re.DOTALL)
    metadata_match = task_metadata_pattern.search(task_feedback)
    
    if not metadata_match: return None
    try:
        metadata_text = metadata_match.group(1)
        task_type = re.search(r"task_type:\s*(\w+)", metadata_text).group(1)
        target_metric = re.search(r"target_metric:\s*(\w+)", metadata_text).group(1)
        return {"task_type": task_type, "target_metric": target_metric}
    except (AttributeError, IndexError):
        return None

def run_meta_orchestrator():
    """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´ê´„í•˜ëŠ” ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì…ë‹ˆë‹¤."""
    print("--- meta_orchestrator ì‹œì‘ (íŒŒì´í”„ë¼ì¸ ë²„ì „) ---")
    try:
        master_df = pd.read_csv(BASE_DATASET_PATH)
        print(f"ê¸°ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: {len(master_df)} í–‰")
    except FileNotFoundError:
        print(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {BASE_DATASET_PATH} ì—ì„œ ê¸°ë³¸ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # TO-BE: ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ Në²ˆ ë°˜ë³µí•˜ëŠ” ìµœìƒìœ„ ì‚¬ì´í´ ë£¨í”„
    for cycle in range(1, MAX_ITERATIONS_PER_TASK + 1):
        if shutdown_flag: break
        print(f"\n{'='*30} ì „ì²´ ì‚¬ì´í´ #{cycle}/{MAX_ITERATIONS_PER_TASK} ì‹œì‘ {'='*30}")

        # ê¸°ì¡´ì˜ ì‘ì—… ë£¨í”„ê°€ ì´ì œ ë‚´ë¶€ ë£¨í”„ê°€ ë©ë‹ˆë‹¤.
        for task_number in EXPERIMENT_PIPELINE:
            if shutdown_flag: break
            print(f"\n--- [ì‚¬ì´í´ {cycle}] íŒŒì´í”„ë¼ì¸ ì‘ì—… #{task_number} ---")
            
            task_feedback = get_task_feedback(task_number)
            if task_feedback.startswith("ì˜¤ë¥˜:"):
                print(task_feedback); continue

            metadata = get_task_metadata(task_feedback)
            if not metadata:
                print(f"[ì˜¤ë¥˜] ì‘ì—… #{task_number}ì˜ í”¼ë“œë°±ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); continue
            
            task_type, target_metric = metadata["task_type"], metadata["target_metric"]
            print(f"ì‘ì—… ìœ í˜•: {task_type}, í•µì‹¬ ëª©í‘œ ì§€í‘œ: {target_metric}")

            task_name = TASK_FILE_NAMES.get(task_number, f"task_{task_number}")
            history_tool = HistoryTool(history_file=os.path.join(AGENTS_DIR, f"{task_name}_history.json"))
            feedback_tool = HumanFeedbackTool()
            code_log_file = os.path.join(AGENTS_DIR, f"{task_name}_code_log.json")
            
            # ê° ì‘ì—…ì— ëŒ€í•´ ìƒˆë¡œìš´ í”¼ì²˜ ìƒì„±ì„ 1íšŒ ì‹œë„í•©ë‹ˆë‹¤ (ë‚´ë¶€ ìê°€-ìˆ˜ì • í¬í•¨).
            last_error = None
            
            for attempt in range(1, MAX_CORRECTION_ATTEMPTS + 1):
                if shutdown_flag: break
                print(f"\n--- ì‘ì—… #{task_number} / ì‹œë„ {attempt}/{MAX_CORRECTION_ATTEMPTS} ---")

                print("[Orchestrator] í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
                past_experiments = history_tool.read_history()
                user_feedback = feedback_tool.read_feedback()
                
                df_for_prompt = master_df[master_df['source'] == 'ours'] if task_type == 'PART_1' else master_df
                
                prompt = (
                    f"{task_feedback}\n\n"
                    f"--- ì´ì „ ì‹¤í—˜ ìš”ì•½ ---\n"
                    f"{json.dumps(past_experiments, indent=2, ensure_ascii=False)}\n\n"
                    f"--- ì‚¬ìš©ì í”¼ë“œë°± ---\n"
                    f"{user_feedback}\n\n"
                    f"--- ë°ì´í„°ì…‹ì˜ ì¼ë¶€ ---\n"
                    f"{df_for_prompt.head(3).to_markdown(index=False)}\n\n"
                    "ì°¸ê³ : ì´ì „ ì‹¤í—˜ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡­ê³  ì°½ì˜ì ì¸ ê°€ì„¤ì„ ì„¸ì›Œì£¼ì„¸ìš”."
                )
                
                if last_error:
                    prompt += f"\n\nì´ì „ ì‹œë„ëŠ” ë‹¤ìŒ ì˜¤ë¥˜ë¡œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤:\n{last_error}\nì˜¤ë¥˜ì˜ ì›ì¸ì„ ë¶„ì„í•˜ê³  ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì—¬ ë‹¤ì‹œ ì œì•ˆí•´ì£¼ì„¸ìš”."
                
                print("[Orchestrator] AI ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...")
                agent_response_raw = feature_engineer_agent.run(prompt).content
                print(f"[Orchestrator] AI ì‘ë‹µ ìˆ˜ì‹ :\n{agent_response_raw}")

                print("[Orchestrator] AI ì‘ë‹µ ê²€ì¦ ì¤‘...")
                response_json = extract_json_from_string(agent_response_raw)

                if not response_json or not all(response_json.get(k) for k in ["feature_name", "hypothesis", "python_code"]):
                    last_error = f"ì—ì´ì „íŠ¸ê°€ ë¶ˆì™„ì „í•œ JSON(null ë˜ëŠ” ë¹ˆ ê°’ í¬í•¨)ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. Raw: {agent_response_raw}"
                    print(f"[ì˜¤ë¥˜] {last_error}")
                    with open(code_log_file, 'a', encoding='utf-8') as f:
                        json.dump({"timestamp": datetime.now().isoformat(), "attempt": attempt, "status": "incomplete_response", "error": last_error, "raw_response": agent_response_raw}, f, ensure_ascii=False, indent=2)
                    continue
                
                print("[Orchestrator] AI ì‘ë‹µ ê²€ì¦ ì™„ë£Œ.")
                feature_name, hypothesis, python_code = response_json["feature_name"], response_json["hypothesis"], response_json["python_code"]
                
                print(f"[Orchestrator] ìƒì„±ëœ í”¼ì²˜: '{feature_name}'")
                print(f"[Orchestrator] ê°€ì„¤: '{hypothesis}'")
                print("[Orchestrator] ì½”ë“œ ì‹¤í–‰ ì¤€ë¹„ ì¤‘...")
                
                df_for_analysis = master_df.copy()
                df_modified, execution_error = execute_feature_code(python_code, df_for_analysis)

                if execution_error:
                    last_error = f"ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨:\n{execution_error}"
                    print(f"[ì˜¤ë¥˜] {last_error}")
                    if "ModuleNotFoundError" not in execution_error:
                        analyze_and_log_failure(execution_error, python_code, hypothesis)
                    with open(code_log_file, 'a', encoding='utf-8') as f:
                        json.dump({"timestamp": datetime.now().isoformat(), "attempt": attempt, "status": "execution_error", "error": last_error, "code": python_code}, f, ensure_ascii=False, indent=2)
                    continue

                print(f"í”¼ì²˜ '{feature_name}'ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„± ë° ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.")
                if feature_name not in df_modified.columns:
                    last_error = f"ë…¼ë¦¬ì  ì˜¤ë¥˜: ì½”ë“œëŠ” ì‹¤í–‰ëì§€ë§Œ '{feature_name}' ì»¬ëŸ¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                    print(f"[ì˜¤ë¥˜] {last_error}")
                    # ì½”ë“œ ë¡œê·¸ ê¸°ë¡
                    with open(code_log_file, 'a', encoding='utf-8') as f:
                        json.dump({"timestamp": datetime.now().isoformat(), "attempt": attempt, "status": "logical_error", "error": last_error, "code": python_code}, f, ensure_ascii=False, indent=2)
                    continue

                # ì˜¬ë°”ë¥¸ ë°©ë²•: ìƒˆ í”¼ì²˜ ì»¬ëŸ¼ì„ master_dfì— ì§ì ‘ í• ë‹¹í•©ë‹ˆë‹¤.
                master_df[feature_name] = df_modified[feature_name]

                # ë…¼ë¦¬ì  ì˜¤ë¥˜ ê²€ì‚¬ 2: í”¼ì²˜ì˜ ë¶„ì‚°ì´ ìˆëŠ”ì§€ í™•ì¸ (ìƒê´€ê´€ê³„ ë¶„ì„ ì „ í•„ìˆ˜)
                if master_df[feature_name].nunique(dropna=False) <= 1:
                    last_error = (
                        f"ë…¼ë¦¬ì  ì˜¤ë¥˜: ìƒì„±ëœ í”¼ì²˜ '{feature_name}'ì˜ ê°’ì´ ëª¨ë‘ ë™ì¼í•˜ì—¬ "
                        f"ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  í–‰ì— ëŒ€í•´ ë‹¤ë¥¸ ê°’ì„ ìƒì„±í•˜ë„ë¡ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”."
                    )
                    print(f"[ì˜¤ë¥˜] {last_error}")
                    # ë…¼ë¦¬ì  ì˜¤ë¥˜ë¥¼ ë¶„ì„í•˜ê³  ê¸°ë¡í•©ë‹ˆë‹¤.
                    analyze_and_log_failure(last_error, python_code, hypothesis)
                    # ì½”ë“œ ë¡œê·¸ ê¸°ë¡
                    with open(code_log_file, 'a', encoding='utf-8') as f:
                        json.dump({"timestamp": datetime.now().isoformat(), "attempt": attempt, "status": "logical_error", "error": last_error, "code": python_code}, f, ensure_ascii=False, indent=2)
                    continue
                
                analysis_report = analyze_correlation(master_df, feature_name, target_metric)
                history_tool.add_event({"feature_name": feature_name, "hypothesis": hypothesis, "analysis": analysis_report})
                print(f"ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ: {analysis_report['interpretation']}")

                # ì½”ë“œ ë¡œê·¸ ê¸°ë¡ (ì„±ê³µ)
                with open(code_log_file, 'a', encoding='utf-8') as f:
                    json.dump({"timestamp": datetime.now().isoformat(), "attempt": attempt, "status": "success", "feature_name": feature_name, "hypothesis": hypothesis, "code": python_code, "analysis": analysis_report}, f, ensure_ascii=False, indent=2)
                
                if abs(analysis_report.get("correlation", 0) or 0) >= 0.5:
                     print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±! ìœ ì˜ë¯¸í•œ í”¼ì²˜ '{feature_name}'ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
                
                # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ì´ ì‘ì—…ì˜ ìê°€-ìˆ˜ì • ë£¨í”„ë¥¼ íƒˆì¶œí•˜ê³  ë‹¤ìŒ ì‘ì—…ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
                break 
            
            else: # for...else êµ¬ë¬¸: break ì—†ì´ ë£¨í”„ê°€ ëë‚˜ë©´ (ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ì‹œ) ì‹¤í–‰
                print(f"\n[ìµœì¢… ì‹¤íŒ¨] ì‘ì—… #{task_number}ì´(ê°€) ìµœëŒ€ ì‹œë„ íšŸìˆ˜({MAX_CORRECTION_ATTEMPTS}) ë‚´ì— ì„±ê³µí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                history_tool.add_event({"status": "failed", "reason": f"Max correction attempts reached ({MAX_CORRECTION_ATTEMPTS}).", "last_error": last_error})
    
    if shutdown_flag:
        print("\n[Orchestrator] ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"\n{'='*30} ëª¨ë“  ì‚¬ì´í´ ì™„ë£Œ {'='*30}")


def main():
    run_meta_orchestrator()

if __name__ == "__main__":
    main() 