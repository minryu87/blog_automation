import os
import json
import re
import time
import random
from datetime import datetime
import pandas as pd
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import urllib.parse
from dotenv import load_dotenv
from urllib.parse import urlparse, parse_qs

# --- Helper Functions ---

def get_html_with_cookie(url, cookie):
    """ì§€ì •ëœ URLì— ì¿ í‚¤ë¥¼ í¬í•¨í•˜ì—¬ GET ìš”ì²­ì„ ë³´ë‚´ê³  HTMLì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
        'Cookie': cookie
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"URL ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")
        return None

def get_json_with_cookie(url, cookie):
    """ì§€ì •ëœ URLì— ì¿ í‚¤ë¥¼ í¬í•¨í•˜ì—¬ GET ìš”ì²­ì„ ë³´ë‚´ê³  JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤. (Referer/User-Agent ì—†ìŒ)"""
    headers = {
        'Cookie': cookie
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"JSON ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")
        return None
    except json.JSONDecodeError:
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜ ({url}). ì‘ë‹µ ë‚´ìš©: {response.text}")
        return None

def extract_article_id_from_url(url):
    """URLì—ì„œ articleIdë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.search(r'/(\d+)', url.split('?')[0])
    return match.group(1) if match else None

# --- Main Logic Steps ---

def step1_initial_crawl(queries_df, cookie, output_file):
    """1ë‹¨ê³„: ê²€ìƒ‰ì–´ ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸° ê²Œì‹œê¸€ ì •ë³´ë¥¼ íŒŒì‹±í•˜ì—¬ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    print("--- 1ë‹¨ê³„: ê¸°ë³¸ ê²Œì‹œê¸€ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘ ---")
    all_results = {}
    for query in tqdm(queries_df['query'], desc="1ë‹¨ê³„: ê²€ìƒ‰ì–´ ì²˜ë¦¬ ì¤‘"):
        encoded_query = urllib.parse.quote(query)
        search_url = f"https://search.naver.com/search.naver?ssc=tab.cafe.all&query={encoded_query}&sm=tab_opt&nso=so:dd,p:3m"
        
        html_content = get_html_with_cookie(search_url, cookie)
        if not html_content:
            all_results[query] = []
            continue

        soup = BeautifulSoup(html_content, 'lxml')
        articles_data = []
        for li in soup.select('li.bx._bx'):
            title_element = li.select_one('a.title_link')
            link = title_element['href'] if title_element else "ë§í¬ ì—†ìŒ"
            article_id = extract_article_id_from_url(link)
            if not article_id:
                continue

            # 'art' í† í° ê°’ì„ URLì—ì„œ ì¶”ì¶œí•©ë‹ˆë‹¤.
            parsed_url = urlparse(link)
            query_params = parse_qs(parsed_url.query)
            art_token = query_params.get('art', [None])[0]

            # ëŒ“ê¸€ ë¯¸ë¦¬ë³´ê¸° ì˜ì—­ì˜ ëª¨ë“  ëŒ“ê¸€ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
            comment_elements = li.select('div.txt_area')
            comment_previews = [c.text.strip().replace('\n', ' ').replace('\t', ' ') for c in comment_elements]

            articles_data.append({
                'query': query,
                'cafe_name': (li.select_one('a.name').text.strip() if li.select_one('a.name') else ""),
                'post_date': (li.select_one('span.sub').text.strip() if li.select_one('span.sub') else ""),
                'title': (title_element.text.strip() if title_element else ""),
                'preview': (li.select_one('a.dsc_link').text.strip() if li.select_one('a.dsc_link') else ""),
                'comment_preview': comment_previews,
                'link': link,
                'article_id': article_id,
                'art_token': art_token
            })
        all_results[query] = articles_data
        print(f"\n'{query}' ê²€ìƒ‰ ê²°ê³¼: {len(articles_data)}ê°œ ê²Œì‹œê¸€ ë°œê²¬.")
        time.sleep(random.uniform(0.7, 1.5))
        
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=4)
    print(f"--- 1ë‹¨ê³„ ì™„ë£Œ: '{output_file}' íŒŒì¼ ì €ì¥ ---")
    return all_results

def step2_get_club_ids(crawled_data, cookie, output_file):
    """2ë‹¨ê³„: ê° ê²Œì‹œê¸€ ë§í¬ì— ì ‘ì†í•˜ì—¬ club_idë¥¼ ì¶”ì¶œí•˜ê³  JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. (ì¹´í˜ë³„ 1íšŒ ì¡°íšŒ ìµœì í™”)"""
    print("\n--- 2ë‹¨ê³„: Club ID ì¶”ì¶œ ì‹œì‘ (ìµœì í™”) ---")
    
    all_articles = [article for articles in crawled_data.values() for article in articles]
    cafe_club_ids = {} # ì¹´í˜ URLë³„ë¡œ club_idë¥¼ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬

    for article in tqdm(all_articles, desc="2ë‹¨ê³„: Club ID ì¶”ì¶œ ì¤‘"):
        article_url = article.get('link')
        if not article_url:
            article['club_id'] = None
            continue
        
        # ì¹´í˜ì˜ base URL ì¶”ì¶œ (e.g., https://cafe.naver.com/dongtantwomom)
        cafe_base_url_match = re.match(r"(https://cafe\.naver\.com/[^/]+)", article_url)
        if not cafe_base_url_match:
            article['club_id'] = None
            continue
        cafe_base_url = cafe_base_url_match.group(1)

        # ì´ë¯¸ í•´ë‹¹ ì¹´í˜ì˜ club_idë¥¼ ì°¾ì•˜ë‹¤ë©´, ì €ì¥ëœ ê°’ì„ ì‚¬ìš©
        if cafe_base_url in cafe_club_ids:
            article['club_id'] = cafe_club_ids[cafe_base_url]
            continue
        
        # ì²˜ìŒ ë³´ëŠ” ì¹´í˜ë¼ë©´, ë„¤íŠ¸ì›Œí¬ ìš”ì²­ìœ¼ë¡œ club_id ì¶”ì¶œ
        time.sleep(random.uniform(1.0, 2.0))
        article_html = get_html_with_cookie(article_url, cookie)

        if not article_html:
            club_id = None
        else:
            club_id_match = re.search(r'g_sClubId\s*=\s*"(\d+)"', article_html)
            club_id = club_id_match.group(1) if club_id_match else None
        
        # ì°¾ì€ club_idë¥¼ í˜„ì¬ ê²Œì‹œê¸€ê³¼ ë”•ì…”ë„ˆë¦¬ì— ëª¨ë‘ ì €ì¥
        article['club_id'] = club_id
        cafe_club_ids[cafe_base_url] = club_id


    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(crawled_data, f, ensure_ascii=False, indent=4)
    print(f"--- 2ë‹¨ê³„ ì™„ë£Œ: '{output_file}' íŒŒì¼ ì €ì¥ ---")
    return crawled_data

def step3_enrich_details(data_with_club_ids, cookie, output_file):
    """3ë‹¨ê³„: ë‘ ê°€ì§€ ë‹¤ë¥¸ JSON êµ¬ì¡°ì— ëª¨ë‘ ëŒ€ì‘í•˜ì—¬ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    print("\n--- 3ë‹¨ê³„: ìƒì„¸ ì •ë³´ (ì¡°íšŒìˆ˜, ëŒ“ê¸€) ìˆ˜ì§‘ ì‹œì‘ ---")
    
    all_articles = [article for articles in data_with_club_ids.values() for article in articles]
    
    for article in tqdm(all_articles, desc="3ë‹¨ê³„: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘"):
        club_id = article.get('club_id')
        article_id = article.get('article_id')
        art_token = article.get('art_token')

        if not club_id or not article_id or not art_token:
            article['views'] = 'ì¶”ì¶œ ì‹¤íŒ¨ (ID ë˜ëŠ” í† í° ì—†ìŒ)'
            article['comments'] = []
            continue
        
        # v3 APIì™€ art í† í°ì„ ì‚¬ìš©í•˜ëŠ” ìƒˆë¡œìš´ URL êµ¬ì¡°
        api_url = f"https://apis.naver.com/cafe-web/cafe-articleapi/v3/cafes/{club_id}/articles/{article_id}?query=&art={art_token}&useCafeId=true&requestFrom=A"
        headers = {'Cookie': cookie}
        
        try:
            time.sleep(random.uniform(0.7, 1.5))
            response = requests.get(api_url, headers=headers, timeout=15)
            response.raise_for_status()
            details = response.json()

            if 'result' not in details:
                raise ValueError("API ì‘ë‹µì— 'result' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            result_data = details['result']
            
            # ë‘ ì¢…ë¥˜ì˜ JSON êµ¬ì¡°ì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ ë¶„ê¸° ì²˜ë¦¬
            if 'article' in result_data:
                # [êµ¬ì¡° 1] 'article' í‚¤ê°€ ìˆëŠ” ê²½ìš° (ì •ìƒ ì‘ë‹µ)
                article_data = result_data.get('article', {})
                article['views'] = article_data.get('readCount', 0)
                comments_data = result_data.get('comments', {}).get('items', [])
            else:
                # [êµ¬ì¡° 2] 'article' í‚¤ê°€ ì—†ëŠ” ê²½ìš° (ì œí•œëœ ì •ë³´ ì‘ë‹µ)
                article['views'] = result_data.get('readCount', 0)
                comments_data = [] # ì´ êµ¬ì¡°ì—ì„œëŠ” ëŒ“ê¸€ ëª©ë¡ì´ ì—†ìŒ

            new_comments = []
            for comment in comments_data:
                timestamp = comment.get('updateDate', 0) / 1000
                new_comments.append({
                    'date': datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S'),
                    'content': comment.get('content', '').strip()
                })
            article['comments'] = new_comments

        except requests.exceptions.RequestException as e:
            print(f"\n[API ìš”ì²­ ì‹¤íŒ¨] {article.get('title', '')}: {e}")
            article['views'] = 'API ìš”ì²­ ì‹¤íŒ¨'
            article['comments'] = []
        except (json.JSONDecodeError, ValueError) as e:
            print(f"\n[JSON ì²˜ë¦¬ ì‹¤íŒ¨] {article.get('title', '')}: {e}")
            article['views'] = 'JSON ì²˜ë¦¬ ì‹¤íŒ¨'
            article['comments'] = []
        except Exception as e:
            print(f"\n[ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜] {article.get('title', '')}: {e}")
            article['views'] = 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'
            article['comments'] = []

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data_with_club_ids, f, ensure_ascii=False, indent=4)
    print(f"--- 3ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… ê²°ê³¼ '{output_file}' íŒŒì¼ ì €ì¥ ---")
    return data_with_club_ids


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # .env íŒŒì¼ì—ì„œ ì¿ í‚¤ ê°’ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    script_dir = os.path.dirname(os.path.abspath(__file__))
    dotenv_path = os.path.join(script_dir, '.env')
    load_dotenv(dotenv_path=dotenv_path)
    
    naver_cookie = os.getenv('NAVER_COOKIE')
    
    # ë””ë²„ê¹…ì„ ìœ„í•´ ì‚¬ìš© ì¤‘ì¸ ì¿ í‚¤ ê°’ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    print("-" * 50)
    print(f"ì‚¬ìš© ì¤‘ì¸ ì¿ í‚¤: {naver_cookie}")
    print("-" * 50)

    if not naver_cookie or naver_cookie == "ì—¬ê¸°ì—_ë„¤ì´ë²„_ì¿ í‚¤_ê°’ì„_ë¶™ì—¬ë„£ìœ¼ì„¸ìš”":
        print(f"ì˜¤ë¥˜: '{dotenv_path}' íŒŒì¼ì— ìœ íš¨í•œ NAVER_COOKIEë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    # ì…/ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    queries_file = os.path.join(script_dir, 'search_queries.csv')
    step1_output_file = os.path.join(script_dir, 'step1_initial_crawl_v2.json')
    step2_output_file = os.path.join(script_dir, 'step2_with_club_id_v2.json')
    final_output_file = os.path.join(script_dir, 'naver_cafe_crawl_result_v2.json')

    try:
        queries_df = pd.read_csv(queries_file)
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: '{queries_file}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê° ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
    # ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œí•´ì„œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    if not os.path.exists(step1_output_file):
        step1_data = step1_initial_crawl(queries_df, naver_cookie, step1_output_file)
    else:
        print(f"--- 1ë‹¨ê³„ ê±´ë„ˆë›°ê¸°: '{step1_output_file}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ---")
        with open(step1_output_file, 'r', encoding='utf-8') as f:
            step1_data = json.load(f)

    if not os.path.exists(step2_output_file):
        step2_data = step2_get_club_ids(step1_data, naver_cookie, step2_output_file)
    else:
        print(f"--- 2ë‹¨ê³„ ê±´ë„ˆë›°ê¸°: '{step2_output_file}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ---")
        with open(step2_output_file, 'r', encoding='utf-8') as f:
            step2_data = json.load(f)

    step3_enrich_details(step2_data, naver_cookie, final_output_file)
    
    print("\n\nğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰")
    print(f"ìµœì¢… ê²°ê³¼ëŠ” '{final_output_file}' íŒŒì¼ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main() 
    main() 
