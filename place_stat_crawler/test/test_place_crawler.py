#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
NaverStatCrawler κΈ°λ¥ ν…μ¤νΈ μ¤ν¬λ¦½νΈ
- λ‹¨μΌ ν΄λΌμ΄μ–ΈνΈμ— λ€ν•΄ νΉμ • λ‚ μ§μ ν”λ μ΄μ¤ PV ν†µκ³„(μ±„λ„, ν‚¤μ›λ“)λ¥Ό μμ§‘ν•μ—¬ μ •μƒ μ‘λ™ μ—¬λ¶€λ¥Ό ν™•μΈν•©λ‹λ‹¤.
"""

import sys
import os
from datetime import datetime, timedelta
from pathlib import Path
import logging

# ν”„λ΅μ νΈ λ£¨νΈ κ²½λ΅λ¥Ό sys.pathμ— μ¶”κ°€ν•μ—¬ λ¨λ“μ„ μ°Ύμ„ μ μλ„λ΅ ν•¨
project_root = Path(__file__).resolve().parents[1] # .parents[2] -> .parents[1]λ΅ μμ •
sys.path.insert(0, str(project_root))

from scripts.util.config import get_config_manager
from scripts.crawler.naver_place_pv_stat_crawler import NaverStatCrawler
from scripts.crawler.naver_place_pv_crawler_base import ApiCallError

# κΈ°λ³Έ λ΅κΉ… μ„¤μ •μ€ config.pyμ— μ„μ„ν•©λ‹λ‹¤.
logger = logging.getLogger(__name__)

def run_place_test():
    """ν”λ μ΄μ¤ ν†µκ³„ μμ§‘κΈ° ν…μ¤νΈ μ‹¤ν–‰"""
    logger.info("=" * 60)
    logger.info("π“ λ„¤μ΄λ²„ ν”λ μ΄μ¤ ν†µκ³„ ν¬λ΅¤λ¬(NaverStatCrawler) λ‹¨λ… ν…μ¤νΈλ¥Ό μ‹μ‘ν•©λ‹λ‹¤.")
    logger.info("=" * 60)

    # 1. μ„¤μ • λ° ν΄λΌμ΄μ–ΈνΈ μ„ νƒ
    config_manager = get_config_manager()
    client_info = config_manager.get_selected_client_config()
    if not client_info:
        logger.error("β ν΄λΌμ΄μ–ΈνΈλ¥Ό μ„ νƒν•μ§€ λ»ν–μµλ‹λ‹¤. ν…μ¤νΈλ¥Ό μΆ…λ£ν•©λ‹λ‹¤.")
        return

    # 2. ν¬λ΅¤λ¬ μ΄κΈ°ν™”
    try:
        auth_config = config_manager.get_auth_config()
        crawler = NaverStatCrawler(client_info, auth_config)
        logger.info(f"β… '{client_info.name}' ν΄λΌμ΄μ–ΈνΈλ΅ ν¬λ΅¤λ¬λ¥Ό μ΄κΈ°ν™”ν–μµλ‹λ‹¤.")
    except Exception as e:
        logger.error(f"β ν¬λ΅¤λ¬ μ΄κΈ°ν™” μ¤‘ μ¤λ¥ λ°μƒ: {e}", exc_info=True)
        return

    # 3. ν…μ¤νΈν•  λ‚ μ§ μ„¤μ • (μ–΄μ )
    target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    logger.info(f"π“… ν…μ¤νΈ λ€μƒ λ‚ μ§: {target_date}")

    # 4. λ°μ΄ν„° μμ§‘ ν…μ¤νΈ
    try:
        # 4-1. μ±„λ„ λ°μ΄ν„° ν…μ¤νΈ
        logger.info("-" * 60)
        logger.info("π“Ά [1/2] μ±„λ„(Channel) λ°μ΄ν„° μμ§‘μ„ ν…μ¤νΈν•©λ‹λ‹¤...")
        channel_data = crawler.fetch_channel_data_for_date(target_date)
        
        if channel_data:
            logger.info(f"β”οΈ μ±„λ„ λ°μ΄ν„° μμ§‘ μ„±κ³µ! (μ΄ {len(channel_data)}κ° μ±„λ„)")
            for item in channel_data[:5]: # μƒμ„ 5κ°λ§ μ¶λ ¥
                logger.info(f"  - μ±„λ„: {item.get('mapped_channel_name', 'N/A')}, PV: {item.get('pv', 0)}")
            if len(channel_data) > 5:
                logger.info("  ...")
        else:
            logger.warning("β οΈ μ±„λ„ λ°μ΄ν„°κ°€ μμ§‘λμ§€ μ•μ•κ±°λ‚ λ°μ΄ν„°κ°€ μ—†μµλ‹λ‹¤.")

        # 4-2. ν‚¤μ›λ“ λ°μ΄ν„° ν…μ¤νΈ
        logger.info("-" * 60)
        logger.info("π“Ά [2/2] ν‚¤μ›λ“(Keyword) λ°μ΄ν„° μμ§‘μ„ ν…μ¤νΈν•©λ‹λ‹¤...")
        keyword_data = crawler.fetch_keyword_data_for_date(target_date)

        if keyword_data:
            logger.info(f"β”οΈ ν‚¤μ›λ“ λ°μ΄ν„° μμ§‘ μ„±κ³µ! (μ΄ {len(keyword_data)}κ° ν‚¤μ›λ“)")
            for item in keyword_data[:5]: # μƒμ„ 5κ°λ§ μ¶λ ¥
                logger.info(f"  - ν‚¤μ›λ“: {item.get('ref_keyword', 'N/A')}, PV: {item.get('pv', 0)}")
            if len(keyword_data) > 5:
                logger.info("  ...")
        else:
            logger.warning("β οΈ ν‚¤μ›λ“ λ°μ΄ν„°κ°€ μμ§‘λμ§€ μ•μ•κ±°λ‚ λ°μ΄ν„°κ°€ μ—†μµλ‹λ‹¤.")

    except ApiCallError as e:
        logger.error(f"β API μΈμ¦ μ¤λ¥κ°€ λ°μƒν–μµλ‹λ‹¤. .env νμΌμ _PLACE_AUTH, _PLACE_COOKIE κ°’μ„ ν™•μΈν•΄μ£Όμ„Έμ”.")
        logger.error(f"   > {e}")
    except Exception as e:
        logger.error(f"β λ°μ΄ν„° μμ§‘ μ¤‘ μμƒμΉ λ»ν• μ¤λ¥ λ°μƒ: {e}", exc_info=True)

    logger.info("=" * 60)
    logger.info("π“ ν…μ¤νΈκ°€ μ™„λ£λμ—μµλ‹λ‹¤.")
    logger.info("=" * 60)


if __name__ == "__main__":
    run_place_test()
