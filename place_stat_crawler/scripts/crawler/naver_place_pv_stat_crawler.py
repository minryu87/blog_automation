#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ í†µê³„ í¬ë¡¤ëŸ¬ (ì¸ì¦ ê´€ë¦¬ í†µí•© ë²„ì „)
ì±„ë„ë³„ PV, ë°©ë¬¸ì, ë¦¬ë·° ë“± ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import pandas as pd
import json
import time
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import logging

# ìƒëŒ€ importë¥¼ ì ˆëŒ€ importë¡œ ë³€ê²½
try:
    from scripts.crawler.naver_place_pv_crawler_base import NaverCrawlerBase
    from scripts.crawler.naver_place_pv_auth_manager import NaverAuthManager
    from scripts.util.logger import logger
    from scripts.util.config import ClientInfo, AuthConfig
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ ì‹œë¥¼ ìœ„í•œ ì ˆëŒ€ import
    import sys
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)
    sys.path.insert(0, parent_dir)
    sys.path.insert(0, current_dir)
    from scripts.crawler.naver_place_pv_crawler_base import NaverCrawlerBase
    from scripts.crawler.naver_place_pv_auth_manager import NaverAuthManager
    from scripts.util.logger import logger
    from scripts.util.config import ClientInfo, AuthConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NaverStatCrawler(NaverCrawlerBase):
    """ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ í†µê³„ ë°ì´í„° í¬ë¡¤ëŸ¬"""

    def __init__(self, client_info: ClientInfo, auth_config: AuthConfig):
        """
        NaverStatCrawler ì´ˆê¸°í™”
        Args:
            client_info (ClientInfo): í´ë¼ì´ì–¸íŠ¸ ì •ë³´
            auth_config (AuthConfig): ì¸ì¦ ì„¤ì •
        """
        auth_manager = NaverAuthManager(
            client_info=client_info,
            auth_config=auth_config,
            auth_type='place'
        )
        super().__init__(client_info=client_info, auth_manager=auth_manager) # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_info = client_info
        self.naver_place_id = None
        self.smart_place_id = None
        
        # ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ API ì„¤ì •
        self.base_url = "https://new.smartplace.naver.com/proxy/bizadvisor/api/v3/sites/sp_311b9ba993e974/report"
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.all_data = {}
        self.channels = set()
        self.channel_appearances = {}  # ì±„ë„ë³„ ì¶œí˜„ íšŸìˆ˜
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.request_delay = 0.5  # ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)
        self.max_retries = 3
        self.timeout = 30
        
        # ì§€ì›í•˜ëŠ” ë©”íŠ¸ë¦­ê³¼ ì°¨ì›
        self.available_metrics = ['pv', 'visitors', 'reviews', 'rating', 'clicks', 'impressions']
        self.available_dimensions = ['mapped_channel_name', 'mapped_channel_id', 'mapped_channel_type', 'mapped_channel_category', 'ref_keyword']
    
    def fetch_channel_data_for_date(self, date: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë‚ ì§œì˜ ì±„ë„ë³„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸°"""
        url = self.base_url
        params = {
            'dimensions': 'mapped_channel_name',
            'startDate': date,
            'endDate': date,
            'metrics': 'pv',
            'sort': 'pv',
            'useIndex': 'revenue-all-channel-detail'
        }
        
        try:
            logger.info(f"ğŸ“Š {date} ì±„ë„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            data = self.make_request('GET', url, params=params)

            print("\n--- [ì±„ë„ API] ì„œë²„ ì‘ë‹µ ---")
            print(data)
            print("--------------------------\n")

            if data and isinstance(data, list):
                logger.info(f"âœ… {date} ì±„ë„ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ: {len(data)}ê°œ í•­ëª©")
                return data
            elif data:
                logger.warning(f"âš ï¸ ì±„ë„ ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹˜: {data}")
                return []
            else:
                logger.info(f"â„¹ï¸ {date} ì±„ë„ ë°ì´í„° ì—†ìŒ.")
                return []
        except ApiCallError as e:
            logger.error(f"âŒ {date} ì±„ë„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ API ì˜¤ë¥˜: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ {date} ì±„ë„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            return []

    def fetch_keyword_data_for_date(self, date: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë‚ ì§œì˜ í‚¤ì›Œë“œë³„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸°"""
        url = self.base_url
        params = {
            'dimensions': 'ref_keyword',
            'startDate': date,
            'endDate': date,
            'metrics': 'pv',
            'sort': 'pv',
            'useIndex': 'revenue-search-channel-detail'
        }
        
        try:
            logger.info(f"ğŸ“Š {date} í‚¤ì›Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            data = self.make_request('GET', url, params=params)

            print("\n--- [í‚¤ì›Œë“œ API] ì„œë²„ ì‘ë‹µ ---")
            print(data)
            print("----------------------------\n")

            if data and isinstance(data, list):
                logger.info(f"âœ… {date} í‚¤ì›Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ: {len(data)}ê°œ í•­ëª©")
                return data
            elif data:
                logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹˜: {data}")
                return []
            else:
                logger.info(f"â„¹ï¸ {date} í‚¤ì›Œë“œ ë°ì´í„° ì—†ìŒ.")
                return []
        except ApiCallError as e:
            logger.error(f"âŒ {date} í‚¤ì›Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ API ì˜¤ë¥˜: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ {date} í‚¤ì›Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            return []
    
    def fetch_comprehensive_data_for_date(self, date: str) -> Dict[str, List[Dict]]:
        """íŠ¹ì • ë‚ ì§œì˜ ì¢…í•© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸° (ì—¬ëŸ¬ ë©”íŠ¸ë¦­)"""
        comprehensive_data = {}
        
        # 1. ê¸°ë³¸ PV ë°ì´í„°
        pv_data = self.fetch_channel_data_for_date(date)
        comprehensive_data['pv'] = pv_data
        
        # 2. ë°©ë¬¸ì ë°ì´í„°
        visitors_data = self.fetch_channel_data_for_date(date)
        comprehensive_data['visitors'] = visitors_data
        
        # 3. ë¦¬ë·° ë°ì´í„°
        reviews_data = self.fetch_channel_data_for_date(date)
        comprehensive_data['reviews'] = reviews_data
        
        # 4. í‰ì  ë°ì´í„°
        rating_data = self.fetch_channel_data_for_date(date)
        comprehensive_data['rating'] = rating_data
        
        # 5. í´ë¦­ ë°ì´í„°
        clicks_data = self.fetch_channel_data_for_date(date)
        comprehensive_data['clicks'] = clicks_data
        
        # 6. ë…¸ì¶œ ë°ì´í„°
        impressions_data = self.fetch_channel_data_for_date(date)
        comprehensive_data['impressions'] = impressions_data
        
        return comprehensive_data
    
    def collect_all_data(self, start_date: str, end_date: str, comprehensive: bool = False) -> Dict[str, List[Dict]]:
        """ì§€ì •ëœ ê¸°ê°„ì˜ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("=" * 60)
        logger.info("ğŸš€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ ì±„ë„ë³„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"ğŸ“… ê¸°ê°„: {start_date} ~ {end_date}")
        logger.info(f"ğŸ“Š ëª¨ë“œ: {'ì¢…í•© ë°ì´í„°' if comprehensive else 'ê¸°ë³¸ PV ë°ì´í„°'}")
        logger.info("=" * 60)
        
        # ë‚ ì§œ ë²”ìœ„ ìƒì„±
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        current_date = start_dt
        while current_date <= end_dt:
            date_str = current_date.strftime('%Y-%m-%d')
            
            if comprehensive:
                # ì¢…í•© ë°ì´í„° ìˆ˜ì§‘
                comprehensive_data = self.fetch_comprehensive_data_for_date(date_str)
                self.all_data[date_str] = comprehensive_data
            else:
                # ê¸°ë³¸ PV ë°ì´í„° ìˆ˜ì§‘
                data = self.fetch_channel_data_for_date(date_str)
                self.all_data[date_str] = data
            
            # ë‹¤ìŒ ë‚ ì§œë¡œ
            current_date += timedelta(days=1)
            
            # ìš”ì²­ ê°„ ë”œë ˆì´
            time.sleep(self.request_delay)
        
        # ì „ì²´ ì±„ë„ ëª©ë¡ íŒŒì•…
        logger.info("\nğŸ“Š ì „ì²´ ì±„ë„ ëª©ë¡ íŒŒì•… ì¤‘...")
        
        for date_str, data_list in self.all_data.items():
            if isinstance(data_list, dict):
                # ì¢…í•© ë°ì´í„° ëª¨ë“œ
                for metric, metric_data in data_list.items():
                    for item in metric_data:
                        if 'mapped_channel_name' in item and item['mapped_channel_name']:
                            channel_name = item['mapped_channel_name']
                            self.channels.add(channel_name)
                            
                            if channel_name not in self.channel_appearances:
                                self.channel_appearances[channel_name] = 0
                            self.channel_appearances[channel_name] += 1
            else:
                # ê¸°ë³¸ ë°ì´í„° ëª¨ë“œ
                for item in data_list:
                    if 'mapped_channel_name' in item and item['mapped_channel_name']:
                        channel_name = item['mapped_channel_name']
                        self.channels.add(channel_name)
                        
                        if channel_name not in self.channel_appearances:
                            self.channel_appearances[channel_name] = 0
                        self.channel_appearances[channel_name] += 1
        
        logger.info(f"\nâœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ {len(self.channels)}ê°œ ì±„ë„ ë°œê²¬:")
        
        # ì¶œí˜„ ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥
        sorted_channels = sorted(self.channel_appearances.items(), key=lambda x: x[1], reverse=True)
        for channel, count in sorted_channels:
            logger.info(f"   - {channel}: {count}ì¼ ì¶œí˜„")
        
        return self.all_data
    
    def collect_keyword_data(self, start_date: str, end_date: str) -> Dict[str, List[Dict]]:
        """ì§€ì •ëœ ê¸°ê°„ì˜ í‚¤ì›Œë“œë³„ ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("=" * 60)
        logger.info("ğŸš€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ í‚¤ì›Œë“œë³„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"ğŸ“… ê¸°ê°„: {start_date} ~ {end_date}")
        logger.info("=" * 60)
        
        # ë‚ ì§œ ë²”ìœ„ ìƒì„±
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        current_date = start_dt
        while current_date <= end_dt:
            date_str = current_date.strftime('%Y-%m-%d')
            
            # í‚¤ì›Œë“œ ë°ì´í„° ìˆ˜ì§‘
            data = self.fetch_keyword_data_for_date(date_str)
            self.all_data[date_str] = data
            
            # ë‹¤ìŒ ë‚ ì§œë¡œ
            current_date += timedelta(days=1)
            
            # ìš”ì²­ ê°„ ë”œë ˆì´
            time.sleep(self.request_delay)
        
        # ì „ì²´ í‚¤ì›Œë“œ ëª©ë¡ íŒŒì•…
        logger.info("\nğŸ“Š ì „ì²´ í‚¤ì›Œë“œ ëª©ë¡ íŒŒì•… ì¤‘...")
        
        keywords = set()
        keyword_appearances = {}
        
        for date_str, data_list in self.all_data.items():
            for item in data_list:
                if 'ref_keyword' in item and item['ref_keyword']:
                    keyword_name = item['ref_keyword']
                    keywords.add(keyword_name)
                    
                    if keyword_name not in keyword_appearances:
                        keyword_appearances[keyword_name] = 0
                    keyword_appearances[keyword_name] += 1
        
        logger.info(f"\nâœ… í‚¤ì›Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œ ë°œê²¬:")
        
        # ì¶œí˜„ ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥
        sorted_keywords = sorted(keyword_appearances.items(), key=lambda x: x[1], reverse=True)
        for keyword, count in sorted_keywords[:10]:  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥
            logger.info(f"   - {keyword}: {count}ì¼ ì¶œí˜„")
        
        return self.all_data
    
    def collect_channel_data(self, start_date: str, end_date: str) -> Dict[str, List[Dict]]:
        """ì§€ì •ëœ ê¸°ê°„ì˜ ì±„ë„ë³„ ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("=" * 60)
        logger.info("ğŸš€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ ì±„ë„ë³„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"ğŸ“… ê¸°ê°„: {start_date} ~ {end_date}")
        logger.info("=" * 60)
        
        # ë‚ ì§œ ë²”ìœ„ ìƒì„±
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        current_date = start_dt
        while current_date <= end_dt:
            date_str = current_date.strftime('%Y-%m-%d')
            
            # ì±„ë„ ë°ì´í„° ìˆ˜ì§‘
            data = self.fetch_channel_data_for_date(date_str)
            self.all_data[date_str] = data
            
            # ë‹¤ìŒ ë‚ ì§œë¡œ
            current_date += timedelta(days=1)
            
            # ìš”ì²­ ê°„ ë”œë ˆì´
            time.sleep(self.request_delay)
        
        # ì „ì²´ ì±„ë„ ëª©ë¡ íŒŒì•…
        logger.info("\nğŸ“Š ì „ì²´ ì±„ë„ ëª©ë¡ íŒŒì•… ì¤‘...")
        
        channels = set()
        channel_appearances = {}
        
        for date_str, data_list in self.all_data.items():
            for item in data_list:
                if 'mapped_channel_name' in item and item['mapped_channel_name']:
                    channel_name = item['mapped_channel_name']
                    channels.add(channel_name)
                    
                    if channel_name not in channel_appearances:
                        channel_appearances[channel_name] = 0
                    channel_appearances[channel_name] += 1
        
        logger.info(f"\nâœ… ì±„ë„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ {len(channels)}ê°œ ì±„ë„ ë°œê²¬:")
        
        # ì¶œí˜„ ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥
        sorted_channels = sorted(channel_appearances.items(), key=lambda x: x[1], reverse=True)
        for channel, count in sorted_channels[:10]:  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥
            logger.info(f"   - {channel}: {count}ì¼ ì¶œí˜„")
        
        return self.all_data
    
    def create_dataframe(self, comprehensive: bool = False) -> pd.DataFrame:
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.all_data:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        if comprehensive:
            return self._create_comprehensive_dataframe()
        else:
            return self._create_basic_dataframe()
    
    def create_keyword_dataframe(self) -> pd.DataFrame:
        """í‚¤ì›Œë“œë³„ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.all_data:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        logger.info("ğŸ“Š í‚¤ì›Œë“œ DataFrame ìƒì„± ì¤‘...")
        
        rows = []
        for date, data_list in self.all_data.items():
            for item in data_list:
                row = {
                    'date': date,
                    'keyword': item.get('ref_keyword', 'Unknown'),
                    'pv': item.get('pv', 0),
                    'keyword_id': item.get('ref_keyword_id', ''),
                    'keyword_type': item.get('ref_keyword_type', ''),
                    'keyword_category': item.get('ref_keyword_category', '')
                }
                rows.append(row)
        
        df = pd.DataFrame(rows)
        
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        df['date'] = pd.to_datetime(df['date'])
        
        # í‚¤ì›Œë“œë³„ ì´ PV ê³„ì‚°
        df['total_pv'] = df.groupby('keyword')['pv'].transform('sum')
        
        logger.info(f"âœ… í‚¤ì›Œë“œ DataFrame ìƒì„± ì™„ë£Œ: {len(df)}í–‰, {len(df['keyword'].unique())}ê°œ í‚¤ì›Œë“œ")
        return df
    
    def create_channel_dataframe(self) -> pd.DataFrame:
        """ì±„ë„ë³„ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.all_data:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ì±„ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        logger.info("ğŸ“Š ì±„ë„ DataFrame ìƒì„± ì¤‘...")
        
        rows = []
        for date, data_list in self.all_data.items():
            for item in data_list:
                row = {
                    'date': date,
                    'channel': item.get('mapped_channel_name', 'Unknown'),
                    'pv': item.get('pv', 0),
                    'channel_id': item.get('mapped_channel_id', ''),
                    'channel_type': item.get('mapped_channel_type', ''),
                    'channel_category': item.get('mapped_channel_category', '')
                }
                rows.append(row)
        
        df = pd.DataFrame(rows)
        
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        df['date'] = pd.to_datetime(df['date'])
        
        # ì±„ë„ë³„ ì´ PV ê³„ì‚°
        df['total_pv'] = df.groupby('channel')['pv'].transform('sum')
        
        logger.info(f"âœ… ì±„ë„ DataFrame ìƒì„± ì™„ë£Œ: {len(df)}í–‰, {len(df['channel'].unique())}ê°œ ì±„ë„")
        return df
    
    def _create_basic_dataframe(self) -> pd.DataFrame:
        """ê¸°ë³¸ PV ë°ì´í„° DataFrame ìƒì„±"""
        logger.info("ğŸ“Š ê¸°ë³¸ DataFrame ìƒì„± ì¤‘...")
        
        rows = []
        for date, data_list in self.all_data.items():
            for item in data_list:
                row = {
                    'date': date,
                    'channel': item.get('mapped_channel_name', 'Unknown'),
                    'pv': item.get('pv', 0),
                    'channel_id': item.get('mapped_channel_id', ''),
                    'channel_type': item.get('mapped_channel_type', ''),
                    'channel_category': item.get('mapped_channel_category', '')
                }
                rows.append(row)
        
        df = pd.DataFrame(rows)
        
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        df['date'] = pd.to_datetime(df['date'])
        
        # ì±„ë„ë³„ ì´ PV ê³„ì‚°
        df['total_pv'] = df.groupby('channel')['pv'].transform('sum')
        
        logger.info(f"âœ… ê¸°ë³¸ DataFrame ìƒì„± ì™„ë£Œ: {len(df)}í–‰, {len(df['channel'].unique())}ê°œ ì±„ë„")
        return df
    
    def _create_comprehensive_dataframe(self) -> pd.DataFrame:
        """ì¢…í•© ë°ì´í„° DataFrame ìƒì„±"""
        logger.info("ğŸ“Š ì¢…í•© DataFrame ìƒì„± ì¤‘...")
        
        rows = []
        for date, comprehensive_data in self.all_data.items():
            # ê° ë©”íŠ¸ë¦­ë³„ ë°ì´í„° í†µí•©
            channel_data = {}
            
            for metric, metric_data in comprehensive_data.items():
                for item in metric_data:
                    channel_name = item.get('mapped_channel_name', 'Unknown')
                    if channel_name not in channel_data:
                        channel_data[channel_name] = {
                            'date': date,
                            'channel': channel_name,
                            'channel_id': item.get('mapped_channel_id', ''),
                            'channel_type': item.get('mapped_channel_type', ''),
                            'channel_category': item.get('mapped_channel_category', '')
                        }
                    
                    # ë©”íŠ¸ë¦­ ê°’ ì¶”ê°€
                    channel_data[channel_name][metric] = item.get(metric, 0)
            
            # í–‰ ë°ì´í„° ì¶”ê°€
            for channel_name, data in channel_data.items():
                rows.append(data)
        
        df = pd.DataFrame(rows)
        
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        df['date'] = pd.to_datetime(df['date'])
        
        # ëˆ„ë½ëœ ë©”íŠ¸ë¦­ ì»¬ëŸ¼ì„ 0ìœ¼ë¡œ ì±„ìš°ê¸°
        for metric in self.available_metrics:
            if metric not in df.columns:
                df[metric] = 0
        
        logger.info(f"âœ… ì¢…í•© DataFrame ìƒì„± ì™„ë£Œ: {len(df)}í–‰, {len(df['channel'].unique())}ê°œ ì±„ë„")
        return df
    
    def calculate_statistics(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """í†µê³„ ê³„ì‚°"""
        logger.info("ğŸ“ˆ í†µê³„ ê³„ì‚° ì¤‘...")
        
        # í‚¤ì›Œë“œ ë°ì´í„°ì¸ì§€ í™•ì¸
        if 'keyword' in df.columns:
            return self._calculate_keyword_statistics(df)
        
        # ì±„ë„ë³„ í†µê³„
        if 'pv' in df.columns:
            channel_stats = df.groupby('channel').agg({
                'pv': ['sum', 'mean', 'std', 'count'],
                'date': ['min', 'max']
            }).round(2)
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬
            channel_stats.columns = [
                'total_pv', 'avg_pv', 'std_pv', 'data_count',
                'start_date', 'end_date'
            ]
        else:
            # ì¢…í•© ë°ì´í„°ì˜ ê²½ìš°
            agg_dict = {}
            for metric in self.available_metrics:
                if metric in df.columns:
                    agg_dict[metric] = ['sum', 'mean', 'std', 'count']
            
            if 'date' in df.columns:
                agg_dict['date'] = ['min', 'max']
            
            channel_stats = df.groupby('channel').agg(agg_dict).round(2)
        
        # ì „ì²´ í†µê³„
        total_stats = {
            'total_channels': len(df['channel'].unique()),
            'date_range': f"{df['date'].min().strftime('%Y-%m-%d')} ~ {df['date'].max().strftime('%Y-%m-%d')}"
        }
        
        # ê° ë©”íŠ¸ë¦­ë³„ ì´í•© ì¶”ê°€
        for metric in self.available_metrics:
            if metric in df.columns:
                total_stats[f'total_{metric}'] = df[metric].sum()
                total_stats[f'avg_{metric}_per_channel'] = df.groupby('channel')[metric].sum().mean()
        
        logger.info(f"âœ… í†µê³„ ê³„ì‚° ì™„ë£Œ: {len(channel_stats)}ê°œ ì±„ë„")
        return channel_stats, total_stats
    
    def _calculate_keyword_statistics(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """í‚¤ì›Œë“œ ë°ì´í„° í†µê³„ ê³„ì‚°"""
        logger.info("ğŸ“ˆ í‚¤ì›Œë“œ í†µê³„ ê³„ì‚° ì¤‘...")
        
        # í‚¤ì›Œë“œë³„ í†µê³„
        if 'pv' in df.columns:
            keyword_stats = df.groupby('keyword').agg({
                'pv': ['sum', 'mean', 'std', 'count'],
                'date': ['min', 'max']
            }).round(2)
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬
            keyword_stats.columns = [
                'total_pv', 'avg_pv', 'std_pv', 'data_count',
                'start_date', 'end_date'
            ]
        else:
            keyword_stats = pd.DataFrame()
        
        # ì „ì²´ í†µê³„
        total_stats = {
            'total_keywords': len(df['keyword'].unique()),
            'date_range': f"{df['date'].min().strftime('%Y-%m-%d')} ~ {df['date'].max().strftime('%Y-%m-%d')}"
        }
        
        # ê° ë©”íŠ¸ë¦­ë³„ ì´í•© ì¶”ê°€
        for metric in self.available_metrics:
            if metric in df.columns:
                total_stats[f'total_{metric}'] = df[metric].sum()
                total_stats[f'avg_{metric}_per_keyword'] = df.groupby('keyword')[metric].sum().mean()
        
        logger.info(f"âœ… í‚¤ì›Œë“œ í†µê³„ ê³„ì‚° ì™„ë£Œ: {len(keyword_stats)}ê°œ í‚¤ì›Œë“œ")
        return keyword_stats, total_stats
    
    def save_to_files(self, df: pd.DataFrame, stats: pd.DataFrame, output_dir: str = ".", comprehensive: bool = False):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        # í‚¤ì›Œë“œ ë°ì´í„°ì¸ì§€ í™•ì¸
        if 'keyword' in df.columns:
            return self._save_keyword_files(df, stats, output_dir, comprehensive)
        
        logger.info("ğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # íŒŒì¼ëª…ì— ëª¨ë“œ í‘œì‹œ
        mode_suffix = "_comprehensive" if comprehensive else "_basic"
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # CSV íŒŒì¼ ì €ì¥
        csv_file = os.path.join(output_dir, f"channel_data{mode_suffix}_{timestamp}.csv")
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ“„ CSV ì €ì¥: {csv_file}")
        
        # Excel íŒŒì¼ ì €ì¥
        excel_file = os.path.join(output_dir, f"channel_data{mode_suffix}_{timestamp}.xlsx")
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Raw Data', index=False)
            stats.to_excel(writer, sheet_name='Channel Statistics')
            
            # ì¢…í•© ë°ì´í„°ì˜ ê²½ìš° ë©”íŠ¸ë¦­ë³„ ì‹œíŠ¸ ì¶”ê°€
            if comprehensive:
                for metric in self.available_metrics:
                    if metric in df.columns:
                        metric_df = df[['date', 'channel', metric]].pivot_table(
                            index='date', columns='channel', values=metric, fill_value=0
                        )
                        metric_df.to_excel(writer, sheet_name=f'{metric.upper()}_Data')
        
        logger.info(f"ğŸ“Š Excel ì €ì¥: {excel_file}")
        
        # JSON íŒŒì¼ ì €ì¥
        json_file = os.path.join(output_dir, f"channel_data{mode_suffix}_{timestamp}.json")
        data_dict = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_channels': len(df['channel'].unique()),
                'total_records': len(df),
                'date_range': f"{df['date'].min().strftime('%Y-%m-%d')} ~ {df['date'].max().strftime('%Y-%m-%d')}",
                'comprehensive_mode': comprehensive,
                'available_metrics': self.available_metrics
            },
            'raw_data': df.to_dict('records'),
            'channel_statistics': stats.to_dict('index'),
            'channel_appearances': self.channel_appearances
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data_dict, f, ensure_ascii=False, indent=2, default=str)
        logger.info(f"ğŸ“‹ JSON ì €ì¥: {json_file}")
    
    def _save_keyword_files(self, df: pd.DataFrame, stats: pd.DataFrame, output_dir: str = ".", comprehensive: bool = False):
        """í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        logger.info("ğŸ’¾ í‚¤ì›Œë“œ íŒŒì¼ ì €ì¥ ì¤‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # íŒŒì¼ëª…ì— ëª¨ë“œ í‘œì‹œ
        mode_suffix = "_comprehensive" if comprehensive else "_basic"
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # CSV íŒŒì¼ ì €ì¥
        csv_file = os.path.join(output_dir, f"keyword_data{mode_suffix}_{timestamp}.csv")
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ“„ CSV ì €ì¥: {csv_file}")
        
        # Excel íŒŒì¼ ì €ì¥
        excel_file = os.path.join(output_dir, f"keyword_data{mode_suffix}_{timestamp}.xlsx")
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Raw Data', index=False)
            stats.to_excel(writer, sheet_name='Keyword Statistics')
            
            # ì¢…í•© ë°ì´í„°ì˜ ê²½ìš° ë©”íŠ¸ë¦­ë³„ ì‹œíŠ¸ ì¶”ê°€
            if comprehensive:
                for metric in self.available_metrics:
                    if metric in df.columns:
                        metric_df = df[['date', 'keyword', metric]].pivot_table(
                            index='date', columns='keyword', values=metric, fill_value=0
                        )
                        metric_df.to_excel(writer, sheet_name=f'{metric.upper()}_Data')
        
        logger.info(f"ğŸ“Š Excel ì €ì¥: {excel_file}")
        
        # JSON íŒŒì¼ ì €ì¥
        json_file = os.path.join(output_dir, f"keyword_data{mode_suffix}_{timestamp}.json")
        data_dict = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_keywords': len(df['keyword'].unique()),
                'total_records': len(df),
                'date_range': f"{df['date'].min().strftime('%Y-%m-%d')} ~ {df['date'].max().strftime('%Y-%m-%d')}",
                'comprehensive_mode': comprehensive,
                'available_metrics': self.available_metrics
            },
            'raw_data': df.to_dict('records'),
            'keyword_statistics': stats.to_dict('index')
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data_dict, f, ensure_ascii=False, indent=2, default=str)
        logger.info(f"ğŸ“‹ JSON ì €ì¥: {json_file}")
    
    def display_results(self, df: pd.DataFrame, stats: pd.DataFrame, comprehensive: bool = False):
        """ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        # ê¸°ë³¸ ì •ë³´
        print(f"ğŸ“… ê¸°ê°„: {df['date'].min().strftime('%Y-%m-%d')} ~ {df['date'].max().strftime('%Y-%m-%d')}")
        print(f"ğŸ“Š ì´ ì±„ë„ ìˆ˜: {len(df['channel'].unique())}")
        print(f"ğŸ“‹ ì´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
        
        if comprehensive:
            # ì¢…í•© ë°ì´í„°ì˜ ê²½ìš° ê° ë©”íŠ¸ë¦­ë³„ ì •ë³´ ì¶œë ¥
            for metric in self.available_metrics:
                if metric in df.columns:
                    total_value = df[metric].sum()
                    print(f"ğŸ“ˆ ì´ {metric.upper()}: {total_value:,}")
        else:
            # ê¸°ë³¸ PV ë°ì´í„°ì˜ ê²½ìš°
            total_pv = df['pv'].sum()
            print(f"ğŸ“ˆ ì´ PV: {total_pv:,}")
        
        # ìƒìœ„ ì±„ë„ (PV ê¸°ì¤€)
        if 'pv' in df.columns:
            top_channels = df.groupby('channel')['pv'].sum().sort_values(ascending=False).head(10)
            print(f"\nğŸ† ìƒìœ„ 10ê°œ ì±„ë„ (PV ê¸°ì¤€):")
            for i, (channel, pv) in enumerate(top_channels.items(), 1):
                print(f"  {i:2d}. {channel}: {pv:,} PV")
        
        # í†µê³„ ìš”ì•½
        print(f"\nğŸ“ˆ í†µê³„ ìš”ì•½:")
        if 'pv' in df.columns:
            print(f"  í‰ê·  PV/ì±„ë„: {df.groupby('channel')['pv'].sum().mean():.2f}")
            print(f"  ìµœëŒ€ PV/ì±„ë„: {df.groupby('channel')['pv'].sum().max():,}")
            print(f"  ìµœì†Œ PV/ì±„ë„: {df.groupby('channel')['pv'].sum().min():,}")
    
    def create_visual_report(self, df: pd.DataFrame, output_dir: str = ".", comprehensive: bool = False):
        """ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            logger.info("ğŸ“Š ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
            
            # í•œê¸€ í°íŠ¸ ì„¤ì •
            plt.rcParams['font.family'] = 'DejaVu Sans'
            plt.rcParams['axes.unicode_minus'] = False
            
            if comprehensive:
                # ì¢…í•© ë°ì´í„° ì‹œê°í™”
                fig, axes = plt.subplots(2, 3, figsize=(18, 12))
                fig.suptitle('Comprehensive Channel Analysis Report', fontsize=16)
                
                metrics_to_plot = [m for m in self.available_metrics if m in df.columns][:6]
                
                for i, metric in enumerate(metrics_to_plot):
                    row, col = i // 3, i % 3
                    metric_data = df.groupby('channel')[metric].sum().sort_values(ascending=False).head(10)
                    metric_data.plot(kind='bar', ax=axes[row, col])
                    axes[row, col].set_title(f'Top 10 Channels by {metric.upper()}')
                    axes[row, col].set_xlabel('Channel')
                    axes[row, col].set_ylabel(metric.upper())
                    axes[row, col].tick_params(axis='x', rotation=45)
                
                plt.tight_layout()
            else:
                # ê¸°ë³¸ PV ë°ì´í„° ì‹œê°í™”
                plt.figure(figsize=(12, 8))
                channel_pv = df.groupby('channel')['pv'].sum().sort_values(ascending=False).head(15)
                channel_pv.plot(kind='bar')
                plt.title('Top 15 Channels by PV', fontsize=14)
                plt.xlabel('Channel')
                plt.ylabel('Total PV')
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
            
            chart_file = os.path.join(output_dir, f"channel_analysis_chart_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ğŸ“Š ì°¨íŠ¸ ì €ì¥: {chart_file}")
            
        except ImportError:
            logger.warning("matplotlib ë˜ëŠ” seabornì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
    
    def run(self, start_date: str, end_date: str, output_dir: str = ".", comprehensive: bool = False):
        """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        logger.info("ğŸš€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
        
        try:
            # 1. ë°ì´í„° ìˆ˜ì§‘
            self.collect_all_data(start_date, end_date, comprehensive)
            
            # 2. DataFrame ìƒì„±
            df = self.create_dataframe(comprehensive)
            
            # 3. í†µê³„ ê³„ì‚°
            stats, total_stats = self.calculate_statistics(df)
            
            # 4. íŒŒì¼ ì €ì¥
            self.save_to_files(df, stats, output_dir, comprehensive)
            
            # 5. ê²°ê³¼ ì¶œë ¥
            self.display_results(df, stats, comprehensive)
            
            # 6. ì‹œê°í™”
            self.create_visual_report(df, output_dir, comprehensive)
            
            logger.info("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False
    
    def run_keyword_analysis(self, start_date: str, end_date: str, output_dir: str = "."):
        """í‚¤ì›Œë“œë³„ ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        logger.info("ğŸš€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘")
        
        try:
            # 1. í‚¤ì›Œë“œ ë°ì´í„° ìˆ˜ì§‘
            self.collect_keyword_data(start_date, end_date)
            
            # 2. í‚¤ì›Œë“œ DataFrame ìƒì„±
            df = self.create_keyword_dataframe()
            
            if df.empty:
                logger.error("âŒ í‚¤ì›Œë“œ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
                return False
            
            # 3. í†µê³„ ê³„ì‚°
            stats, total_stats = self.calculate_statistics(df)
            
            # 4. íŒŒì¼ ì €ì¥
            self.save_to_files(df, stats, output_dir, comprehensive=False)
            
            # 5. ê²°ê³¼ ì¶œë ¥
            self.display_results(df, stats, comprehensive=False)
            
            # 6. ì‹œê°í™”
            self.create_visual_report(df, output_dir, comprehensive=False)
            
            logger.info("âœ… í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return False
    
    def run_channel_analysis(self, start_date: str, end_date: str, output_dir: str = "."):
        """ì±„ë„ë³„ ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        logger.info("ğŸš€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ ì±„ë„ ë¶„ì„ ì‹œì‘")
        
        try:
            # 1. ì±„ë„ ë°ì´í„° ìˆ˜ì§‘
            self.collect_channel_data(start_date, end_date)
            
            # 2. ì±„ë„ DataFrame ìƒì„±
            df = self.create_channel_dataframe()
            
            if df.empty:
                logger.error("âŒ ì±„ë„ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
                return False
            
            # 3. í†µê³„ ê³„ì‚°
            stats, total_stats = self.calculate_statistics(df)
            
            # 4. íŒŒì¼ ì €ì¥
            self.save_to_files(df, stats, output_dir, comprehensive=False)
            
            # 5. ê²°ê³¼ ì¶œë ¥
            self.display_results(df, stats, comprehensive=False)
            
            # 6. ì‹œê°í™”
            self.create_visual_report(df, output_dir, comprehensive=False)
            
            logger.info("âœ… ì±„ë„ ë¶„ì„ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì±„ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return False


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    import asyncio
    
    try:
        # ì„¤ì • ë§¤ë‹ˆì €ì—ì„œ í´ë¼ì´ì–¸íŠ¸ ì •ë³´ ì¡°íšŒ
        from scripts.util.config import get_config_manager
        config_manager = get_config_manager()
        client = config_manager.get_selected_client_config()
        
        if not client:
            print("âŒ í´ë¼ì´ì–¸íŠ¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .envë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return
        
        print(f"âœ… ì„ íƒëœ í´ë¼ì´ì–¸íŠ¸: {client.name}")
        
        # í¬ë¡¤ëŸ¬ ìƒì„± (í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‚¬ìš©)
        crawler = NaverStatCrawler(client, client.auth_config)
        
        # í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„¤ì •
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        
        print("\në„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ ë¶„ì„ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. í‚¤ì›Œë“œë³„ ë¶„ì„ (ìœ ì… í‚¤ì›Œë“œ í†µê³„)")
        print("2. ì±„ë„ë³„ ë¶„ì„ (ìœ ì… ì±„ë„ í†µê³„)")
        print("3. ì¢…í•© ë¶„ì„ (ëª¨ë“  ë©”íŠ¸ë¦­)")
        
        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = "./output"
        os.makedirs(output_dir, exist_ok=True)
        
        if choice == "1":
            # í‚¤ì›Œë“œë³„ ë¶„ì„
            success = crawler.run_keyword_analysis(start_date, end_date, output_dir)
            if success:
                print("âœ… í‚¤ì›Œë“œ ë¶„ì„ ì„±ê³µ!")
            else:
                print("âŒ í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨!")
                
        elif choice == "2":
            # ì±„ë„ë³„ ë¶„ì„
            success = crawler.run_channel_analysis(start_date, end_date, output_dir)
            if success:
                print("âœ… ì±„ë„ ë¶„ì„ ì„±ê³µ!")
            else:
                print("âŒ ì±„ë„ ë¶„ì„ ì‹¤íŒ¨!")
                
        elif choice == "3":
            # ì¢…í•© ë¶„ì„ (ê¸°ì¡´ ê¸°ëŠ¥)
            success = crawler.run(start_date, end_date, output_dir, comprehensive=True)
            if success:
                print("âœ… ì¢…í•© ë¶„ì„ ì„±ê³µ!")
            else:
                print("âŒ ì¢…í•© ë¶„ì„ ì‹¤íŒ¨!")
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
