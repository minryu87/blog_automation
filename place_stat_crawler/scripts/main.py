#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ë„¤ì´ë²„ ë§µ ë¦¬ë·° í¬ë¡¤ë§ ë©”ì¸ ì›Œí¬í”Œë¡œìš°
ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ê´€ë¦¬í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import asyncio
import logging
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scripts.crawler.place_review_crawler import NaverPlaceReviewCrawler
from scripts.util.config import get_config_manager
from scripts.util.data_saver import DataSaver
from scripts.util.proxy_manager import get_proxy_manager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CrawlingWorkflow:
    """í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config_manager = get_config_manager()
        self.data_saver = DataSaver()
        self.crawler = None
        self.proxy_manager = None
    
    async def initialize(self):
        """ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”"""
        logger.info("í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì‹œì‘")
        
        # ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬
        if not self.config_manager.validate_config():
            logger.error("ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨")
            return False
        
        # ì¸ì¦ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        auth_config = self.config_manager.get_auth_config()
        
        # í¬ë¡¤ëŸ¬ ìƒì„±
        self.crawler = NaverPlaceReviewCrawler(
            naver_id=auth_config.naver_id,
            naver_password=auth_config.naver_password,
            use_proxy=auth_config.use_proxy
        )
        
        # í”„ë¡ì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        if auth_config.use_proxy:
            self.proxy_manager = await get_proxy_manager()
        
        logger.info("í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì™„ë£Œ")
        return True
    
    async def crawl_single_store(self, 
                                store_name: str, 
                                store_detail: str,
                                channel_id: str = None,
                                channel_place_id: str = None,
                                recent_crawling_at: datetime = None) -> Dict[str, Any]:
        """ë‹¨ì¼ ë§¤ì¥ í¬ë¡¤ë§"""
        logger.info(f"ë‹¨ì¼ ë§¤ì¥ í¬ë¡¤ë§ ì‹œì‘: {store_name} ({store_detail})")
        
        try:
            # í¬ë¡¤ë§ ì‹¤í–‰
            result = await self.crawler.crawl_place_reviews(
                channel_id=channel_id or f"channel_{store_name}",
                store_name=store_name,
                store_detail=store_detail,
                channel_place_id=channel_place_id,
                store_channel_recent_crawling_at=recent_crawling_at
            )
            
            # ê²°ê³¼ ì €ì¥
            if result.get('success', False):
                saved_files = self.data_saver.save_crawling_result(
                    result, store_name, datetime.now()
                )
                result['saved_files'] = saved_files
                logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {store_name} - {result.get('total_reviews', 0)}ê°œ ë¦¬ë·°")
            else:
                logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {store_name} - {result.get('error', 'Unknown error')}")
            
            return result
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {store_name}, {e}")
            return {
                "success": False,
                "error": str(e),
                "store_name": store_name,
                "store_detail": store_detail
            }
    
    async def crawl_multiple_stores(self, 
                                  stores: List[Dict[str, Any]],
                                  batch_name: str = None) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ë§¤ì¥ ë°°ì¹˜ í¬ë¡¤ë§"""
        if not batch_name:
            batch_name = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        logger.info(f"ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘: {len(stores)}ê°œ ë§¤ì¥")
        
        results = []
        successful_count = 0
        failed_count = 0
        
        for i, store in enumerate(stores, 1):
            logger.info(f"ì§„í–‰ë¥ : {i}/{len(stores)} - {store.get('store_name', 'Unknown')}")
            
            result = await self.crawl_single_store(
                store_name=store.get('store_name'),
                store_detail=store.get('store_detail'),
                channel_id=store.get('channel_id'),
                channel_place_id=store.get('channel_place_id'),
                recent_crawling_at=store.get('recent_crawling_at')
            )
            
            results.append(result)
            
            if result.get('success', False):
                successful_count += 1
            else:
                failed_count += 1
            
            # ë°°ì¹˜ ê°„ ëŒ€ê¸°
            if i < len(stores):
                await asyncio.sleep(2)
        
        # ë°°ì¹˜ ê²°ê³¼ ì €ì¥
        batch_result = {
            "batch_name": batch_name,
            "total_stores": len(stores),
            "successful_count": successful_count,
            "failed_count": failed_count,
            "results": results,
            "timestamp": datetime.now().isoformat()
        }
        
        saved_files = self.data_saver.save_batch_results(results, batch_name)
        batch_result['saved_files'] = saved_files
        
        logger.info(f"ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ: {successful_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
        return batch_result
    
    async def crawl_from_file(self, 
                             file_path: str,
                             batch_name: str = None) -> Dict[str, Any]:
        """íŒŒì¼ì—ì„œ ë§¤ì¥ ëª©ë¡ì„ ì½ì–´ì„œ í¬ë¡¤ë§"""
        import csv
        import json
        
        logger.info(f"íŒŒì¼ì—ì„œ ë§¤ì¥ ëª©ë¡ ë¡œë“œ: {file_path}")
        
        stores = []
        file_ext = Path(file_path).suffix.lower()
        
        try:
            if file_ext == '.csv':
                with open(file_path, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    stores = list(reader)
            elif file_ext == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    stores = json.load(f)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")
            
            logger.info(f"ë§¤ì¥ ëª©ë¡ ë¡œë“œ ì™„ë£Œ: {len(stores)}ê°œ")
            
            return await self.crawl_multiple_stores(stores, batch_name)
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "file_path": file_path
            }
    
    def generate_sample_data(self, output_file: str = "sample_stores.csv"):
        """ìƒ˜í”Œ ë§¤ì¥ ë°ì´í„° ìƒì„±"""
        import csv
        
        sample_stores = [
            {
                "store_name": "ìŠ¤íƒ€ë²…ìŠ¤ ê°•ë‚¨ì ",
                "store_detail": "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123",
                "channel_id": "channel_starbucks_gangnam",
                "channel_place_id": None,
                "recent_crawling_at": (datetime.now() - timedelta(days=7)).isoformat()
            },
            {
                "store_name": "ì˜¬ë¦¬ë¸Œì˜ ëª…ë™ì ",
                "store_detail": "ì„œìš¸ì‹œ ì¤‘êµ¬ ëª…ë™ê¸¸ 45",
                "channel_id": "channel_oliveyoung_myeongdong",
                "channel_place_id": None,
                "recent_crawling_at": (datetime.now() - timedelta(days=7)).isoformat()
            },
            {
                "store_name": "ì´ë§ˆíŠ¸ ì ì‹¤ì ",
                "store_detail": "ì„œìš¸ì‹œ ì†¡íŒŒêµ¬ ì˜¬ë¦¼í”½ë¡œ 240",
                "channel_id": "channel_emart_jamsil",
                "channel_place_id": None,
                "recent_crawling_at": (datetime.now() - timedelta(days=7)).isoformat()
            }
        ]
        
        with open(output_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=sample_stores[0].keys())
            writer.writeheader()
            writer.writerows(sample_stores)
        
        logger.info(f"ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {output_file}")
        return output_file
    
    def get_statistics(self) -> Dict[str, Any]:
        """í¬ë¡¤ë§ í†µê³„ ë°˜í™˜"""
        if not self.crawler:
            return {"error": "í¬ë¡¤ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        return self.crawler.get_statistics()

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    workflow = CrawlingWorkflow()
    
    # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
    if not await workflow.initialize():
        logger.error("ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì²« ì‹¤í–‰ì‹œ)
    sample_file = workflow.generate_sample_data()
    
    # íŒŒì¼ì—ì„œ ë§¤ì¥ ëª©ë¡ì„ ì½ì–´ì„œ í¬ë¡¤ë§
    batch_result = await workflow.crawl_from_file(sample_file, "sample_batch")
    
    if batch_result.get('success', False):
        print("âœ… ë°°ì¹˜ í¬ë¡¤ë§ ì„±ê³µ!")
        print(f"ğŸ“Š ê²°ê³¼: {batch_result['successful_count']}ê°œ ì„±ê³µ, {batch_result['failed_count']}ê°œ ì‹¤íŒ¨")
        print(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼: {batch_result.get('saved_files', {})}")
    else:
        print(f"âŒ ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤íŒ¨: {batch_result.get('error', 'Unknown error')}")
    
    # í†µê³„ ì¶œë ¥
    stats = workflow.get_statistics()
    print(f"ğŸ“ˆ í¬ë¡¤ë§ í†µê³„: {stats}")

if __name__ == "__main__":
    asyncio.run(main())
