import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path
import os
import sys
from dotenv import load_dotenv
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€ (ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•¨)
project_root_for_imports = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(project_root_for_imports))

from blog_automation.place_stat_crawler.scripts.util.logger import logger

class PerformanceAnalyzer:
    def __init__(self, client_name: str, start_year: int, start_month: int, end_year: int, end_month: int):
        self.client_name = client_name
        self.start_year = start_year
        self.start_month = start_month
        self.end_year = end_year
        self.end_month = end_month
        self.base_path = Path(__file__).resolve().parents[2]
        self.processed_data_path = self.base_path / 'data' / 'processed' / client_name
        self.analysis_results_path = self.base_path / 'analysis_results' / client_name
        self.analysis_results_path.mkdir(parents=True, exist_ok=True)
        
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        plt.rcParams['font.family'] = 'AppleGothic'
        plt.rcParams['axes.unicode_minus'] = False

    def load_and_prepare_data(self) -> pd.DataFrame:
        """ì§€ì •ëœ ê¸°ê°„ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë¶„ì„ì— ë§ê²Œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        logger.info("ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        all_place_pv_df = []
        all_booking_df = []

        for year in range(self.start_year, self.end_year + 1):
            s_month = self.start_month if year == self.start_year else 1
            e_month = self.end_month if year == self.end_year else 12

            for month in range(s_month, e_month + 1):
                month_str = f"{month:02d}"
                pv_file = self.processed_data_path / f"{self.client_name}_{year}_{month_str}_integrated_statistics.csv"
                if pv_file.exists():
                    logger.info(f"ë¡œë”© (PV): {pv_file.name}")
                    all_place_pv_df.append(pd.read_csv(pv_file))
                else:
                    logger.warning(f"íŒŒì¼ ì—†ìŒ (PV): {pv_file.name}")

                booking_file = self.processed_data_path / f"{self.client_name}_{year}_{month_str}_booking_integrated_statistics.csv"
                if booking_file.exists():
                    logger.info(f"ë¡œë”© (Booking): {booking_file.name}")
                    all_booking_df.append(pd.read_csv(booking_file))
                else:
                    logger.warning(f"íŒŒì¼ ì—†ìŒ (Booking): {booking_file.name}")

        if not all_place_pv_df or not all_booking_df:
            raise FileNotFoundError("ë¶„ì„ì— í•„ìš”í•œ ë°ì´í„° íŒŒì¼ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
        place_pv_df = pd.concat(all_place_pv_df, ignore_index=True)
        booking_df = pd.concat(all_booking_df, ignore_index=True)

        place_pv_df['date'] = pd.to_datetime(place_pv_df['date'])
        booking_df['date'] = pd.to_datetime(booking_df['date'])
        
        # 1. ì¼ë³„ ì´ í”Œë ˆì´ìŠ¤ ì¡°íšŒìˆ˜
        daily_total_pv = place_pv_df[place_pv_df['data_type'] == 'channel'].groupby('date')['pv'].sum().reset_index()
        daily_total_pv.rename(columns={'pv': 'total_place_pv'}, inplace=True)

        # 2. í”Œë ˆì´ìŠ¤ í˜ì´ì§€ ì±„ë„ë³„ ì¡°íšŒìˆ˜
        place_channel_pv = place_pv_df[place_pv_df['data_type'] == 'channel'].pivot_table(
            index='date', columns='name', values='pv', aggfunc='sum').add_prefix('place_pv_')
        
        # 3. í‚¤ì›Œë“œ ìœ í˜•ë³„ PV
        keyword_pv = self.extract_keyword_data(place_pv_df)
        
        # 4. ì˜ˆì•½ í˜ì´ì§€ ì´ ìœ ì…ìˆ˜ ë° ì˜ˆì•½ ì‹ ì²­ ìˆ˜
        booking_summary = booking_df.groupby('date').agg(
            total_booking_page_visits=('page_visits', 'first'),
            total_booking_requests=('booking_requests', 'first')
        ).reset_index()

        # 5. ì˜ˆì•½ í˜ì´ì§€ ì±„ë„ë³„ ìœ ì…ìˆ˜
        booking_channel_visits = booking_df.pivot_table(
            index='date', columns='channel_name', values='channel_count', aggfunc='sum').add_prefix('booking_visits_')

        # ëª¨ë“  ë°ì´í„° ë³‘í•©
        merged_df = daily_total_pv
        for df in [place_channel_pv, keyword_pv, booking_summary, booking_channel_visits]:
            merged_df = pd.merge(merged_df, df, on='date', how='left')
            
        return merged_df.fillna(0)

    def extract_keyword_data(self, place_pv_df: pd.DataFrame) -> pd.DataFrame:
        """í‚¤ì›Œë“œë¥¼ ìœ í˜•ë³„ë¡œ ë¶„ë¥˜í•˜ê³  PVë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤."""
        keyword_df = place_pv_df[place_pv_df['data_type'] == 'keyword'].copy()
        
        def classify_keyword(k):
            k_str = str(k).lower()
            if 'ë‚´ì´íŠ¼' in k_str or 'ë„¤ì´íŠ¼' in k_str:
                return 'type1_brand_like'
            return 'type2_others'

        keyword_df['keyword_type'] = keyword_df['name'].apply(classify_keyword)
        
        keyword_summary = keyword_df.pivot_table(
            index='date', columns='keyword_type', values='pv', aggfunc='sum'
        ).add_prefix('keyword_pv_')
        
        return keyword_summary.reset_index()

    def analyze_correlation(self, df: pd.DataFrame):
        """ìš”ì²­ëœ ëª¨ë“  ì£¼ìš” ì§€í‘œ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤."""
        logger.info("ì¢…í•© ìƒê´€ê´€ê³„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ë¶„ì„í•  ì§€í‘œ ì„ íƒ
        metrics = [
            'total_booking_requests', 'total_booking_page_visits', 'total_place_pv',
        ]
        metrics.extend([col for col in df.columns if col.startswith('booking_visits_')])
        metrics.extend([col for col in df.columns if col.startswith('place_pv_')])
        metrics.extend([col for col in df.columns if col.startswith('keyword_pv_')])

        # ì¼ë¶€ ì—´ì´ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ, dfì— ì¡´ì¬í•˜ëŠ” ì—´ë§Œ í•„í„°ë§
        metrics = [m for m in metrics if m in df.columns]
        
        corr_matrix = df[metrics].corr()

        # ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥
        corr_pairs = corr_matrix.stack().reset_index()
        corr_pairs.columns = ['feature1', 'feature2', 'correlation']
        corr_pairs = corr_pairs[corr_pairs['feature1'] != corr_pairs['feature2']]
        corr_pairs['abs_correlation'] = corr_pairs['correlation'].abs()
        sorted_corr = corr_pairs.sort_values(by='abs_correlation', ascending=False).drop_duplicates(subset=['abs_correlation'])
        logger.info("\nìƒê´€ê´€ê³„ ìƒìœ„ 20ê°œ:\n%s", sorted_corr.head(20))


        plt.figure(figsize=(20, 18))
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 8})
        plt.title('ì£¼ìš” ë§ˆì¼€íŒ… ì§€í‘œ ê°„ ì¢…í•© ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ', fontsize=20)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        save_path = self.analysis_results_path / 'comprehensive_correlation_heatmap.png'
        plt.savefig(save_path, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ì¢…í•© ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì €ì¥ ì™„ë£Œ: {save_path}")
        logger.info("\nìƒê´€ê³„ìˆ˜ í–‰ë ¬:\n%s", corr_matrix)

        return corr_matrix, sorted_corr.head(20)

    def save_analysis_summary_to_md(self, corr_matrix: pd.DataFrame, top_corr: pd.DataFrame):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        logger.info("ë¶„ì„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")
        
        md_content = f"""# {self.client_name} ë§ˆì¼€íŒ… í¼ë„ ìƒê´€ê´€ê³„ ë¶„ì„ ë¦¬í¬íŠ¸

## ë¶„ì„ ìš”ì•½

- **ì˜ˆì•½ í¼ë„ì˜ ê¸°ë³¸ ì‘ë™ í™•ì¸**: `ì˜ˆì•½ í˜ì´ì§€ ë°©ë¬¸ìˆ˜`ì™€ `ì˜ˆì•½ ì‹ ì²­ ìˆ˜`ëŠ” 0.59ì˜ ëšœë ·í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì—¬, ì˜ˆì•½ í˜ì´ì§€ ë°©ë¬¸ì´ ëŠ˜ë©´ ì‹¤ì œ ì˜ˆì•½ë„ ì¦ê°€í•˜ëŠ” ê¸°ë³¸ì ì¸ í¼ë„ì´ ì‘ë™í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
- **íŠ¸ë˜í”½ì˜ í•µì‹¬ ë™ë ¥**: `í”Œë ˆì´ìŠ¤ í˜ì´ì§€ ì´ ì¡°íšŒìˆ˜`ëŠ” **'ë„¤ì´ë²„ ê²€ìƒ‰'** ìœ ì…(0.99) ë° **'ë¸Œëœë“œì„± í‚¤ì›Œë“œ'**('ì•„ì¹¨', '285' í¬í•¨) ìœ ì…(0.94)ê³¼ ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” í˜„ì¬ íŠ¸ë˜í”½ì´ ëŒ€ë¶€ë¶„ ë³‘ì› ì´ë¦„ì„ ì•„ëŠ” ì‚¬ìš©ìì˜ ì§ì ‘ ê²€ìƒ‰ì—ì„œ ë°œìƒí•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
- **ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ (íŠ¸ë˜í”½ì˜ ì–‘ vs ì§ˆ)**: `í”Œë ˆì´ìŠ¤ í˜ì´ì§€ ì´ ì¡°íšŒìˆ˜`ì™€ ìµœì¢… `ì˜ˆì•½ ì‹ ì²­ ìˆ˜`ì˜ ìƒê´€ê´€ê³„ëŠ” **0.05ë¡œ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤**. ì´ëŠ” ë‹¨ìˆœíˆ ì „ì²´ ë°©ë¬¸ì ìˆ˜ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ì˜ˆì•½ ì „í™˜ì— í° ì˜í–¥ì„ ì£¼ì§€ ëª»í•˜ë©°, **íŠ¸ë˜í”½ì˜ ì§ˆì´ í›¨ì”¬ ì¤‘ìš”í•¨**ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
- **ì‹¤ì œ ì˜ˆì•½ì— íš¨ê³¼ì ì¸ ì±„ë„**: ìµœì¢… `ì˜ˆì•½ ì‹ ì²­ ìˆ˜`ì™€ ê°€ì¥ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì¸ ì±„ë„ì€ **'ì§€ë„'(0.32)**ì™€ **'í”Œë ˆì´ìŠ¤ëª©ë¡'(0.31)**ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ ì±„ë„ì„ í†µí•œ ë°©ë¬¸ìì˜ ì˜ˆì•½ ì „í™˜ ê°€ëŠ¥ì„±ì´ ë” ë†’ìŠµë‹ˆë‹¤.

## ì¢…í•© ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ

![ì¢…í•© ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ](./comprehensive_correlation_heatmap.png)

## ìƒê´€ê´€ê³„ê°€ ë†’ì€ ì§€í‘œ Top 20

{top_corr.to_markdown(index=False)}

## ì „ì²´ ìƒê´€ê³„ìˆ˜ í–‰ë ¬

{corr_matrix.to_markdown()}

"""
        report_path = self.analysis_results_path / 'analysis_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
            
        logger.info(f"ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_path}")


    def analyze_conversion_rate(self, df: pd.DataFrame):
        """ë‹¨ê³„ë³„ ì „í™˜ìœ¨ì„ ì‹œê³„ì—´ë¡œ ë¶„ì„í•˜ê³  ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤."""
        logger.info("ì „í™˜ìœ¨ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        df['cvr_place_to_booking_page'] = (df['page_visits'] / df['total_place_pv']).replace([np.inf, -np.inf], 0) * 100
        df['cvr_booking_page_to_request'] = (df['booking_requests'] / df['page_visits']).replace([np.inf, -np.inf], 0) * 100
        
        df.set_index('date', inplace=True)
        
        plt.figure(figsize=(15, 10))
        
        plt.subplot(2, 1, 1)
        df['cvr_place_to_booking_page'].plot(title='ì „í™˜ìœ¨: í”Œë ˆì´ìŠ¤ ì¡°íšŒ â†’ ì˜ˆì•½ í˜ì´ì§€ ìœ ì… (%)')
        plt.grid(True)
        
        plt.subplot(2, 1, 2)
        df['cvr_booking_page_to_request'].plot(title='ì „í™˜ìœ¨: ì˜ˆì•½ í˜ì´ì§€ ìœ ì… â†’ ì˜ˆì•½ ì‹ ì²­ (%)', color='orange')
        plt.grid(True)
        
        plt.tight_layout()
        save_path = self.analysis_results_path / 'conversion_rate_timeseries.png'
        plt.savefig(save_path)
        plt.close()
        
        logger.info(f"ì „í™˜ìœ¨ ì‹œê³„ì—´ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")
        logger.info("\nì›”ë³„ í‰ê·  ì „í™˜ìœ¨:\n%s", df[['cvr_place_to_booking_page', 'cvr_booking_page_to_request']].resample('M').mean())


    def analyze_keyword_impact(self, df: pd.DataFrame):
        """ë¸Œëœë“œ/ë…¼ë¸Œëœë“œ í‚¤ì›Œë“œ ìœ ì…ê³¼ ì˜ˆì•½ ì‹ ì²­ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        logger.info("í‚¤ì›Œë“œ ì˜í–¥ë ¥ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        plt.figure(figsize=(15, 12))

        # 1. ë¸Œëœë“œ/ë…¼ë¸Œëœë“œ í‚¤ì›Œë“œ PV ì‹œê³„ì—´
        plt.subplot(3, 1, 1)
        df[['brand_keyword_pv', 'non_brand_keyword_pv']].plot(ax=plt.gca(), title='ì¼ë³„ ë¸Œëœë“œ/ë…¼ë¸Œëœë“œ í‚¤ì›Œë“œ PV')
        plt.grid(True)

        # 2. ë¸Œëœë“œ í‚¤ì›Œë“œ PVì™€ ì˜ˆì•½ ì‹ ì²­ ìˆ˜
        plt.subplot(3, 1, 2)
        sns.regplot(x='brand_keyword_pv', y='booking_requests', data=df, scatter_kws={'alpha':0.3})
        plt.title('ë¸Œëœë“œ í‚¤ì›Œë“œ PVì™€ ì˜ˆì•½ ì‹ ì²­ ìˆ˜ì˜ ê´€ê³„')
        plt.grid(True)

        # 3. ë…¼ë¸Œëœë“œ í‚¤ì›Œë“œ PVì™€ ì˜ˆì•½ ì‹ ì²­ ìˆ˜
        plt.subplot(3, 1, 3)
        sns.regplot(x='non_brand_keyword_pv', y='booking_requests', data=df, scatter_kws={'alpha':0.3}, color='g')
        plt.title('ë…¼ë¸Œëœë“œ í‚¤ì›Œë“œ PVì™€ ì˜ˆì•½ ì‹ ì²­ ìˆ˜ì˜ ê´€ê³„')
        plt.grid(True)

        plt.tight_layout()
        save_path = self.analysis_results_path / 'keyword_impact_analysis.png'
        plt.savefig(save_path)
        plt.close()
        logger.info(f"í‚¤ì›Œë“œ ì˜í–¥ë ¥ ë¶„ì„ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")

    def run_analysis(self):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            merged_df = self.load_and_prepare_data()
            corr_matrix, top_corr = self.analyze_correlation(merged_df)
            self.save_analysis_summary_to_md(corr_matrix, top_corr)
            # ê¸°ì¡´ ë‹¤ë¥¸ ë¶„ì„ë“¤ë„ í•„ìš” ì‹œ ì—¬ê¸°ì— ì¶”ê°€ ê°€ëŠ¥
            # self.analyze_conversion_rate(merged_df.copy())
            # self.analyze_keyword_impact(merged_df.set_index('date').copy())
            logger.info("ğŸ‰ ëª¨ë“  ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except FileNotFoundError as e:
            logger.error(e)
        except Exception as e:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)


if __name__ == '__main__':
    # .env íŒŒì¼ ê²½ë¡œë¥¼ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì§€ì •
    dotenv_path = Path(__file__).resolve().parents[2] / '.env'
    load_dotenv(dotenv_path=dotenv_path)
    
    client_list_str = os.getenv("CLIENT_LIST")
    if not client_list_str:
        logger.error(f"'{dotenv_path}' ê²½ë¡œì— '.env' íŒŒì¼ì´ ì—†ê±°ë‚˜ íŒŒì¼ ë‚´ì— 'CLIENT_LIST'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)
        
    try:
        client_list = json.loads(client_list_str)
    except json.JSONDecodeError:
        logger.error("CLIENT_LIST í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆ: '[\"CLIENT1\", \"CLIENT2\"]'")
        sys.exit(1)

    client_name = ""
    if len(client_list) == 1:
        client_name = client_list[0]
        logger.info(f"ìë™ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ '{client_name}'ì— ëŒ€í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("\n=== ë¶„ì„í•  í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš” ===")
        for i, name in enumerate(client_list):
            print(f"  {i + 1}. {name}")
        
        while True:
            try:
                choice = int(input(f"\në²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-{len(client_list)}): "))
                if 1 <= choice <= len(client_list):
                    client_name = client_list[choice - 1]
                    logger.info(f"í´ë¼ì´ì–¸íŠ¸ '{client_name}'ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
                    break
                else:
                    print(f"âŒ 1ì—ì„œ {len(client_list)} ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("âŒ ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    if not client_name:
        logger.error("í´ë¼ì´ì–¸íŠ¸ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(1)
        
    # ë¶„ì„í•  í´ë¼ì´ì–¸íŠ¸ì™€ ê¸°ê°„ ì„¤ì •
    analyzer = PerformanceAnalyzer(
        client_name=client_name,
        start_year=2024,
        start_month=7,
        end_year=2025,
        end_month=7
    )
    analyzer.run_analysis()
