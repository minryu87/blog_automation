import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path
import os
import sys
from dotenv import load_dotenv
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€ (ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•¨)
project_root_for_imports = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(project_root_for_imports))

from blog_automation.place_stat_crawler.scripts.util.logger import logger

class PerformanceAnalyzer:
    def __init__(self, client_name: str, start_year: int, start_month: int, end_year: int, end_month: int):
        self.client_name = client_name
        self.start_year = start_year
        self.start_month = start_month
        self.end_year = end_year
        self.end_month = end_month
        self.base_path = Path(__file__).resolve().parents[2]
        self.processed_data_path = self.base_path / 'data' / 'processed' / client_name
        self.analysis_results_path = self.base_path / 'data' / 'analyzed' / client_name
        self.analysis_results_path.mkdir(parents=True, exist_ok=True)
        
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        plt.rcParams['font.family'] = 'AppleGothic'
        plt.rcParams['axes.unicode_minus'] = False

        # ë¶„ì„í•  ì£¼ìš” ì§€í‘œ ìŒ ì •ì˜
        self.focused_pairs = [
            ("1. ì˜ˆì•½ ì „í™˜ ë¶„ì„", "total_booking_requests", "total_booking_page_visits"),
            ("1. ì˜ˆì•½ ì „í™˜ ë¶„ì„", "total_booking_requests", "booking_visits_ì§€ë„"),
            ("1. ì˜ˆì•½ ì „í™˜ ë¶„ì„", "total_booking_requests", "booking_visits_í”Œë ˆì´ìŠ¤ëª©ë¡"),
            ("1. ì˜ˆì•½ ì „í™˜ ë¶„ì„", "total_booking_page_visits", "total_place_pv"),
            ("2. í”Œë ˆì´ìŠ¤ íŠ¸ë˜í”½ ë¶„ì„", "total_place_pv", "place_pv_ë„¤ì´ë²„ê²€ìƒ‰"),
            ("2. í”Œë ˆì´ìŠ¤ íŠ¸ë˜í”½ ë¶„ì„", "total_place_pv", "place_pv_ë„¤ì´ë²„ì§€ë„"),
            ("2. í”Œë ˆì´ìŠ¤ íŠ¸ë˜í”½ ë¶„ì„", "total_place_pv", "keyword_pv_type1_brand_like"),
            ("2. í”Œë ˆì´ìŠ¤ íŠ¸ë˜í”½ ë¶„ì„", "total_place_pv", "keyword_pv_type2_others"),
        ]

    def load_and_prepare_data(self) -> pd.DataFrame:
        """ì§€ì •ëœ ê¸°ê°„ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë¶„ì„ì— ë§ê²Œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        logger.info("ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        all_place_pv_df = []
        all_booking_df = []

        for year in range(self.start_year, self.end_year + 1):
            s_month = self.start_month if year == self.start_year else 1
            e_month = self.end_month if year == self.end_year else 12

            for month in range(s_month, e_month + 1):
                month_str = f"{month:02d}"
                pv_file = self.processed_data_path / f"{self.client_name}_{year}_{month_str}_integrated_statistics.csv"
                if pv_file.exists():
                    logger.info(f"ë¡œë”© (PV): {pv_file.name}")
                    all_place_pv_df.append(pd.read_csv(pv_file))
                else:
                    logger.warning(f"íŒŒì¼ ì—†ìŒ (PV): {pv_file.name}")

                booking_file = self.processed_data_path / f"{self.client_name}_{year}_{month_str}_booking_integrated_statistics.csv"
                if booking_file.exists():
                    logger.info(f"ë¡œë”© (Booking): {booking_file.name}")
                    all_booking_df.append(pd.read_csv(booking_file))
                else:
                    logger.warning(f"íŒŒì¼ ì—†ìŒ (Booking): {booking_file.name}")

        if not all_place_pv_df or not all_booking_df:
            raise FileNotFoundError("ë¶„ì„ì— í•„ìš”í•œ ë°ì´í„° íŒŒì¼ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
        place_pv_df = pd.concat(all_place_pv_df, ignore_index=True)
        booking_df = pd.concat(all_booking_df, ignore_index=True)

        place_pv_df['date'] = pd.to_datetime(place_pv_df['date'])
        booking_df['date'] = pd.to_datetime(booking_df['date'])
        
        # 1. ì¼ë³„ ì´ í”Œë ˆì´ìŠ¤ ì¡°íšŒìˆ˜
        daily_total_pv = place_pv_df[place_pv_df['data_type'] == 'channel'].groupby('date')['pv'].sum().reset_index()
        daily_total_pv.rename(columns={'pv': 'total_place_pv'}, inplace=True)

        # 2. í”Œë ˆì´ìŠ¤ í˜ì´ì§€ ì±„ë„ë³„ ì¡°íšŒìˆ˜
        place_channel_pv = place_pv_df[place_pv_df['data_type'] == 'channel'].pivot_table(
            index='date', columns='name', values='pv', aggfunc='sum').add_prefix('place_pv_')
        
        # 3. í‚¤ì›Œë“œ ìœ í˜•ë³„ PV
        keyword_pv = self.extract_keyword_data(place_pv_df)
        
        # 4. ì˜ˆì•½ í˜ì´ì§€ ì´ ìœ ì…ìˆ˜ ë° ì˜ˆì•½ ì‹ ì²­ ìˆ˜
        booking_summary = booking_df.groupby('date').agg(
            total_booking_page_visits=('page_visits', 'first'),
            total_booking_requests=('booking_requests', 'first')
        ).reset_index()

        # 5. ì˜ˆì•½ í˜ì´ì§€ ì±„ë„ë³„ ìœ ì…ìˆ˜
        booking_channel_visits = booking_df.pivot_table(
            index='date', columns='channel_name', values='channel_count', aggfunc='sum').add_prefix('booking_visits_')

        # ëª¨ë“  ë°ì´í„° ë³‘í•©
        merged_df = daily_total_pv
        for df in [place_channel_pv, keyword_pv, booking_summary, booking_channel_visits]:
            merged_df = pd.merge(merged_df, df, on='date', how='left')
            
        return merged_df.fillna(0)

    def extract_keyword_data(self, place_pv_df: pd.DataFrame) -> pd.DataFrame:
        """í‚¤ì›Œë“œë¥¼ ìœ í˜•ë³„ë¡œ ë¶„ë¥˜í•˜ê³  PVë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤."""
        keyword_df = place_pv_df[place_pv_df['data_type'] == 'keyword'].copy()
        
        def classify_keyword(k):
            k_str = str(k).lower()
            # í´ë¼ì´ì–¸íŠ¸ë³„ ë¸Œëœë“œ í‚¤ì›Œë“œë¥¼ ì—¬ê¸°ì„œ ë™ì ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ì§€ê¸ˆì€ í•˜ë“œì½”ë”©ëœ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            if self.client_name == 'GOODMORNINGHANIGURO':
                brand_keywords = ['ì•„ì¹¨', '285']
            elif self.client_name == 'NATENCLINIC':
                brand_keywords = ['ë‚´ì´íŠ¼', 'ë„¤ì´íŠ¼']
            else:
                brand_keywords = []

            if any(kw in k_str for kw in brand_keywords):
                return 'type1_brand_like'
            return 'type2_others'

        keyword_df['keyword_type'] = keyword_df['name'].apply(classify_keyword)
        
        keyword_summary = keyword_df.pivot_table(
            index='date', columns='keyword_type', values='pv', aggfunc='sum'
        ).add_prefix('keyword_pv_')
        
        return keyword_summary.reset_index()
        
    def _calculate_correlations(self, df: pd.DataFrame, pairs: list) -> dict:
        """ì •ì˜ëœ ìŒì— ëŒ€í•œ ìƒê´€ê³„ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        results = {}
        for category, col1, col2 in pairs:
            pair_key = f"{col1} vs {col2}"
            # ë°ì´í„°í”„ë ˆì„ì— ë‘ ì»¬ëŸ¼ì´ ëª¨ë‘ ì¡´ì¬í•  ê²½ìš°ì—ë§Œ ê³„ì‚°
            if col1 in df.columns and col2 in df.columns and df[col1].nunique() > 1 and df[col2].nunique() > 1:
                correlation = df[col1].corr(df[col2])
                results[pair_key] = {'category': category, 'correlation': correlation}
            else:
                results[pair_key] = {'category': category, 'correlation': np.nan}
        return results

    def run_focused_analysis(self, df: pd.DataFrame):
        """ìš”ì²­ëœ í˜•ì‹ì˜ ë¦¬í¬íŠ¸ë¥¼ ìœ„í•œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        logger.info("í¬ì»¤ìŠ¤ ë¶„ì„(ì›”ë³„, ì•ˆì •ì„±)ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ì „ì²´ ê¸°ê°„ ìƒê´€ê´€ê³„
        overall_corr_results = self._calculate_correlations(df, self.focused_pairs)
        overall_corr_df = pd.DataFrame.from_dict(overall_corr_results, orient='index').reset_index().rename(columns={'index': 'pair'})
        
        # ì›”ë³„ ìƒê´€ê´€ê³„
        df_monthly = df.set_index('date').groupby(pd.Grouper(freq='M'))
        monthly_corr_list = []

        for month, group in df_monthly:
            if len(group) < 2: continue # ë°ì´í„°ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ ìƒê´€ê´€ê³„ ê³„ì‚° ë¶ˆê°€
            month_str = month.strftime('%Y-%m')
            monthly_results = self._calculate_correlations(group, self.focused_pairs)
            for pair, values in monthly_results.items():
                monthly_corr_list.append({
                    'month': month_str,
                    'category': values['category'],
                    'pair': pair,
                    'correlation': values['correlation']
                })
        
        monthly_corr_df = pd.DataFrame(monthly_corr_list)

        # ì•ˆì •ì„± ë¶„ì„ (í‘œì¤€í¸ì°¨)
        stability_df = monthly_corr_df.groupby('pair')['correlation'].std().reset_index()
        stability_df.rename(columns={'correlation': 'std_dev'}, inplace=True)
        stability_df = stability_df.sort_values(by='std_dev').dropna()

        self.plot_monthly_correlation_trends(monthly_corr_df)

        return overall_corr_df, stability_df, monthly_corr_df

    def plot_monthly_correlation_trends(self, monthly_corr_df: pd.DataFrame):
        """ì›”ë³„ ìƒê´€ê´€ê³„ íŠ¸ë Œë“œë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
        logger.info("ì›”ë³„ ìƒê´€ê´€ê³„ íŠ¸ë Œë“œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        
        pivot_df = monthly_corr_df.pivot(index='month', columns='pair', values='correlation')
        
        plt.figure(figsize=(20, 12))
        for column in pivot_df.columns:
            plt.plot(pivot_df.index, pivot_df[column], marker='o', linestyle='-', label=column)
            
        plt.title('ì›”ë³„ ì£¼ìš” ì§€í‘œ ìƒê´€ê´€ê³„ íŠ¸ë Œë“œ', fontsize=20)
        plt.xlabel('ì›”')
        plt.ylabel('ìƒê´€ê³„ìˆ˜')
        plt.xticks(rotation=45)
        plt.grid(True, which='both', linestyle='--', linewidth=0.5)
        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
        plt.tight_layout(rect=[0, 0, 0.85, 1])
        
        save_path = self.analysis_results_path / 'monthly_correlation_trends.png'
        plt.savefig(save_path)
        plt.close()
        logger.info(f"ì›”ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")

    def save_analysis_summary_to_md(self, overall_corr, stability_df, monthly_corr_df):
        """ìš”ì²­ëœ í˜•ì‹ì— ë§ì¶° ë¶„ì„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        logger.info("ìƒˆë¡œìš´ í˜•ì‹ì˜ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")

        md_content = f"""# {self.client_name} ë§ˆì¼€íŒ… í¼ë„ ìƒê´€ê´€ê³„ ë¶„ì„ ë¦¬í¬íŠ¸

## 1. ì „ì²´ ê¸°ê°„ ì£¼ìš” í¼ë„ ë‹¨ê³„ë³„ ìƒê´€ê´€ê³„

ì „ì²´ ë¶„ì„ ê¸°ê°„ë™ì•ˆì˜ í‰ê· ì ì¸ ê´€ê³„ì…ë‹ˆë‹¤.

{overall_corr.to_markdown(index=False)}

## 2. ìƒê´€ê´€ê³„ ì•ˆì •ì„± ë¶„ì„ (ë³€ë™ì„± ë‚®ì€ ìˆœ)

ì›”ë³„ ìƒê´€ê´€ê³„ì˜ í‘œì¤€í¸ì°¨ì…ë‹ˆë‹¤. ê°’ì´ ë‚®ì„ìˆ˜ë¡ ê¸°ê°„ì— ìƒê´€ì—†ì´ ê¾¸ì¤€í•˜ê³  ì•ˆì •ì ì¸ ê´€ê³„ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

{stability_df.to_markdown(index=False)}

## 3. ì›”ë³„ ìƒê´€ê´€ê³„ íŠ¸ë Œë“œ

ì£¼ìš” ê´€ê³„ë“¤ì´ ì›”ë³„ë¡œ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹ì • ë§ˆì¼€íŒ… í™œë™ì´ë‚˜ ì´ë²¤íŠ¸ì™€ì˜ ì—°ê´€ì„±ì„ íŒŒì•…í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.

![ì›”ë³„ ìƒê´€ê´€ê³„ íŠ¸ë Œë“œ](monthly_correlation_trends.png)

## 4. ì›”ë³„ ìƒê´€ê´€ê³„ ìƒì„¸ ë°ì´í„°

{monthly_corr_df.sort_values(by=['month', 'category']).to_markdown(index=False)}
"""
        report_path = self.analysis_results_path / f'{self.client_name}_focused_analysis_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
            
        logger.info(f"ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_path}")

    def run_analysis(self):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            merged_df = self.load_and_prepare_data()
            overall_corr, stability_df, monthly_corr_df = self.run_focused_analysis(merged_df)
            self.save_analysis_summary_to_md(overall_corr, stability_df, monthly_corr_df)

            logger.info("ğŸ‰ ëª¨ë“  ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except FileNotFoundError as e:
            logger.error(e)
        except Exception as e:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)


if __name__ == '__main__':
    # .env íŒŒì¼ ê²½ë¡œë¥¼ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì§€ì •
    dotenv_path = Path(__file__).resolve().parents[2] / '.env'
    load_dotenv(dotenv_path=dotenv_path)
    
    client_list_str = os.getenv("CLIENT_LIST")
    if not client_list_str:
        logger.error(f"'{dotenv_path}' ê²½ë¡œì— '.env' íŒŒì¼ì´ ì—†ê±°ë‚˜ íŒŒì¼ ë‚´ì— 'CLIENT_LIST'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)
        
    try:
        client_list = json.loads(client_list_str)
    except json.JSONDecodeError:
        logger.error("CLIENT_LIST í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆ: '[\"CLIENT1\", \"CLIENT2\"]'")
        sys.exit(1)

    client_name = ""
    if len(client_list) == 1:
        client_name = client_list[0]
        logger.info(f"ìë™ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ '{client_name}'ì— ëŒ€í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("\n=== ë¶„ì„í•  í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš” ===")
        for i, name in enumerate(client_list):
            print(f"  {i + 1}. {name}")
        
        while True:
            try:
                choice = int(input(f"\në²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-{len(client_list)}): "))
                if 1 <= choice <= len(client_list):
                    client_name = client_list[choice - 1]
                    logger.info(f"í´ë¼ì´ì–¸íŠ¸ '{client_name}'ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
                    break
                else:
                    print(f"âŒ 1ì—ì„œ {len(client_list)} ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("âŒ ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    if not client_name:
        logger.error("í´ë¼ì´ì–¸íŠ¸ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(1)
        
    # ë¶„ì„í•  í´ë¼ì´ì–¸íŠ¸ì™€ ê¸°ê°„ ì„¤ì •
    analyzer = PerformanceAnalyzer(
        client_name=client_name,
        start_year=2024,
        start_month=9,
        end_year=2025,
        end_month=7
    )
    analyzer.run_analysis()