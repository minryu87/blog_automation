{
  "timestamp": "2025-07-22T04:17:13.449008",
  "attempt": 1,
  "status": "success",
  "feature_name": "query_intent_satisfaction_vs_competitors",
  "hypothesis": "우리 포스트가 대표 검색어와의 의미적 유사도(검색 의도 충족도로 간주)가 경쟁사 포스트 그룹의 평균 유사도보다 높을수록, 사용자는 우리 포스트가 더 관련성 높다고 판단하여 클릭할 가능성이 커지므로, `non_brand_average_ctr`이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # A good multilingual model for general semantic similarity tasks.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well our post satisfies query intent compared to competitors.\n\n    The feature is a ratio:\n    (Similarity of our post to query) / (Average similarity of competitor posts to query)\n\n    Args:\n        df: The input DataFrame with columns 'post_title', 'post_body',\n            'representative_query', and 'source'.\n\n    Returns:\n        The DataFrame with the new feature column 'query_intent_satisfaction_vs_competitors'.\n    \"\"\"\n    if df.empty or 'representative_query' not in df.columns or 'source' not in df.columns:\n        df['query_intent_satisfaction_vs_competitors'] = 1.0\n        return df\n\n    model = get_model()\n    \n    # Preserve the original index to ensure correct assignment at the end.\n    df_processed = df.copy()\n\n    # 2. Prepare data for encoding\n    # Combine title and body for a holistic representation of content.\n    # Fill NaNs to prevent errors during string operations and encoding.\n    df_processed['content'] = df_processed['post_title'].fillna('') + ' ' + df_processed['post_body'].fillna('')\n    queries = df_processed['representative_query'].fillna('').tolist()\n    contents = df_processed['content'].tolist()\n\n    # 3. Efficiently encode all data in batches\n    query_embeddings = model.encode(queries, convert_to_tensor=True)\n    content_embeddings = model.encode(contents, convert_to_tensor=True)\n\n    # 4. Calculate semantic similarity for each post against its own query\n    # .diag() provides the element-wise cosine similarity between aligned embeddings.\n    similarities = util.cos_sim(query_embeddings, content_embeddings).diag()\n    df_processed['query_content_similarity'] = similarities.cpu().numpy()\n\n    # 5. Calculate the feature score for each query group\n    ratio_map = {}\n    for query, group in df_processed.groupby('representative_query'):\n        if not query:\n            continue\n\n        our_rows = group[group['source'] == 'ours']\n        competitor_rows = group[group['source'] != 'ours']\n\n        # Proceed only if both our post and competitor posts exist for the query\n        if our_rows.empty or competitor_rows.empty:\n            continue\n\n        # In case of multiple 'our' posts for a query, take the average.\n        our_score = our_rows['query_content_similarity'].mean()\n        competitor_avg_score = competitor_rows['query_content_similarity'].mean()\n        \n        # Calculate the ratio, handling division by zero.\n        if competitor_avg_score > 0:\n            ratio = our_score / competitor_avg_score\n        else:\n            # If competitor average similarity is zero, our non-zero score is infinitely better.\n            # We cap it at a high value, or 1.0 if our score is also zero.\n            ratio = 5.0 if our_score > 0 else 1.0\n\n        ratio_map[query] = ratio\n\n    # 6. Map the calculated ratios back to the original DataFrame structure\n    df['query_intent_satisfaction_vs_competitors'] = df['representative_query'].map(ratio_map)\n\n    # Fill with a neutral value (1.0) for queries that couldn't be processed (e.g., no competitors)\n    df['query_intent_satisfaction_vs_competitors'].fillna(1.0, inplace=True)\n\n    return df\n",
  "analysis": {
    "correlation": 0.15751084812461594,
    "p_value": 0.12740008519180987,
    "interpretation": "약한 양의 상관관계(0.1575)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.1274)."
  }
}{
  "timestamp": "2025-07-22T04:35:13.529218",
  "attempt": 1,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'relative_query_satisfaction_score'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # A good multilingual model for semantic similarity.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well our post satisfies the query's semantic intent relative to competitors.\n\n    The score is a ratio: (Our Post's Query-Content Similarity) / (Competitors' Avg Query-Content Similarity)\n    A score > 1 means our post is more semantically relevant to the query than the competition's average.\n    \"\"\"\n    feature_name = 'relative_query_satisfaction_score'\n    df[feature_name] = np.nan\n\n    if df.empty:\n        return df\n\n    try:\n        model = get_model()\n\n        # 2. Prepare data for efficient batch processing.\n        # Preserve the original index to prevent misalignment.\n        df_copy = df.copy()\n        queries = df_copy['representative_query'].fillna('').astype(str).tolist()\n        # Combine title and body for a comprehensive content representation.\n        contents = (df_copy['post_title'].fillna('') + ' ' + df_copy['post_body'].fillna('')).astype(str).tolist()\n\n        # 3. Encode all texts in two efficient batches.\n        query_embeddings = model.encode(queries, convert_to_tensor=True, show_progress_bar=False)\n        content_embeddings = model.encode(contents, convert_to_tensor=True, show_progress_bar=False)\n\n        # Create a mapping from the original DataFrame index to the tensor's positional index.\n        # This is a robust way to avoid index-related errors.\n        df_index_to_pos = pd.Series(range(len(df_copy)), index=df_copy.index)\n\n        results = {}\n\n        # 4. Group by query to perform relational analysis.\n        for query, group in df_copy.groupby('representative_query'):\n            if pd.isna(query) or query == '':\n                continue\n\n            # Identify 'ours' and 'competitors' within the group by their original index.\n            our_rows = group[group['source'] == 'ours']\n            competitor_rows = group[group['source'] == 'competitors']\n\n            # This feature requires both our post and competitor posts for comparison.\n            if our_rows.empty or competitor_rows.empty:\n                continue\n\n            # 5. Get tensor positions using the pre-built map. This is safer than using group indices directly.\n            our_positions = df_index_to_pos.loc[our_rows.index].tolist()\n            competitor_positions = df_index_to_pos.loc[competitor_rows.index].tolist()\n\n            # The query is the same for the entire group, so we can use the first row's position.\n            query_pos = df_index_to_pos.loc[group.index[0]]\n            \n            # Slice the embedding tensors to get embeddings for the current group.\n            group_query_embedding = query_embeddings[query_pos:query_pos+1] # Keep 2D for util.cos_sim\n            our_content_embeddings = content_embeddings[our_positions]\n            competitor_content_embeddings = content_embeddings[competitor_positions]\n\n            # 6. Calculate semantic similarity scores.\n            our_sim_scores = util.cos_sim(group_query_embedding, our_content_embeddings).flatten()\n            competitor_sim_scores = util.cos_sim(group_query_embedding, competitor_content_embeddings).flatten()\n            \n            # Calculate the average satisfaction score for competitors.\n            competitor_avg_score = competitor_sim_scores.mean().item()\n\n            # Avoid division by zero if competitors have zero similarity.\n            if competitor_avg_score == 0:\n                continue\n\n            # 7. Calculate the final ratio and store it.\n            # We iterate through each of 'our' posts in the group (usually just one).\n            for i, original_index in enumerate(our_rows.index):\n                our_score = our_sim_scores[i].item()\n                relative_score = our_score / competitor_avg_score\n                results[original_index] = relative_score\n        \n        # 8. Assign all calculated results to the DataFrame at once.\n        if results:\n            df[feature_name] = df.index.map(results).astype('float64')\n\n    except Exception as e:\n        print(f\"Error generating feature '{feature_name}': {e}\")\n        # Return the dataframe with the empty feature column in case of an error.\n        df[feature_name] = np.nan\n\n    return df"
}{
  "timestamp": "2025-07-22T04:36:07.353225",
  "attempt": 2,
  "status": "success",
  "feature_name": "relative_query_intent_fulfillment",
  "hypothesis": "우리 포스트가 경쟁자 그룹의 평균보다 대표 검색어의 의도를 더 충실하게 반영할수록 (즉, 포스트 본문과 검색어 간의 의미적 유사도가 높을수록), 사용자의 검색 의도 만족도가 높아져 비브랜드 평균 CTR이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton instance of the SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well our post fulfills the query intent compared to competitors.\n\n    For each 'representative_query' group, this feature computes:\n    1. The semantic similarity between each of 'our' posts and the query.\n    2. The average semantic similarity between all competitor posts and the query.\n    3. The final feature is the ratio of our post's similarity to the competitors' average similarity.\n       This creates a post-specific score, ensuring variance.\n    \"\"\"\n    # 2. Always check for an empty DataFrame first.\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        return df\n\n    # Ensure the required columns exist\n    required_columns = ['representative_query', 'post_body', 'source']\n    if not all(col in df.columns for col in required_columns):\n        # If essential columns are missing, return the original df with a warning or log\n        # For this context, we'll add a placeholder column if it doesn't exist.\n        if 'relative_query_intent_fulfillment' not in df.columns:\n            df['relative_query_intent_fulfillment'] = np.nan\n        return df\n\n    model = get_model()\n\n    # Initialize the feature column with a default value\n    df['relative_query_intent_fulfillment'] = np.nan\n\n    # Group by the representative query to perform analysis within the same search context\n    grouped = df.groupby('representative_query')\n\n    for query, group_df in grouped:\n        # Separate our posts from competitor posts\n        # IMPORTANT: Keep the original DataFrame index to assign results correctly\n        ours_df = group_df[group_df['source'] == 'ours']\n        competitors_df = group_df[group_df['source'] == 'competitor']\n\n        # Skip if there are no 'ours' posts or no competitors to benchmark against\n        if ours_df.empty or competitors_df.empty:\n            continue\n\n        # Prepare texts for efficient batch encoding. Combine all into one list.\n        query_text = [str(query)]\n        our_bodies = ours_df['post_body'].fillna('').astype(str).tolist()\n        competitor_bodies = competitors_df['post_body'].fillna('').astype(str).tolist()\n\n        all_texts = query_text + our_bodies + competitor_bodies\n\n        # 3. Use efficient batch processing for all texts in the group\n        embeddings = model.encode(all_texts, convert_to_tensor=True)\n\n        # Separate the embeddings back into their respective groups\n        query_embedding = embeddings[0]\n        ours_embeddings = embeddings[1 : 1 + len(our_bodies)]\n        competitors_embeddings = embeddings[1 + len(our_bodies) :]\n\n        # Calculate cosine similarity between the query and each post body\n        our_scores = util.cos_sim(query_embedding, ours_embeddings).flatten()\n        competitor_scores = util.cos_sim(query_embedding, competitors_embeddings).flatten()\n\n        # Calculate the average score for the competitor group\n        if len(competitor_scores) > 0:\n            competitor_benchmark_score = competitor_scores.mean().item()\n        else:\n             # This case is already handled by the empty check, but as a safeguard:\n            continue\n\n        # Avoid division by zero\n        if competitor_benchmark_score == 0:\n            # If competitors have zero similarity, any positive score from us is infinitely better.\n            # We can assign a large number, but a neutral 1.0 is safer.\n            # Or we can calculate the simple score without comparison.\n            # Here, we will just assign the raw score for our posts.\n            relative_scores = our_scores.cpu().numpy()\n        else:\n            # This is the core logic: calculate the ratio for each of our posts\n            # This ensures that each 'our' post gets a unique value, creating variance.\n            relative_scores = (our_scores / competitor_benchmark_score).cpu().numpy()\n\n        # 4. Assign the calculated feature values back to the original DataFrame using the preserved index\n        df.loc[ours_df.index, 'relative_query_intent_fulfillment'] = relative_scores\n\n    # 5. Return the full DataFrame with the new column.\n    return df",
  "analysis": {
    "correlation": 0.29365724695451556,
    "p_value": 0.0038733826518317404,
    "interpretation": "약한 양의 상관관계(0.2937)를 발견했습니다. 이 결과는 통계적으로 유의미합니다(p-value: 0.0039)."
  }
}{
  "timestamp": "2025-07-22T04:57:46.484206",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_query_intent_fulfillment",
  "hypothesis": "우리 포스트의 내용(제목+본문)이 '대표 검색어'와 의미적으로 더 가까울수록, 즉 검색 의도를 더 잘 충족시킬수록, 경쟁사 그룹 평균 대비 클릭률이 높을 것이다. 이 상대적인 의도 충족도를 측정하면 `non_brand_average_ctr`과 양의 상관관계를 보일 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well our post satisfies a query's intent relative to competitors.\n\n    1.  For each 'representative_query' group, calculate the semantic similarity between the query\n        and the content of every post (ours and competitors). This is the 'intent_score'.\n    2.  For the 'ours' post, find its 'intent_score'.\n    3.  Calculate the average 'intent_score' for all 'competitor' posts.\n    4.  The final feature is the ratio: (Our Score / Competitor Average Score).\n        A value > 1 means our post is more aligned with the query intent than the average competitor.\n    \"\"\"\n    # 2. ALWAYS check for empty DataFrames.\n    if not df.empty:\n        pass\n    else:\n        df['relative_query_intent_fulfillment'] = pd.Series(dtype=float)\n        return df\n\n    model = get_model()\n\n    # Preserve the original index to ensure correct final assignment.\n    original_index = df.index\n\n    # Combine title and body for a more comprehensive content representation.\n    df_copy = df.copy()\n    df_copy['full_text'] = df_copy['post_title'].fillna('') + ' ' + df_copy['post_body'].fillna('')\n    \n    # Prepare texts for efficient batch encoding.\n    all_texts = df_copy['full_text'].tolist()\n    unique_queries = df_copy['representative_query'].dropna().unique().tolist()\n    \n    if not unique_queries:\n        df['relative_query_intent_fulfillment'] = 1.0\n        return df\n\n    # 3. Use efficient batch processing for all texts and queries.\n    text_embeddings = model.encode(all_texts, convert_to_tensor=True, show_progress_bar=False)\n    query_embeddings = model.encode(unique_queries, convert_to_tensor=True, show_progress_bar=False)\n\n    query_embedding_map = {query: emb for query, emb in zip(unique_queries, query_embeddings)}\n\n    # Series to store results, aligned with the original DataFrame index.\n    results = pd.Series(index=original_index, dtype=float)\n\n    # Process each query group.\n    for query, group_df in df_copy.groupby('representative_query'):\n        if pd.isna(query) or query not in query_embedding_map:\n            continue\n\n        query_emb = query_embedding_map[query]\n\n        # Get integer locations of group rows in the original DataFrame to slice the tensor.\n        group_indices_loc = df_copy.index.get_indexer(group_df.index)\n        group_text_embeddings = text_embeddings[group_indices_loc]\n\n        # Calculate semantic similarity (intent satisfaction score).\n        intent_scores = util.cos_sim(query_emb, group_text_embeddings).flatten()\n        group_scores = pd.Series(intent_scores.cpu().numpy(), index=group_df.index)\n\n        # Separate 'ours' from 'competitors'.\n        ours_posts = group_df[group_df['source'] == 'ours']\n        competitor_posts = group_df[group_df['source'] == 'competitor']\n\n        if ours_posts.empty:\n            continue\n\n        # Get our post's score.\n        our_score = group_scores.loc[ours_posts.index].mean() # Use mean() in case of multiple 'ours' posts.\n\n        # Calculate average competitor score, handling the case of no competitors.\n        if not competitor_posts.empty:\n            competitor_avg_score = group_scores.loc[competitor_posts.index].mean()\n        else:\n            # If no competitors, our relative score is undefined. A neutral value of 1.0 is a safe default.\n            competitor_avg_score = our_score\n\n        # Calculate the relative fulfillment ratio, avoiding division by zero.\n        if competitor_avg_score > 1e-9:\n            relative_fulfillment = our_score / competitor_avg_score\n        else:\n            # If competitors have zero alignment, our fulfillment is infinitely better.\n            # We cap this to prevent extreme values. 1.0 if our score is also near zero, else a high value.\n            relative_fulfillment = 1.0 if our_score < 1e-9 else 5.0 \n\n        # Assign the result to all 'ours' posts in the group.\n        results.loc[ours_posts.index] = relative_fulfillment\n\n    # 4. Return the full DataFrame with the new column.\n    # The feature is only meaningful for 'ours' posts. Fill competitors with a neutral 1.0.\n    df['relative_query_intent_fulfillment'] = results.fillna(1.0)\n\n    return df",
  "analysis": {
    "correlation": 0.15751070551138116,
    "p_value": 0.12740043412438135,
    "interpretation": "약한 양의 상관관계(0.1575)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.1274)."
  }
}{
  "timestamp": "2025-07-22T05:18:04.598638",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_query_fulfillment_score",
  "hypothesis": "우리 포스트가 경쟁사 포스트 그룹보다 대표 검색어의 의미적 의도를 더 잘 충족시킬수록(즉, 검색어와 본문 간의 의미적 유사도가 높을수록), 사용자의 검색 의도에 더 부합하여 비브랜드 평균 CTR이 높아질 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Use a multilingual model suitable for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well a post fulfills the semantic intent of its representative query,\n    relative to its competitors for the same query.\n    \"\"\"\n    # 0. Handle empty DataFrame\n    if df.empty:\n        df['relative_query_fulfillment_score'] = np.nan\n        return df\n\n    # 1. Initialize model and prepare data\n    model = get_model()\n    \n    # Ensure required columns are present and have the correct type\n    df['post_body'] = df['post_body'].fillna('').astype(str)\n    df['representative_query'] = df['representative_query'].fillna('').astype(str)\n\n    # 2. Efficiently encode unique queries and all post bodies in batches\n    unique_queries = df['representative_query'].unique().tolist()\n    post_bodies = df['post_body'].tolist()\n\n    # Encode to tensors for GPU acceleration if available\n    query_embeddings = model.encode(unique_queries, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(post_bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # 3. Calculate individual fulfillment scores (semantic similarity between each post and its query)\n    query_embedding_map = {query: emb for query, emb in zip(unique_queries, query_embeddings)}\n    \n    # Create a tensor of query embeddings ordered to match the DataFrame rows\n    # This is a critical step for correct 1-to-1 comparison\n    ordered_query_embeddings = torch.stack([query_embedding_map[q] for q in df['representative_query']])\n    \n    # Calculate cosine similarity between each body and its corresponding query embedding\n    # .diag() extracts the one-to-one similarities from the similarity matrix\n    fulfillment_scores = util.cos_sim(body_embeddings, ordered_query_embeddings).diag()\n    df['temp_fulfillment_score'] = fulfillment_scores.cpu().numpy()\n\n    # 4. Calculate the average fulfillment score for 'ours' vs. 'competitor' within each query group\n    # This creates a per-query benchmark\n    avg_scores = df.groupby(['representative_query', 'source'])['temp_fulfillment_score'].mean().unstack()\n\n    # Handle cases where a query group might be missing 'ours' or 'competitor' posts\n    if 'ours' not in avg_scores.columns:\n        avg_scores['ours'] = np.nan\n    if 'competitor' not in avg_scores.columns:\n        avg_scores['competitor'] = np.nan\n\n    # 5. Calculate the relative fulfillment score (ours / competitor)\n    # A score > 1 means our posts are more relevant to the query than competitors, on average.\n    # A score < 1 means our posts are less relevant.\n    # Fill with 1.0 (parity) if competitor score is missing/zero or if our score is missing.\n    relative_scores = (avg_scores['ours'] / avg_scores['competitor']).fillna(1.0)\n    # Handle infinite values that can arise if competitor score is 0 but ours is not\n    relative_scores.replace([np.inf, -np.inf], 1.0, inplace=True)\n\n    # 6. Map the calculated group-level relative score back to each post in the DataFrame\n    # This ensures every post within the same query group gets the same relative score.\n    df['relative_query_fulfillment_score'] = df['representative_query'].map(relative_scores)\n    # Final fillna for any queries that might not have been in the original mapping\n    df['relative_query_fulfillment_score'].fillna(1.0, inplace=True)\n\n    # 7. Clean up and return\n    df.drop(columns=['temp_fulfillment_score'], inplace=True)\n\n    return df",
  "analysis": {
    "correlation": 0.3116492175023857,
    "p_value": 0.0021098459808704464,
    "interpretation": "중간 정도의 양의 상관관계(0.3116)를 발견했습니다. 이 결과는 통계적으로 유의미합니다(p-value: 0.0021)."
  }
}{
  "timestamp": "2025-07-22T05:40:28.569392",
  "attempt": 1,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'query_intent_satisfaction_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model that supports Korean and is well-regarded.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a feature that compares how well our post satisfies the query intent\n    versus the competitor average for the same query.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with columns 'representative_query',\n                           'post_body', and 'source'.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the new feature column\n                      'query_intent_satisfaction_vs_competitors'.\n    \"\"\"\n    feature_name = 'query_intent_satisfaction_vs_competitors'\n    \n    # 4. ALWAYS check for empty DataFrames.\n    if not df.empty:\n        df_copy = df.copy()\n    else:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n\n    model = get_model()\n\n    # Pre-process text columns to handle potential NaN values.\n    df_copy['representative_query'] = df_copy['representative_query'].fillna('').astype(str)\n    df_copy['post_body'] = df_copy['post_body'].fillna('').astype(str)\n\n    # 2. Use efficient batch processing.\n    # Encode unique queries to avoid redundant computations.\n    unique_queries = df_copy['representative_query'].unique().tolist()\n    query_embeddings_tensor = model.encode(unique_queries, convert_to_tensor=True, show_progress_bar=False)\n    query_to_embedding = {query: emb for query, emb in zip(unique_queries, query_embeddings_tensor)}\n\n    # Encode all post bodies at once.\n    body_embeddings_tensor = model.encode(df_copy['post_body'].tolist(), convert_to_tensor=True, show_progress_bar=False)\n\n    results = pd.Series(index=df.index, dtype=float, name=feature_name)\n\n    # Group by the representative query to perform the comparison.\n    for query, group in df_copy.groupby('representative_query'):\n        if not query or query not in query_to_embedding:\n            results.loc[group.index] = np.nan\n            continue\n\n        query_embedding = query_to_embedding[query]\n\n        # Identify 'ours' vs 'competitors' within the group.\n        our_rows = group[group['source'] == 'ours']\n        competitor_rows = group[group['source'] == 'competitors']\n\n        # If either group is missing, the comparison is not possible.\n        if our_rows.empty or competitor_rows.empty:\n            results.loc[group.index] = np.nan\n            continue\n\n        # SELF-CORRECTION from past mistakes: Use original DataFrame's index to get correct integer\n        # positions for slicing the embedding tensor, preventing data misalignment.\n        our_locs = df.index.get_indexer_for(our_rows.index)\n        competitor_locs = df.index.get_indexer_for(competitor_rows.index)\n        \n        # Slice the main embeddings tensor using the correct positional indices.\n        our_body_embeddings = body_embeddings_tensor[our_locs]\n        competitor_body_embeddings = body_embeddings_tensor[competitor_locs]\n\n        # Calculate cosine similarity between the query and post bodies.\n        our_similarities = util.cos_sim(query_embedding, our_body_embeddings)\n        competitor_similarities = util.cos_sim(query_embedding, competitor_body_embeddings)\n\n        # Calculate the average satisfaction score for each group.\n        our_avg_score = our_similarities.mean().item()\n        competitor_avg_score = competitor_similarities.mean().item()\n\n        # Calculate the final ratio, handling division by zero.\n        if competitor_avg_score > 1e-9:\n            satisfaction_ratio = our_avg_score / competitor_avg_score\n        else:\n            # If competitor score is zero, the ratio is undefined or infinite.\n            # Assigning a large number or np.nan are options. np.nan is safer for models.\n            satisfaction_ratio = np.nan\n        \n        # Assign the calculated group-level ratio to all rows in the group.\n        results.loc[group.index] = satisfaction_ratio\n\n    # 3. Return the full DataFrame with the new column.\n    df_copy[feature_name] = results\n    return df_copy"
}{
  "timestamp": "2025-07-22T05:41:12.166989",
  "attempt": 2,
  "status": "success",
  "feature_name": "relative_query_intent_fulfillment",
  "hypothesis": "우리 포스트의 내용이 대표 검색어와 의미적으로 더 가까울수록(검색 의도를 더 잘 충족할수록), 그리고 이 충족도가 경쟁 포스트 그룹의 평균보다 높을수록, non-brand CTR이 높을 것이다. 이 피처는 각 '우리' 포스트의 고유한 충족도를 경쟁 그룹 평균과 비교하여 개별적인 관계 점수를 생성함으로써, 이전 시도의 '모든 값이 동일한' 문제를 해결한다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Use a model that is effective for Korean and general semantic similarity.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well our post's content fulfills the search intent of the representative query,\n    relative to the average fulfillment of competitor posts for the same query.\n\n    This addresses the previous logical error where the feature had a constant value.\n    By calculating the fulfillment score for EACH 'ours' post individually before comparing\n    it to the competitor average for that query, this version ensures variance in the final feature.\n    \"\"\"\n    # Always check for an empty DataFrame first.\n    if df.empty:\n        return df\n\n    feature_name = 'relative_query_intent_fulfillment'\n    # Initialize the column with a neutral value (1.0 means 'as good as competitors')\n    df[feature_name] = 1.0\n    \n    model = get_model()\n\n    # Group by the representative query to handle each context separately.\n    # preserve_index=False is not used to maintain the original index for correct assignment.\n    grouped = df.groupby('representative_query')\n\n    for query, group_df in grouped:\n        if not isinstance(query, str) or not query:\n            continue\n\n        # Separate our posts from competitor posts within the group\n        ours_df = group_df[group_df['source'] == 'ours']\n        competitors_df = group_df[group_df['source'] == 'competitor']\n\n        # We need at least one of our posts and one competitor post to make a comparison.\n        if ours_df.empty or competitors_df.empty:\n            continue\n\n        # Prepare texts for batch encoding. This is highly efficient.\n        our_bodies = ours_df['post_body'].fillna('').astype(str).tolist()\n        competitor_bodies = competitors_df['post_body'].fillna('').astype(str).tolist()\n        \n        # Encode the query and all post bodies in a single batch.\n        all_texts = [query] + our_bodies + competitor_bodies\n        embeddings = model.encode(all_texts, convert_to_tensor=True)\n\n        # Separate the embeddings back out\n        query_embedding = embeddings[0:1]\n        our_embeddings = embeddings[1:1+len(our_bodies)]\n        competitor_embeddings = embeddings[1+len(our_bodies):]\n\n        # Calculate semantic similarity (intent fulfillment score) for each post against the query.\n        our_fulfillment_scores = util.cos_sim(query_embedding, our_embeddings).flatten()\n        competitor_fulfillment_scores = util.cos_sim(query_embedding, competitor_embeddings).flatten()\n\n        # Calculate the average fulfillment score for the competitor group.\n        # Add a small epsilon to prevent division by zero.\n        avg_competitor_fulfillment = competitor_fulfillment_scores.mean().item()\n        \n        if avg_competitor_fulfillment < 1e-9:\n            # If competitors have zero similarity, avoid division by zero.\n            # Assign a large number to our posts to signify high relative fulfillment.\n            # This is an edge case but important for robustness.\n            relative_scores = np.full(len(our_fulfillment_scores), 10.0)\n        else:\n            # Calculate the relative fulfillment for EACH of our posts.\n            # This is the key step that ensures variance.\n            relative_scores = (our_fulfillment_scores / avg_competitor_fulfillment).cpu().numpy()\n\n        # Assign the calculated scores back to the original DataFrame using the correct index.\n        # This is critical for preventing data misalignment.\n        df.loc[ours_df.index, feature_name] = relative_scores\n\n    return df",
  "analysis": {
    "correlation": 0.29365724695451556,
    "p_value": 0.0038733826518317404,
    "interpretation": "약한 양의 상관관계(0.2937)를 발견했습니다. 이 결과는 통계적으로 유의미합니다(p-value: 0.0039)."
  }
}{
  "timestamp": "2025-07-22T06:00:44.844300",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_query_fulfillment_score",
  "hypothesis": "포스트 본문이 대표 검색어와의 의미적 유사성을 통해 검색 의도를 더 잘 충족할수록, 그리고 이 충족도가 동일 검색어에 대한 경쟁자 평균보다 높을수록, 비브랜드 평균 CTR이 높을 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score representing how much better our post fulfills the query intent\n    compared to the average competitor for the same query.\n\n    The score is calculated as: (Our Post's Similarity) / (Competitors' Average Similarity)\n    where similarity is the cosine similarity between the post_body and the representative_query.\n    \"\"\"\n    # 2. Check for empty DataFrame and required columns to prevent errors.\n    if df.empty:\n        df['relative_query_fulfillment_score'] = pd.Series(dtype=float)\n        return df\n\n    required_columns = ['representative_query', 'post_body', 'source']\n    for col in required_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Missing required column: {col}\")\n\n    # Ensure the new column exists, even if processing fails for some rows\n    df['relative_query_fulfillment_score'] = 1.0\n\n    # 3. Use efficient batch processing for all embeddings at once.\n    model = get_model()\n\n    # Prepare text data, ensuring no NaNs and correct type\n    queries = df['representative_query'].fillna('').astype(str).tolist()\n    bodies = df['post_body'].fillna('').astype(str).tolist()\n\n    # Encode in batches\n    query_embeddings = model.encode(queries, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # Calculate the direct semantic similarity between each query and its corresponding body\n    # .diag() efficiently gets the pairwise similarities for corresponding elements.\n    df['temp_similarity'] = util.cos_sim(query_embeddings, body_embeddings).diag().cpu().numpy()\n\n    # 4. Group by query to calculate the relative score against competitors.\n    # This approach correctly calculates a per-query benchmark.\n    for query, group_df in df.groupby('representative_query'):\n        if query == '': # Skip groups with no query\n            continue\n\n        # Correctly separate 'ours' and 'competitor' posts within the group\n        our_post_mask = group_df['source'] == 'ours'\n        competitor_posts_mask = group_df['source'] == 'competitor'\n\n        our_indices = group_df[our_post_mask].index\n        competitor_indices = group_df[competitor_posts_mask].index\n\n        # If there are no 'ours' posts or no 'competitor' posts, we can't compare.\n        # The default value of 1.0 will be used.\n        if our_indices.empty or competitor_indices.empty:\n            continue\n\n        # Calculate the average similarity for all competitors in this group\n        competitor_avg_similarity = df.loc[competitor_indices, 'temp_similarity'].mean()\n\n        # Get the similarity for our post(s) in this group\n        our_similarities = df.loc[our_indices, 'temp_similarity']\n        \n        # Avoid division by zero. If competitors have 0 similarity, the ratio is undefined.\n        # A ratio of 1.0 implies being on par, which is a safe default.\n        # A small epsilon is added to avoid floating point inaccuracies.\n        if competitor_avg_similarity > 1e-9:\n            relative_score = our_similarities / competitor_avg_similarity\n            # 5. Assign the calculated feature back to the original DataFrame using its index.\n            # This preserves the original DataFrame's structure and prevents data misalignment.\n            df.loc[our_indices, 'relative_query_fulfillment_score'] = relative_score\n        else:\n            # If competitor avg is 0, and our score is positive, this is infinitely good.\n            # We cap it at a high value. If our score is also 0, it's 1.0 (on par).\n            high_score_cap = 5.0 # A reasonable cap for 'infinitely better'\n            relative_scores = our_similarities.apply(lambda x: high_score_cap if x > 0 else 1.0)\n            df.loc[our_indices, 'relative_query_fulfillment_score'] = relative_scores\n\n    # Clean up the temporary column\n    df.drop(columns=['temp_similarity'], inplace=True)\n\n    # 6. Return the full DataFrame with the new feature column.\n    return df",
  "analysis": {
    "correlation": 0.29365719222086073,
    "p_value": 0.0038733895900607616,
    "interpretation": "약한 양의 상관관계(0.2937)를 발견했습니다. 이 결과는 통계적으로 유의미합니다(p-value: 0.0039)."
  }
}{
  "timestamp": "2025-07-22T06:19:29.416294",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_query_intent_fulfillment",
  "hypothesis": "우리 포스트가 경쟁자 그룹의 평균보다 대표 검색어의 의미적 의도를 더 잘 충족시킬수록(즉, 검색어와 콘텐츠 간의 의미적 유사도가 상대적으로 높을수록), 사용자의 검색 만족도가 높아져 비브랜드 평균 CTR이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Use a powerful multilingual model suitable for Korean.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Measures how well our post satisfies the semantic intent of the representative query\n    compared to the average competitor post for the same query.\n\n    The feature is calculated as:\n    (Our Post's Query-Content Similarity) / (Average Competitor Query-Content Similarity)\n\n    A value > 1 indicates our post is more semantically aligned with the query than competitors.\n    The feature is only calculated for 'ours' posts.\n    \"\"\"\n    # 2. Check for empty DataFrame to prevent errors.\n    if not df.empty and 'representative_query' in df.columns and 'post_title' in df.columns and 'post_body' in df.columns:\n        # Ensure we don't modify the original DataFrame passed to the function\n        df_copy = df.copy()\n\n        model = get_model()\n\n        # 3. Prepare data for batch processing, preserving original index integrity.\n        df_copy['combined_text'] = df_copy['post_title'].fillna('') + ' ' + df_copy['post_body'].fillna('')\n        content_texts = df_copy['combined_text'].tolist()\n\n        # Handle cases where all queries might be null\n        if df_copy['representative_query'].isnull().all():\n            df['relative_query_intent_fulfillment'] = np.nan\n            return df\n        \n        unique_queries = df_copy['representative_query'].dropna().unique().tolist()\n        \n        # 4. Encode all content and queries in efficient batches.\n        content_embeddings = model.encode(content_texts, convert_to_tensor=True)\n        query_embeddings = model.encode(unique_queries, convert_to_tensor=True)\n\n        # Create a mapping from query string to its embedding tensor for easy lookup.\n        query_embedding_map = {query: emb for query, emb in zip(unique_queries, query_embeddings)}\n\n        # 5. Calculate pairwise query-content similarity for each row efficiently.\n        # Map each row's query to its embedding, stack them into a tensor,\n        # then calculate cosine similarity against content embeddings.\n        # .diag() extracts the correct pairwise similarities (row_i_query vs row_i_content).\n        # Create a placeholder for rows with null queries\n        default_embedding = torch.zeros(model.get_sentence_embedding_dimension()).to(content_embeddings.device)\n        aligned_query_embeddings_list = [\n            query_embedding_map.get(q, default_embedding) for q in df_copy['representative_query']\n        ]\n        aligned_query_embeddings = torch.stack(aligned_query_embeddings_list)\n        \n        similarities = util.cos_sim(aligned_query_embeddings, content_embeddings).diag()\n        df_copy['query_content_similarity'] = similarities.tolist()\n\n        # 6. Calculate the competitor benchmark for each query group.\n        # This creates a map: {query: avg_competitor_similarity}\n        competitor_benchmark = df_copy[df_copy['source'] == 'competitor'].groupby('representative_query')['query_content_similarity'].mean().to_dict()\n\n        # 7. Map the competitor average similarity back to each row.\n        df_copy['competitor_avg_similarity'] = df_copy['representative_query'].map(competitor_benchmark)\n\n        # 8. Calculate the final relational feature.\n        # The division will result in NaN or inf for rows with no competitor benchmark, which is handled.\n        feature_values = df_copy['query_content_similarity'] / df_copy['competitor_avg_similarity']\n\n        # Fill NaN values that might result from division by zero (no competitors) with a neutral value like 1.0\n        feature_values.fillna(1.0, inplace=True)\n        # Replace infinite values (if competitor avg similarity was 0) with a large but finite number, or 1.0\n        feature_values.replace([np.inf, -np.inf], 1.0, inplace=True)\n\n        # 9. Assign the feature to the final DataFrame.\n        df['relative_query_intent_fulfillment'] = feature_values\n\n        # The feature is only meaningful for 'ours' posts, so set others to NaN.\n        df.loc[df['source'] == 'competitor', 'relative_query_intent_fulfillment'] = np.nan\n\n    else:\n        # If the dataframe is empty or missing columns, return it with a placeholder column.\n        df['relative_query_intent_fulfillment'] = np.nan\n\n    return df\n",
  "analysis": {
    "correlation": 0.12908076068817995,
    "p_value": 0.21251071859542844,
    "interpretation": "약한 양의 상관관계(0.1291)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.2125)."
  }
}{
  "timestamp": "2025-07-22T06:36:59.207357",
  "attempt": 1,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'relative_query_intent_fulfillment'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # A strong, multilingual model suitable for Korean text.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well our post satisfies the query intent compared to competitors.\n\n    This is measured by:\n    1. Calculating the semantic similarity between each post's content (title + body) and its representative query.\n    2. For each query group, calculating the average similarity for all competitor posts.\n    3. Creating a ratio: (our post's similarity) / (competitors' average similarity).\n    \"\"\"\n    feature_name = 'relative_query_intent_fulfillment'\n    # Initialize the feature column with NaN. Only 'ours' posts will get a calculated value.\n    df[feature_name] = np.nan\n\n    # 2. Always check for an empty DataFrame before processing.\n    if not df.empty:\n        model = get_model()\n\n        # 3. Prepare data for efficient batch processing. Handle potential NaN values.\n        queries = df['representative_query'].fillna('').astype(str).tolist()\n        # Combine title and body for a more comprehensive content representation.\n        contents = (df['post_title'].fillna('').astype(str) + ' ' + df['post_body'].fillna('').astype(str)).tolist()\n\n        # 4. Compute all embeddings at once for maximum efficiency.\n        # The model will not show a progress bar during this operation.\n        query_embeddings = model.encode(queries, convert_to_tensor=True, show_progress_bar=False)\n        content_embeddings = model.encode(contents, convert_to_tensor=True, show_progress_bar=False)\n\n        # 5. Calculate the core 'intent satisfaction' score for every single post.\n        # This is the cosine similarity between each post's content and its own representative query.\n        # The .diag() method correctly extracts the pair-wise similarity sim(query_i, content_i) for all i.\n        intent_scores = util.cos_sim(query_embeddings, content_embeddings).diag()\n\n        # 6. Use a temporary DataFrame to manage calculations while preserving the original index.\n        # This is critical for correctly mapping results back to the original DataFrame.\n        temp_df = pd.DataFrame({\n            'intent_score': intent_scores.cpu().numpy(),\n            'source': df['source'],\n            'representative_query': df['representative_query']\n        }, index=df.index)\n\n        # 7. Calculate the average competitor intent score for each query group.\n        # First, create a Series containing scores only from competitors.\n        competitor_scores = temp_df['intent_score'].where(temp_df['source'] == 'competitors')\n\n        # Then, use groupby().transform('mean'). This is a powerful operation that:\n        #  - Calculates the mean score for competitors within each 'representative_query' group.\n        #  - Broadcasts this mean value to *all* rows belonging to that group, including 'ours' posts.\n        #  - Preserves the original DataFrame's index, ensuring perfect alignment.\n        #  - Automatically handles groups with no competitors by assigning NaN.\n        temp_df['competitor_avg_intent_score'] = competitor_scores.groupby(temp_df['representative_query']).transform('mean')\n\n        # 8. Isolate 'ours' posts to calculate and assign the final feature value.\n        our_posts_mask = temp_df['source'] == 'ours'\n        our_intent_scores = temp_df.loc[our_posts_mask, 'intent_score']\n        competitor_avg_scores = temp_df.loc[our_posts_mask, 'competitor_avg_intent_score']\n\n        # 9. Calculate the final ratio.\n        # Add a small epsilon to the denominator to prevent division-by-zero errors if a competitor score is 0.\n        epsilon = 1e-9\n        relative_fulfillment_ratio = our_intent_scores / (competitor_avg_scores + epsilon)\n\n        # 10. Assign the calculated feature value only to the 'ours' post rows in the original DataFrame.\n        df.loc[our_posts_mask, feature_name] = relative_fulfillment_ratio\n\n    return df\n"
}{
  "timestamp": "2025-07-22T06:38:02.108979",
  "attempt": 2,
  "status": "success",
  "feature_name": "relative_query_fulfillment_score",
  "hypothesis": "우리 포스트의 내용이 대표 검색어와 의미적으로 더 가까울수록(검색 의도를 더 잘 충족시킬수록), 경쟁자 그룹의 평균적인 의도 충족도에 비해 상대적으로 더 높은 CTR을 보일 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton instance of the SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a model that is strong for Korean and general semantic similarity.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score representing how much better our post fulfills the query intent\n    compared to the average competitor for the same query.\n\n    The score is calculated as:\n    (Similarity of our post body to its query) / (Average similarity of competitor post bodies to the same query)\n    \n    This provides a relative measure of query-content alignment, ensuring variance\n    even if all 'ours' posts share the same representative query.\n    \"\"\"\n    # 2. Check for an empty DataFrame to prevent errors.\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # 3. Separate 'ours' from 'competitor' posts, preserving the original index.\n    ours_df = df[df['source'] == 'ours'].copy()\n    competitor_df = df[df['source'] == 'competitor'].copy()\n\n    # If there are no 'ours' posts to analyze, return the df with an empty feature column.\n    if ours_df.empty:\n        df['relative_query_fulfillment_score'] = 0\n        return df\n\n    # 4. Calculate the average query fulfillment score for competitors, grouped by query.\n    query_benchmarks = {}\n    if not competitor_df.empty:\n        # Ensure 'post_body' is string type to prevent errors\n        competitor_df['post_body'] = competitor_df['post_body'].fillna('').astype(str)\n        \n        competitor_groups = competitor_df.groupby('representative_query')\n        \n        for query, group in competitor_groups:\n            if query:\n                competitor_bodies = group['post_body'].tolist()\n\n                # Encode in batches for efficiency\n                query_embedding = model.encode(query, convert_to_tensor=True)\n                body_embeddings = model.encode(competitor_bodies, convert_to_tensor=True)\n\n                # Calculate similarity and find the mean for the group\n                similarities = util.cos_sim(query_embedding, body_embeddings)\n                # The result is a (1, N) tensor, so we take the mean of that single row.\n                benchmark_score = similarities[0].mean().item()\n                query_benchmarks[query] = benchmark_score\n\n    # 5. Calculate the query fulfillment score for each of 'our' posts.\n    # Ensure 'post_body' and 'representative_query' are clean strings\n    our_queries = ours_df['representative_query'].fillna('').astype(str).tolist()\n    our_bodies = ours_df['post_body'].fillna('').astype(str).tolist()\n\n    # Encode in batches for efficiency\n    our_query_embeddings = model.encode(our_queries, convert_to_tensor=True)\n    our_body_embeddings = model.encode(our_bodies, convert_to_tensor=True)\n    \n    # .diag() efficiently gets the similarity between each (query, body) pair\n    our_raw_scores = util.cos_sim(our_query_embeddings, our_body_embeddings).diag()\n\n    # 6. Create a pandas Series for our scores, CRUCIALLY using the original index from ours_df.\n    # This ensures correct alignment when we assign the feature back to the main DataFrame.\n    our_scores_series = pd.Series(our_raw_scores.cpu().numpy(), index=ours_df.index)\n    \n    # 7. Map the calculated competitor benchmarks to each 'our' post based on its query.\n    # This creates a Series of benchmark scores aligned with ours_df.\n    our_benchmarks = ours_df['representative_query'].map(query_benchmarks)\n\n    # 8. Calculate the final relative score, handling potential division by zero.\n    # The resulting series will have variance as long as our_scores_series has variance.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        relative_scores = our_scores_series / our_benchmarks\n    \n    # Replace infinity (from division by zero) and NaN (from missing benchmarks) with 0.\n    relative_scores.replace([np.inf, -np.inf], 0, inplace=True)\n    relative_scores.fillna(0, inplace=True)\n\n    # 9. Assign the correctly-indexed relative scores to the new column in the original DataFrame.\n    df['relative_query_fulfillment_score'] = relative_scores\n    # Fill non-'ours' rows with a neutral value.\n    df['relative_query_fulfillment_score'].fillna(0, inplace=True)\n\n    return df",
  "analysis": {
    "correlation": 0.29365734643751684,
    "p_value": 0.0038733700410478436,
    "interpretation": "약한 양의 상관관계(0.2937)를 발견했습니다. 이 결과는 통계적으로 유의미합니다(p-value: 0.0039)."
  }
}{
  "timestamp": "2025-07-22T07:01:51.648137",
  "attempt": 1,
  "status": "success",
  "feature_name": "relational_query_intent_fulfillment",
  "hypothesis": "우리 포스트 그룹이 경쟁자 포스트 그룹의 평균보다 대표 검색어의 의미적 의도를 더 잘 충족시킬수록(즉, 검색어와 포스트 본문 간의 평균 의미적 유사도가 경쟁자 평균보다 높을수록), 사용자는 검색 결과에서 해당 포스트를 더 관련성 높다고 판단하여 클릭할 가능성이 높아지므로, 'non_brand_average_ctr'이 증가할 것입니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean content.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the relative query intent fulfillment by comparing the semantic similarity\n    of 'our' posts to the representative query against the average similarity of 'competitor' posts.\n    \"\"\"\n    feature_name = 'relational_query_intent_fulfillment'\n    \n    # Initialize the feature column with a default value (NaN).\n    # This ensures the column exists even if the DataFrame is empty or processing fails.\n    df[feature_name] = np.nan\n\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # Create a copy for processing to avoid SettingWithCopyWarning.\n    df_proc = df.copy()\n    df_proc['representative_query'] = df_proc['representative_query'].fillna('').astype(str)\n    df_proc['post_body'] = df_proc['post_body'].fillna('').astype(str)\n\n    # Group by the representative query to perform calculations within each query context.\n    grouped = df_proc.groupby('representative_query')\n\n    for query, group in grouped:\n        # Skip groups with no representative query.\n        if not query.strip():\n            continue\n\n        # Separate 'ours' and 'competitor' posts within the group.\n        ours_posts = group[group['source'] == 'ours']\n        competitor_posts = group[group['source'] == 'competitor']\n\n        # A relational feature requires both groups to be present for comparison.\n        if ours_posts.empty or competitor_posts.empty:\n            continue\n\n        # Encode the query text. It's encoded once per group.\n        query_embedding = model.encode(query, convert_to_tensor=True)\n\n        # Calculate average semantic score for 'our' posts in the group.\n        our_bodies = ours_posts['post_body'].tolist()\n        our_embeddings = model.encode(our_bodies, convert_to_tensor=True, batch_size=32)\n        our_avg_score = util.cos_sim(query_embedding, our_embeddings).mean().item()\n\n        # Calculate average semantic score for 'competitor' posts in the group.\n        competitor_bodies = competitor_posts['post_body'].tolist()\n        competitor_embeddings = model.encode(competitor_bodies, convert_to_tensor=True, batch_size=32)\n        competitor_avg_score = util.cos_sim(query_embedding, competitor_embeddings).mean().item()\n\n        # Calculate the final relational score with safe division.\n        if competitor_avg_score > 1e-9:\n            feature_value = our_avg_score / competitor_avg_score\n        else:\n            # If competitor avg score is effectively zero, our score is comparatively high if non-zero.\n            if our_avg_score > 1e-9:\n                # Capping at 2.0 to prevent extreme outliers, signifying clear superiority.\n                feature_value = 2.0\n            else:\n                # Both are equally irrelevant to the query, so the ratio is 1.\n                feature_value = 1.0\n        \n        # Assign the calculated group-level feature value to all original rows in the group.\n        # This ensures correct alignment with the original DataFrame's index.\n        df.loc[group.index, feature_name] = feature_value\n\n    return df",
  "analysis": {
    "correlation": 0.3116492108329269,
    "p_value": 0.0021098464708259757,
    "interpretation": "중간 정도의 양의 상관관계(0.3116)를 발견했습니다. 이 결과는 통계적으로 유의미합니다(p-value: 0.0021)."
  }
}{
  "timestamp": "2025-07-22T07:22:30.698094",
  "attempt": 1,
  "status": "success",
  "feature_name": "relational_query_fulfillment_score",
  "hypothesis": "우리 포스트가 대표 검색어의 의도를 얼마나 충족시키는지를 의미론적으로 측정한 '쿼리 충족도'를 계산합니다. 이 점수를 동일 검색어 그룹 내 경쟁 포스트들의 평균 '쿼리 충족도'와 비교한 상대적 비율이 높을수록, 우리 포스트가 경쟁 그룹 대비 검색 의도를 더 잘 만족시킨다는 의미이므로, 사용자의 클릭을 유도할 가능성이 커져 `non_brand_average_ctr`이 높을 것입니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a well-regarded multilingual model suitable for Korean.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    feature_name = 'relational_query_fulfillment_score'\n    # Initialize with a neutral value of 1.0, assuming posts are on-par with competitors by default.\n    df[feature_name] = 1.0\n\n    # 2. Check for empty DataFrame or required columns to prevent errors.\n    required_cols = ['representative_query', 'post_title', 'post_body', 'source']\n    if df.empty or not all(col in df.columns for col in required_cols):\n        return df\n\n    model = get_model()\n\n    # Create a copy to avoid SettingWithCopyWarning during calculations.\n    temp_df = df.copy()\n\n    # 3. Prepare texts for embedding.\n    # Combine title and body for a comprehensive content representation.\n    # fillna('') ensures that missing text fields don't cause errors.\n    contents = (temp_df['post_title'].fillna('') + ' ' + temp_df['post_body'].fillna('')).tolist()\n    \n    # Get unique queries to encode them only once, which is highly efficient.\n    unique_queries = temp_df['representative_query'].astype(str).unique().tolist()\n\n    # 4. Efficiently encode all contents and unique queries in batches.\n    content_embeddings = model.encode(contents, convert_to_tensor=True, show_progress_bar=False)\n    query_embeddings = model.encode(unique_queries, convert_to_tensor=True, show_progress_bar=False)\n\n    # 5. Map unique query embeddings back to each row for pairwise comparison.\n    query_to_embedding_map = {query: emb for query, emb in zip(unique_queries, query_embeddings)}\n    query_embeddings_list = temp_df['representative_query'].astype(str).map(query_to_embedding_map).tolist()\n    query_embeddings_per_row = torch.stack(query_embeddings_list)\n\n    # 6. Calculate each post's individual fulfillment score (similarity to its own query).\n    # The .diag() efficiently gets the pairwise similarity for each row, avoiding a full NxN matrix calculation.\n    fulfillment_scores = util.cos_sim(query_embeddings_per_row, content_embeddings).diag()\n    temp_df['query_fulfillment'] = fulfillment_scores.cpu().numpy()\n\n    # 7. Calculate the average fulfillment score for competitors within each query group.\n    # Create a Series containing only competitor scores, with NaNs for 'ours' posts.\n    competitor_fulfillment = temp_df.where(temp_df['source'] == 'competitor')['query_fulfillment']\n\n    # Use transform to calculate the mean for each group and broadcast it back to all rows in that group.\n    # This efficiently calculates the group-wise mean of competitor scores.\n    avg_competitor_fulfillment = temp_df.groupby('representative_query')['query_fulfillment'].transform(\n        lambda x: competitor_fulfillment.loc[x.index].mean()\n    )\n    temp_df['avg_competitor_fulfillment'] = avg_competitor_fulfillment\n\n    # 8. Calculate the final relational score.\n    # This feature is only meaningful for 'ours' posts, comparing them to the competitor benchmark.\n    our_posts_mask = temp_df['source'] == 'ours'\n    \n    our_scores = temp_df.loc[our_posts_mask, 'query_fulfillment']\n    competitor_avg_scores = temp_df.loc[our_posts_mask, 'avg_competitor_fulfillment']\n\n    # Calculate the ratio. This indicates how much better/worse our post is than the competitor average.\n    relational_scores = our_scores / competitor_avg_scores\n    \n    # Handle edge cases:\n    # - np.inf: Our score is positive, competitor avg is 0. Our post is infinitely better.\n    # - np.nan: No competitors in the group, so avg is NaN.\n    # In both cases, a neutral score of 1.0 is a safe fallback.\n    relational_scores.replace([np.inf, np.nan], 1.0, inplace=True)\n    \n    # To prevent extreme outliers from skewing models, cap the score at a reasonable upper bound.\n    relational_scores.clip(upper=5.0, inplace=True)\n\n    # 9. Assign the calculated feature back to the original DataFrame.\n    # Use .loc with the mask to ensure correct alignment and avoid chained assignment warnings.\n    df.loc[our_posts_mask, feature_name] = relational_scores\n    \n    return df",
  "analysis": {
    "correlation": 0.12908075770449712,
    "p_value": 0.21251072925521108,
    "interpretation": "약한 양의 상관관계(0.1291)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.2125)."
  }
}