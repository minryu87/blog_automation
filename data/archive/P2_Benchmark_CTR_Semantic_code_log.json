{
  "timestamp": "2025-07-22T04:03:08.833912",
  "attempt": 1,
  "status": "success",
  "feature_name": "actionability_score_ratio_vs_competitors",
  "hypothesis": "포스트 본문이 경쟁자 그룹의 평균적인 본문보다 '해결책', '가이드', '방법'과 같은 실행 가능한 정보와 의미적으로 더 가까울수록, 사용자의 문제 해결 기대감을 높여 클릭률(CTR)이 높을 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton instance of the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean text\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the ratio of a post's semantic 'actionability' score to the average\n    score of its competitors for the same representative query.\n    \"\"\"\n    # 2. Check for empty DataFrame to prevent errors.\n    if df.empty:\n        df['actionability_score_ratio_vs_competitors'] = pd.Series(dtype=float)\n        return df\n\n    # Use a copy to avoid SettingWithCopyWarning\n    df_feature = df.copy()\n    feature_name = 'actionability_score_ratio_vs_competitors'\n    df_feature[feature_name] = np.nan\n\n    # Define sentences that embody the concept of 'actionability' or 'practical guidance'.\n    actionability_concept_sentences = [\n        \"문제를 해결하는 방법\",         # How to solve the problem\n        \"자세한 실행 가이드\",             # Detailed execution guide\n        \"단계별 절차 안내\",             # Step-by-step procedure guide\n        \"실용적인 팁과 노하우\",           # Practical tips and know-how\n        \"궁금증을 풀어드립니다\",           # We will answer your questions\n        \"빠른 해결책 제시\"              # Proposing a quick solution\n    ]\n\n    try:\n        model = get_model()\n\n        # 3. Create a single 'actionability' concept vector by averaging the embeddings.\n        concept_embeddings = model.encode(actionability_concept_sentences, convert_to_tensor=True)\n        actionability_concept_vector = util.normalize_embeddings(concept_embeddings).mean(axis=0, keepdim=True)\n\n        # 4. Efficiently process all post bodies in a single batch.\n        post_bodies = df_feature['post_body'].fillna('').astype(str).tolist()\n        body_embeddings = model.encode(post_bodies, convert_to_tensor=True, show_progress_bar=False)\n\n        # 5. Calculate the cosine similarity of each post to the actionability concept.\n        # This results in an (N, 1) tensor.\n        actionability_scores = util.cos_sim(body_embeddings, actionability_concept_vector)\n        df_feature['temp_actionability_score'] = actionability_scores.cpu().numpy().flatten()\n\n        # 6. Calculate the average competitor score for each query group.\n        competitor_df = df_feature[df_feature['source'] == 'competitor']\n        # Handle cases where some queries may not have competitors\n        if not competitor_df.empty:\n            mean_competitor_scores = competitor_df.groupby('representative_query')['temp_actionability_score'].mean()\n\n            # 7. Map the average competitor score to each 'ours' post based on its query.\n            ours_indices = df_feature['source'] == 'ours'\n            our_posts_queries = df_feature.loc[ours_indices, 'representative_query']\n            mapped_competitor_avg = our_posts_queries.map(mean_competitor_scores)\n\n            # 8. Calculate the ratio for 'ours' posts.\n            our_scores = df_feature.loc[ours_indices, 'temp_actionability_score']\n            # Ensure the divisor has the same index as the dividend\n            mapped_competitor_avg.index = our_scores.index\n            \n            ratio = our_scores / mapped_competitor_avg\n\n            # Assign the calculated ratio to the final feature column for 'ours' posts.\n            df_feature.loc[ours_indices, feature_name] = ratio\n\n    except Exception as e:\n        # In case of any error (e.g., model loading), ensure the column exists with NaNs.\n        print(f\"Error during feature generation for {feature_name}: {e}\")\n        # The column is already initialized with NaN, so we can just pass.\n        pass\n    finally:\n        # Clean up temporary column\n        if 'temp_actionability_score' in df_feature.columns:\n            df_feature = df_feature.drop(columns=['temp_actionability_score'])\n        \n        # Replace inf/-inf resulting from division by zero with a neutral value (1.0).\n        # Also fill NaNs for 'ours' posts that had no competitors (map resulted in NaN).\n        df_feature[feature_name] = df_feature[feature_name].replace([np.inf, -np.inf], 1.0).fillna(1.0)\n\n\n    # 9. Return the full DataFrame with the new feature column.\n    return df_feature",
  "analysis": {
    "correlation": 0.1818150448902249,
    "p_value": 0.07783590186596824,
    "interpretation": "약한 양의 상관관계(0.1818)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.0778)."
  }
}{
  "timestamp": "2025-07-22T04:24:32.521397",
  "attempt": 1,
  "status": "success",
  "feature_name": "title_body_cohesion_vs_competitors",
  "hypothesis": "우리 포스트의 '제목-본문 의미적 일관성'이 동일 대표 검색어에 대한 경쟁자 그룹의 평균적인 일관성보다 높을수록, 포스트의 주제 집중도가 명확해져 사용자 만족도를 높이고, 결과적으로 비브랜드 평균 CTR(`non_brand_average_ctr`)을 향상시킬 것입니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Use a multilingual model suitable for Korean text.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the title-body semantic cohesion for each post and benchmarks it\n    against the average cohesion of competitors for the same representative query.\n\n    A value > 1 indicates our post is more cohesive than the competitor average.\n    A value < 1 indicates it is less cohesive.\n    \"\"\"\n    # 2. Always check for an empty DataFrame to prevent errors.\n    if df.empty:\n        df['title_body_cohesion_vs_competitors'] = pd.Series(dtype=float)\n        return df\n\n    model = get_model()\n\n    # 3. Preserve the original index for accurate final assignment.\n    original_index = df.index\n\n    # 4. Prepare data for efficient batch processing.\n    titles = df['post_title'].fillna('').astype(str).tolist()\n    bodies = df['post_body'].fillna('').astype(str).tolist()\n\n    # 5. Encode all titles and bodies in two separate, efficient batches.\n    title_embeddings = model.encode(titles, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # 6. Calculate element-wise cosine similarity (cohesion) for each title-body pair.\n    # .diag() correctly extracts the similarity between the i-th title and i-th body.\n    cohesion_scores = util.cos_sim(title_embeddings, body_embeddings).diag()\n\n    # Create a temporary working DataFrame to keep data aligned by index.\n    work_df = pd.DataFrame({\n        'representative_query': df['representative_query'],\n        'source': df['source'],\n        'temp_cohesion': cohesion_scores.cpu().numpy()\n    }, index=original_index)\n\n    # 7. Use groupby().transform() for an efficient vectorized calculation.\n    # Create a Series of cohesion scores for competitors only.\n    competitor_cohesion = work_df['temp_cohesion'].where(work_df['source'] == 'competitor')\n    \n    # Calculate the mean cohesion of competitors for each query group and broadcast it to all members of the group.\n    competitor_avg_cohesion = competitor_cohesion.groupby(work_df['representative_query']).transform('mean')\n\n    # 8. Calculate the benchmark ratio by dividing each post's cohesion by its group's competitor average.\n    # This calculation correctly produces the desired ratio for 'ours' posts and a different value for 'competitor' posts.\n    benchmark_ratio = work_df['temp_cohesion'] / competitor_avg_cohesion\n    \n    # 9. Assign the calculated ratios to the new column in the original DataFrame.\n    df['title_body_cohesion_vs_competitors'] = benchmark_ratio\n    \n    # Explicitly set the feature value to NaN for all competitor posts, as the feature is not applicable to them.\n    df.loc[df['source'] == 'competitor', 'title_body_cohesion_vs_competitors'] = np.nan\n    \n    return df\n",
  "analysis": {
    "correlation": 0.10142660917550145,
    "p_value": 0.3280633480976571,
    "interpretation": "약한 양의 상관관계(0.1014)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3281)."
  }
}{
  "timestamp": "2025-07-22T04:43:01.178521",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_actionability_vs_competitors",
  "hypothesis": "우리 포스트의 본문이 '방법', '해결', '팁' 등 실용적, 해결책 지향적 의미에 가까울수록, 그리고 이러한 경향이 경쟁 포스트 그룹의 평균보다 강할수록, 사용자의 문제 해결 기대감을 높여 클릭률(non_brand_average_ctr)이 높을 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Lazy-loads and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Use a multilingual model suitable for Korean text.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the ratio of a post's semantic 'actionability' score compared to the average\n    actionability score of its competitors for the same query.\n    The actionability score is determined by the semantic similarity of the post body to a \n    vector representing practical, solution-oriented concepts.\n    \"\"\"\n    # Check for empty or invalid DataFrame to prevent errors.\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        return df\n\n    feature_name = 'semantic_actionability_vs_competitors'\n    \n    # Initialize the feature column with a neutral default value (1.0).\n    # This ensures the column exists even if calculation is skipped.\n    df[feature_name] = 1.0\n\n    # Ensure all required columns are present.\n    required_cols = ['post_body', 'source', 'representative_query']\n    if not all(col in df.columns for col in required_cols):\n        # Silently return if data is not as expected, preserving the default feature value.\n        return df\n\n    model = get_model()\n\n    # Define keywords to create an 'ideal actionability' semantic vector.\n    actionability_keywords = [\n        '방법', '해결', '팁', '후기', '비교', '추천', '노하우', \n        '가이드', '전략', '최적화', '하는 법', '원인', '증상', '리뷰'\n    ]\n\n    # 1. Create the ideal actionability vector by averaging keyword embeddings.\n    ideal_embeddings = model.encode(actionability_keywords, convert_to_tensor=True)\n    ideal_actionability_vector = ideal_embeddings.mean(axis=0, keepdim=True)\n\n    # Make a copy to avoid SettingWithCopyWarning during intermediate calculations.\n    df_copy = df.copy()\n\n    # 2. Efficiently calculate actionability scores for all posts in a single batch.\n    post_bodies = df_copy['post_body'].fillna('').astype(str).tolist()\n    body_embeddings = model.encode(post_bodies, convert_to_tensor=True, show_progress_bar=False)\n    \n    # Calculate cosine similarity and add as a temporary column.\n    similarities = util.cos_sim(body_embeddings, ideal_actionability_vector).flatten().cpu().numpy()\n    df_copy['temp_actionability_score'] = similarities\n\n    # 3. Calculate the average competitor score for each representative query.\n    # This assumes the provided competitor data is the relevant 'top competitor' group.\n    df_competitors = df_copy[df_copy['source'] == 'competitor']\n    \n    # If there are no competitors, we cannot perform the comparison.\n    # The function will return the DataFrame with the default feature value of 1.0.\n    if df_competitors.empty:\n        return df\n\n    # Create a mapping from each query to its average competitor score.\n    avg_competitor_score_map = df_competitors.groupby('representative_query')['temp_actionability_score'].mean()\n\n    # 4. Map the average competitor score to all posts based on their query.\n    df_copy['temp_competitor_avg_score'] = df_copy['representative_query'].map(avg_competitor_score_map)\n\n    # 5. Calculate the final ratio feature for 'ours' posts only.\n    our_posts_mask = (df_copy['source'] == 'ours')\n    \n    our_scores = df_copy.loc[our_posts_mask, 'temp_actionability_score']\n    competitor_avg_scores = df_copy.loc[our_posts_mask, 'temp_competitor_avg_score']\n    \n    # Calculate the ratio, safely handling division by zero or NaN.\n    # A neutral value of 1.0 is used where the ratio is undefined.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.divide(our_scores, competitor_avg_scores)\n    \n    ratio = pd.Series(ratio, index=our_scores.index).replace([np.inf, -np.inf], 1.0).fillna(1.0)\n    \n    # 6. Assign the calculated ratio back to the original DataFrame, preserving its index.\n    # Using .loc with the mask ensures values are assigned only to the correct 'ours' rows.\n    df.loc[our_posts_mask, feature_name] = ratio\n    \n    return df",
  "analysis": {
    "correlation": 0.07696860701613716,
    "p_value": 0.45847115743956096,
    "interpretation": "약한 양의 상관관계(0.0770)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.4585)."
  }
}{
  "timestamp": "2025-07-22T05:02:49.623013",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_actionability_vs_competitors",
  "hypothesis": "우리 포스트 본문이 특정 대표 검색어의 경쟁자 그룹 평균보다 '방법', '해결', '가이드' 등 실용적/행동 지향적 의미와 더 가까울수록, 독자의 문제 해결 기대감을 높여 비브랜드 평균 클릭률(non_brand_average_ctr)이 높아질 것이다. 이 피처는 우리 포스트의 실용적 가치가 경쟁사 대비 얼마나 우위에 있는지를 정량화한다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the ratio of a post's semantic 'actionability' score compared to the\n    average score of its competitors for the same representative query.\n    \"\"\"\n    feature_name = \"semantic_actionability_vs_competitors\"\n\n    # Handle empty DataFrame gracefully\n    if not df.empty:\n        # Preserve the original index to ensure correct feature assignment later.\n        # This is critical to prevent data misalignment.\n        original_index = df.index\n\n        # Get the pre-trained model.\n        model = get_model()\n\n        # 1. Define 'actionability' concepts and create an ideal semantic vector.\n        actionability_phrases = [\n            \"자세한 설명\", \"해결 방법\", \"단계별 가이드\", \"실용적인 팁\",\n            \"문제를 해결하는 과정\", \"어떻게 하는지\", \"순서대로 알아보기\"\n        ]\n        actionability_embeddings = model.encode(actionability_phrases, convert_to_tensor=True)\n        ideal_actionability_vector = util.normalize_embeddings(actionability_embeddings).mean(axis=0)\n\n        # 2. Encode all post bodies in a single, efficient batch.\n        bodies = df['post_body'].fillna('').astype(str).tolist()\n        body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n        # 3. Calculate the actionability score for every post against the ideal vector.\n        actionability_scores = util.cos_sim(body_embeddings, ideal_actionability_vector).flatten().cpu().numpy()\n\n        # 4. Create a temporary DataFrame to hold scores and grouping info, using the original index.\n        temp_df = pd.DataFrame({\n            'temp_actionability_score': actionability_scores,\n            'representative_query': df['representative_query'],\n            'source': df['source']\n        }, index=original_index)\n\n        # 5. Calculate the average competitor score for each representative query.\n        df_competitors = temp_df[temp_df['source'] == 'competitor']\n        \n        if not df_competitors.empty:\n            competitor_avg_scores = df_competitors.groupby('representative_query')['temp_actionability_score'].mean().rename('competitor_avg_actionability')\n            temp_df = temp_df.merge(competitor_avg_scores, on='representative_query', how='left')\n        else:\n            # If no competitors exist in the dataset, create a placeholder column to avoid errors.\n            temp_df['competitor_avg_actionability'] = np.nan\n\n        # 6. Calculate the final feature: the ratio of our score to the competitor average.\n        # This is calculated only for 'ours' posts.\n        is_ours = temp_df['source'] == 'ours'\n        our_scores = temp_df.loc[is_ours, 'temp_actionability_score']\n        competitor_avg = temp_df.loc[is_ours, 'competitor_avg_actionability']\n\n        # The ratio will be NaN if the competitor average is NaN or zero, which is correct.\n        ratio = our_scores / competitor_avg\n\n        # 7. Assign the calculated feature back to the original DataFrame using the preserved index.\n        df[feature_name] = ratio\n\n    else:\n        # If the input DataFrame is empty, create an empty column for the feature.\n        df[feature_name] = np.nan\n\n    return df\n",
  "analysis": {
    "correlation": 0.13394268979288124,
    "p_value": 0.19564159659108277,
    "interpretation": "약한 양의 상관관계(0.1339)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.1956)."
  }
}{
  "timestamp": "2025-07-22T05:25:24.956113",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_distance_from_ideal",
  "hypothesis": "포스트의 제목과 본문을 결합한 의미론적 벡터가, 동일 대표 검색어에 대한 '상위 성과' 경쟁자 그룹의 평균적인 의미론적 중심(Centroid)에 가까울수록 (즉, 거리가 짧을수록), 사용자의 검색 의도에 더 부합하고 핵심 주제에 집중하는 것으로 간주되어 클릭률(CTR)이 높을 것이다. 이 피처는 단순히 모든 경쟁자를 벤치마킹하는 대신, 성과가 좋은 경쟁자들의 의미론적 특징을 '이상적인 목표'로 설정하여 거리를 측정한다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a SentenceTransformer model, loading it only once.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model that is effective for Korean.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic distance of 'our' posts to the centroid of \n    high-performing competitor posts for each representative query.\n\n    Args:\n        df: DataFrame containing post data with columns 'source', 'representative_query',\n            'post_title', 'post_body', and 'non_brand_average_ctr'.\n\n    Returns:\n        DataFrame with a new column 'semantic_distance_from_ideal'.\n    \"\"\"\n    # 0. Handle empty DataFrame\n    if df.empty:\n        df['semantic_distance_from_ideal'] = pd.Series(dtype=float)\n        return df\n\n    # Preserve the original index to ensure correct assignment later\n    original_index_df = df.copy()\n\n    model = get_model()\n\n    # 1. Prepare data for encoding\n    # Combine title and body for a holistic semantic representation.\n    # Fill NaNs to prevent errors during string concatenation.\n    titles = original_index_df['post_title'].fillna('').astype(str)\n    bodies = original_index_df['post_body'].fillna('').astype(str)\n    full_text = (titles + ' ' + bodies).tolist()\n\n    # 2. Batch encode all documents for efficiency\n    # This is significantly faster than row-by-row .apply()\n    all_embeddings = model.encode(full_text, convert_to_tensor=True, show_progress_bar=False)\n\n    # Initialize the feature column with NaN\n    original_index_df['semantic_distance_from_ideal'] = np.nan\n\n    # 3. Group by query to calculate per-query benchmarks\n    # Using the original DataFrame's index for grouping ensures alignment\n    for query, group_indices in original_index_df.groupby('representative_query').groups.items():\n        group_df = original_index_df.loc[group_indices]\n\n        # Separate our posts from competitor posts\n        ours_df = group_df[group_df['source'] == 'ours']\n        competitors_df = group_df[group_df['source'] == 'competitor']\n\n        # Skip if there are no 'ours' posts or no competitors to compare against\n        if ours_df.empty or competitors_df.empty:\n            continue\n\n        # 4. Identify high-performing competitors to create an 'ideal' benchmark\n        # This prevents low-quality competitors from skewing the ideal point.\n        median_ctr = competitors_df['non_brand_average_ctr'].median()\n        if pd.isna(median_ctr):\n            # If CTRs are all NaN, we can't determine performance. Use all competitors as a fallback.\n            high_perf_competitors_df = competitors_df\n        else:\n            # Define high-performing as having CTR at or above the median for the group\n            high_perf_competitors_df = competitors_df[competitors_df['non_brand_average_ctr'] >= median_ctr]\n        \n        # Fallback if the median filter removes all competitors (e.g., all have same low CTR)\n        if high_perf_competitors_df.empty:\n            high_perf_competitors_df = competitors_df\n\n        # 5. Get embeddings for the current group using their positional indices\n        # This correctly maps DataFrame indices (labels) to tensor positions (integers).\n        our_positions = original_index_df.index.get_indexer(ours_df.index)\n        competitor_positions = original_index_df.index.get_indexer(high_perf_competitors_df.index)\n        \n        our_embeddings = all_embeddings[our_positions]\n        competitor_embeddings = all_embeddings[competitor_positions]\n\n        if competitor_embeddings.shape[0] == 0:\n            continue\n\n        # 6. Calculate the semantic centroid of high-performing competitors\n        centroid = torch.mean(competitor_embeddings, dim=0)\n\n        # 7. Calculate cosine distance from our posts to the ideal centroid\n        # Cosine Distance = 1 - Cosine Similarity\n        similarities = util.cos_sim(our_embeddings, centroid)\n        distances = 1 - similarities\n\n        # The result is a 2D tensor (e.g., [[0.1], [0.2]]), flatten it for assignment.\n        distances_flat = distances.flatten().cpu().numpy()\n\n        # 8. Assign the calculated distances back to the correct rows in the main DataFrame\n        original_index_df.loc[ours_df.index, 'semantic_distance_from_ideal'] = distances_flat\n\n    return original_index_df",
  "analysis": {
    "correlation": 0.0859909837270764,
    "p_value": 0.40734366182824,
    "interpretation": "약한 양의 상관관계(0.0860)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.4073)."
  }
}{
  "timestamp": "2025-07-22T05:45:38.984875",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_title_body_semantic_cohesion",
  "hypothesis": "우리 포스트의 제목과 본문 간 의미적 일관성(cohesion)이 동일 대표 검색어에 대한 경쟁자 그룹의 평균적인 일관성보다 높을수록, 사용자는 해당 포스트를 주제에 더 집중되고 신뢰할 수 있다고 판단하여 더 높은 클릭률(CTR)로 이어질 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Use a multilingual model robust for various content types.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic cohesion between a post's title and body, and benchmarks\n    it against the average cohesion of competitor posts for the same representative query.\n\n    The final feature is a ratio: (Our Post's Cohesion) / (Competitor Group's Average Cohesion).\n    A value > 1 suggests higher-than-average semantic consistency.\n    A value < 1 suggests lower-than-average semantic consistency.\n    \"\"\"\n    feature_name = 'relative_title_body_semantic_cohesion'\n    \n    # Initialize the feature column with a neutral value (1.0) or NaN.\n    # Using 1.0 is often safer for downstream models.\n    df[feature_name] = 1.0\n\n    if df.empty:\n        return df\n\n    # Ensure required columns are present\n    required_cols = ['post_title', 'post_body', 'representative_query', 'source']\n    if not all(col in df.columns for col in required_cols):\n        # Return df with the initialized column if data is insufficient\n        return df\n\n    # Use a copy to avoid SettingWithCopyWarning and to preserve the original DataFrame's index\n    df_copy = df.copy()\n\n    model = get_model()\n\n    # 2. Process data in batches for efficiency. NEVER use .apply() for encoding.\n    titles = df_copy['post_title'].fillna('').astype(str).tolist()\n    bodies = df_copy['post_body'].fillna('').astype(str).tolist()\n\n    # Encode all texts at once\n    title_embeddings = model.encode(titles, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # 3. Calculate cohesion for EVERY post simultaneously.\n    # util.cos_sim returns an (N, N) matrix. .diag() correctly extracts the element-wise similarity\n    # sim(title_i, body_i) for each post i, resulting in a tensor of shape (N,).\n    cohesion_scores = util.cos_sim(title_embeddings, body_embeddings).diag()\n    df_copy['temp_cohesion'] = cohesion_scores.cpu().tolist()\n\n    # Store results in a dictionary {index: value} to map back safely\n    results = {}\n\n    # 4. Group by query and calculate the relative score for 'ours' posts.\n    # This approach preserves the original index, preventing data misalignment.\n    for query, group in df_copy.groupby('representative_query'):\n        ours_posts = group[group['source'] == 'ours']\n        competitor_posts = group[group['source'] == 'competitor']\n\n        # Proceed only if both 'ours' and 'competitor' posts exist for the query\n        if ours_posts.empty or competitor_posts.empty:\n            continue\n\n        # Calculate the average cohesion for the competitor group\n        avg_competitor_cohesion = competitor_posts['temp_cohesion'].mean()\n\n        # Avoid division by zero. If benchmark is 0, relative score is undefined or neutral.\n        # We treat it as neutral (1.0) because there's no meaningful benchmark to compare against.\n        if pd.isna(avg_competitor_cohesion) or avg_competitor_cohesion == 0:\n            for idx in ours_posts.index:\n                results[idx] = 1.0\n            continue\n\n        # Calculate the ratio for each 'ours' post in the group\n        for idx, our_post in ours_posts.iterrows():\n            our_cohesion = our_post['temp_cohesion']\n            relative_cohesion = our_cohesion / avg_competitor_cohesion\n            results[idx] = relative_cohesion\n\n    # 5. Safely map the calculated results back to the original DataFrame's column\n    # using the original index. This is the critical step to prevent misalignment.\n    if results:\n        df[feature_name] = df.index.map(results).fillna(1.0)\n\n    return df",
  "analysis": {
    "correlation": 0.10142660490691904,
    "p_value": 0.3280633685603432,
    "interpretation": "약한 양의 상관관계(0.1014)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3281)."
  }
}{
  "timestamp": "2025-07-22T06:05:40.903836",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_semantic_actionability",
  "hypothesis": "포스트 본문이 경쟁사 대비 '방법', '해결', '가이드' 등 실용적/행동 지향적 의미에 더 가까울수록, 사용자는 해당 콘텐츠가 자신의 문제를 직접 해결해 줄 것이라 기대하여 클릭률(CTR)이 높아질 것이다. 이 피처는 우리 포스트의 실용적 의미 점수를 경쟁사 그룹의 평균 점수로 나눈 값으로, 1보다 크면 경쟁사보다 더 행동 지향적인 콘텐츠임을 의미한다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a pre-trained sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a model that performs well for Korean and English\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the 'relative_semantic_actionability' feature.\n\n    This feature measures how action-oriented our post's body text is compared to the\n    average of its competitors for the same representative query. It is calculated by:\n    1. Defining a set of 'actionability' keywords (e.g., 'how to', 'solution', '방법').\n    2. Creating a semantic 'actionability' vector by averaging the embeddings of these keywords.\n    3. Calculating the cosine similarity of each post's body embedding to this actionability vector.\n    4. For each of our posts, computing the ratio of its score to the average score of its competitors.\n    \"\"\"\n    feature_name = 'relative_semantic_actionability'\n\n    # Immediately return the original DataFrame if it's empty, adding the empty feature column.\n    if not df.empty:\n        df[feature_name] = np.nan\n    else:\n        return df\n\n    # Define action-oriented keywords in Korean and English for broader semantic capture.\n    action_keywords = [\n        '방법', '해결', '가이드', '팁', '전략', '단계', '하는 법', '솔루션',\n        'how to', 'solution', 'guide', 'tip', 'strategy', 'steps', 'tutorial'\n    ]\n\n    try:\n        model = get_model()\n\n        # Create a single 'actionability' vector by averaging keyword embeddings.\n        action_embeddings = model.encode(action_keywords, convert_to_tensor=True, show_progress_bar=False)\n        actionability_vector = action_embeddings.mean(dim=0)\n\n        # Prepare and encode all post bodies in one batch for efficiency.\n        # Preserving original index is crucial.\n        df_work = df.copy()\n        bodies = df_work['post_body'].fillna('').astype(str).tolist()\n        body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n        # Ensure both tensors are on the same device before similarity calculation.\n        actionability_vector_device = actionability_vector.to(body_embeddings.device)\n\n        # Calculate the actionability score for every post.\n        actionability_scores = util.cos_sim(body_embeddings, actionability_vector_device)\n        df_work['temp_action_score'] = actionability_scores.cpu().numpy().flatten()\n\n        # Group by query to calculate the ratio against the correct competitors.\n        # The loop iterates through each group but assignments are made back to the main DataFrame\n        # using original indices, ensuring correctness.\n        for query, group in df_work.groupby('representative_query'):\n            ours_posts = group[group['source'] == 'ours']\n            competitor_posts = group[group['source'] == 'competitor']\n\n            # Proceed only if both our post and competitor posts exist for the query.\n            if ours_posts.empty or competitor_posts.empty:\n                continue\n\n            # Calculate the average actionability score for all competitors in the group.\n            avg_competitor_score = competitor_posts['temp_action_score'].mean()\n\n            # Get the score for our post(s).\n            our_scores = ours_posts['temp_action_score']\n            our_indices = ours_posts.index\n\n            # Calculate the ratio and assign it back to the original DataFrame.\n            # Handle division by zero or near-zero if competitors have no actionability.\n            if avg_competitor_score > 1e-6:\n                ratio = our_scores / avg_competitor_score\n                df.loc[our_indices, feature_name] = ratio\n            else:\n                # If competitors have a score of 0, our post is infinitely better.\n                # Capping at a high value like 5.0 is a reasonable default.\n                # Or setting to 1.0 implies 'no difference'. Let's use 1.0 for neutrality.\n                df.loc[our_indices, feature_name] = 1.0\n\n    except Exception as e:\n        # In case of an error (e.g., model loading), log it and return the original df.\n        print(f\"Error generating {feature_name}: {e}\")\n        # Ensure the column exists even if calculation fails.\n        if feature_name not in df.columns:\n            df[feature_name] = np.nan\n\n    return df",
  "analysis": {
    "correlation": 0.3049536049760363,
    "p_value": 0.002656747140101359,
    "interpretation": "중간 정도의 양의 상관관계(0.3050)를 발견했습니다. 이 결과는 통계적으로 유의미합니다(p-value: 0.0027)."
  }
}{
  "timestamp": "2025-07-22T06:24:57.396070",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_semantic_actionability_score",
  "hypothesis": "A post's body text that is semantically more aligned with practical, 'how-to' solutions compared to the average of its competitors will attract more clicks. This increased 'relative actionability' suggests content that better matches user intent for solutions, leading to a higher `non_brand_average_ctr`.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a relative semantic actionability score for each 'ours' post.\n    This score is the ratio of a post's body text actionability score to the\n    average actionability score of its competitors for the same query.\n    \"\"\"\n    feature_name = 'relative_semantic_actionability_score'\n\n    # Always check for an empty DataFrame\n    if not df.empty:\n        pass\n    else:\n        df[feature_name] = np.nan\n        return df\n\n    model = get_model()\n    device = model.device\n\n    # Define \"actionability\" probe sentences in Korean\n    actionability_probes = [\n        \"문제를 해결하는 구체적인 방법\",\n        \"단계별 실행 가이드\",\n        \"실용적인 문제 해결책\",\n        \"스스로 할 수 있는 방법\",\n        \"자세한 해결 과정 설명\"\n    ]\n\n    # Encode probes and calculate the ideal actionability centroid vector\n    with torch.no_grad():\n        probe_embeddings = model.encode(actionability_probes, convert_to_tensor=True, device=device)\n        actionability_centroid = torch.mean(probe_embeddings, dim=0)\n\n    # Prepare post bodies for encoding\n    post_bodies = df['post_body'].fillna('').astype(str).tolist()\n\n    # Batch encode all post bodies\n    with torch.no_grad():\n        body_embeddings = model.encode(post_bodies, convert_to_tensor=True, show_progress_bar=False, device=device)\n\n    # Calculate cosine similarity of each post body to the actionability centroid\n    actionability_scores = util.cos_sim(body_embeddings, actionability_centroid).flatten()\n\n    # Create a temporary DataFrame to hold scores and facilitate calculations\n    temp_df = pd.DataFrame({\n        'representative_query': df['representative_query'],\n        'source': df['source'],\n        'actionability_score': actionability_scores.cpu().numpy()\n    }, index=df.index)\n\n    # Calculate the average actionability score of competitors for each query\n    competitor_scores = temp_df[temp_df['source'] == 'competitor']\n    query_avg_map = competitor_scores.groupby('representative_query')['actionability_score'].mean()\n\n    # Map the competitor average to each row based on its query\n    temp_df['competitor_avg_actionability'] = temp_df['representative_query'].map(query_avg_map)\n\n    # Isolate 'ours' posts for the final calculation\n    ours_mask = temp_df['source'] == 'ours'\n    ours_rows = temp_df[ours_mask]\n\n    # Safely calculate the ratio for 'ours' posts\n    # Default to 1.0 (neutral) if competitor average is missing or zero\n    relative_scores = np.divide(\n        ours_rows['actionability_score'],\n        ours_rows['competitor_avg_actionability'],\n        out=np.full(len(ours_rows), 1.0),\n        where=ours_rows['competitor_avg_actionability'].notna() & (ours_rows['competitor_avg_actionability'] != 0)\n    )\n\n    # Assign the calculated scores to a new series with the correct index\n    final_scores = pd.Series(relative_scores, index=ours_rows.index)\n\n    # Map the scores back to the original DataFrame's new column\n    df[feature_name] = final_scores\n\n    # Fill all other rows (competitors, and 'ours' posts that failed calculation) with a neutral 1.0\n    df[feature_name].fillna(1.0, inplace=True)\n\n    return df\n",
  "analysis": {
    "correlation": 0.1601918983800022,
    "p_value": 0.12096866733918003,
    "interpretation": "약한 양의 상관관계(0.1602)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.1210)."
  }
}{
  "timestamp": "2025-07-22T06:47:15.514997",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_actionability_vs_competitors",
  "hypothesis": "포스트 본문이 특정 검색어의 경쟁자 그룹 평균보다 '문제 해결 방법', '단계별 가이드'와 같은 실용적/행동 지향적 의미에 더 가까울수록, 사용자는 해당 콘텐츠가 자신의 검색 의도를 더 잘 충족시킬 것으로 기대하게 되어 클릭률(non_brand_average_ctr)이 높아질 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a SentenceTransformer model using a global cache.\"\"\"\n    global _model\n    if _model is None:\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a feature that benchmarks the 'actionability' of our content against competitors.\n\n    The feature measures how semantically close a post's body is to action-oriented concepts\n    (like 'how-to', 'step-by-step') compared to the average competitor for the same query.\n    A value > 1 indicates our post is more 'actionable' than the competitor average.\n    \"\"\"\n    feature_name = 'semantic_actionability_vs_competitors'\n\n    if not df.empty and 'post_body' in df.columns:\n        model = get_model()\n\n        # Define probe sentences that represent the concept of 'actionability'\n        actionability_probes = [\n            \"문제를 해결하는 방법\",\n            \"단계별 실행 가이드\",\n            \"실용적인 팁과 노하우\",\n            \"어떻게 해야 하나요?\",\n            \"자세한 과정 설명\"\n        ]\n\n        # Encode probes and create a single 'ideal actionability' vector by averaging\n        probe_embeddings = model.encode(actionability_probes, convert_to_tensor=True)\n        ideal_actionability_vector = probe_embeddings.mean(dim=0)\n\n        # Preserve the original index to ensure correct assignment later\n        original_index = df.index\n        bodies = df['post_body'].fillna('').tolist()\n        body_embeddings = model.encode(bodies, convert_to_tensor=True)\n\n        # Calculate the cosine similarity of each post body to the ideal vector\n        actionability_scores = util.cos_sim(body_embeddings, ideal_actionability_vector).squeeze().cpu().numpy()\n        \n        # Use a temporary DataFrame to hold scores and align them with the original index\n        temp_df = pd.DataFrame({\n            'temp_actionability_score': actionability_scores,\n            'source': df['source'],\n            'representative_query': df['representative_query']\n        }, index=original_index)\n\n        # Calculate the average actionability score for competitors within each query group\n        competitor_scores_df = temp_df[temp_df['source'] == 'competitor']\n        \n        # Handle case where there are no competitors at all in the dataset\n        if not competitor_scores_df.empty:\n            competitor_avg_by_query = competitor_scores_df.groupby('representative_query')['temp_actionability_score'].mean()\n            # Map the average competitor score back to each row in the temp DataFrame\n            temp_df['competitor_avg_score'] = temp_df['representative_query'].map(competitor_avg_by_query)\n        else:\n            # If no competitors exist, there's no benchmark. We'll handle this by filling NaNs later.\n            temp_df['competitor_avg_score'] = np.nan\n\n        # Initialize the final feature column with a default of NaN\n        df[feature_name] = np.nan\n\n        # Identify rows corresponding to 'ours' posts\n        our_rows_mask = (temp_df['source'] == 'ours')\n\n        # Get the actionability scores for our posts\n        our_actionability_score = temp_df.loc[our_rows_mask, 'temp_actionability_score']\n        \n        # Get the corresponding average competitor score for our posts\n        competitor_avg_for_ours = temp_df.loc[our_rows_mask, 'competitor_avg_score']\n\n        # To avoid division by zero or by NaN, replace problematic denominators with a value \n        # that results in a neutral ratio of 1. We use the score of our own post as a fallback,\n        # effectively making the ratio 1 if the competitor average is 0 or NaN.\n        safe_denominator = competitor_avg_for_ours.fillna(our_actionability_score).replace(0, 1)\n        # If the denominator became 0 after all that (e.g. our score was also 0), just make it 1.\n        safe_denominator[safe_denominator == 0] = 1\n        \n        # Calculate the ratio\n        ratio = our_actionability_score / safe_denominator\n        \n        # Assign the calculated ratio to the final feature column for 'ours' posts\n        df.loc[our_rows_mask, feature_name] = ratio\n\n    else:\n        # If the dataframe is empty, ensure the column exists with the correct dtype\n        df[feature_name] = pd.Series(dtype=float)\n\n    return df",
  "analysis": {
    "correlation": 0.1600846736795018,
    "p_value": 0.12122099053928387,
    "interpretation": "약한 양의 상관관계(0.1601)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.1212)."
  }
}{
  "timestamp": "2025-07-22T07:08:15.114590",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_differentiation_from_competitors",
  "hypothesis": "Our posts that are more semantically differentiated from their closest competitor (i.e., offer a unique angle or cover the topic differently) will attract a higher click-through rate from the SERP, as they stand out from the homogenous content.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for the Korean text in the dataset.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic differentiation of each 'ours' post from its closest competitor\n    for the same representative query.\n    \"\"\"\n    feature_name = 'semantic_differentiation_from_competitors'\n    df[feature_name] = np.nan\n\n    if df.empty or 'representative_query' not in df.columns:\n        return df\n\n    model = get_model()\n\n    # Filter for posts that can be compared\n    valid_df = df[df['representative_query'].notna()].copy()\n    if valid_df.empty:\n        return df\n\n    # Group by the query to compare our posts with their direct competitors\n    grouped = valid_df.groupby('representative_query')\n\n    all_our_indices = []\n    all_differentiation_scores = []\n\n    for _, group in grouped:\n        ours_df = group[group['source'] == 'ours']\n        competitors_df = group[group['source'] == 'competitor']\n\n        # Proceed only if there's at least one of our posts and one competitor to compare against\n        if ours_df.empty or competitors_df.empty:\n            continue\n\n        # 2. Use efficient batch processing for embeddings\n        our_bodies = ours_df['post_body'].fillna('').astype(str).tolist()\n        competitor_bodies = competitors_df['post_body'].fillna('').astype(str).tolist()\n\n        # Encode bodies into semantic vectors (embeddings)\n        our_embeddings = model.encode(our_bodies, convert_to_tensor=True, show_progress_bar=False)\n        competitor_embeddings = model.encode(competitor_bodies, convert_to_tensor=True, show_progress_bar=False)\n\n        # Calculate cosine similarity between each of our posts and all competitor posts\n        # The result is a matrix of shape (num_ours, num_competitors)\n        cos_sim_matrix = util.cos_sim(our_embeddings, competitor_embeddings)\n\n        # For each of our posts, find the similarity to its *closest* competitor\n        # A higher similarity score means it's very similar to at least one competitor\n        if cos_sim_matrix.numel() == 0:\n            continue\n        max_similarity_to_competitor, _ = torch.max(cos_sim_matrix, dim=1)\n\n        # The differentiation score is 1 minus the max similarity.\n        # A score closer to 1 means the post is highly unique.\n        # A score closer to 0 means it's very similar to a competitor.\n        differentiation_scores = 1.0 - max_similarity_to_competitor\n        \n        # Store the original indices and scores for batch assignment later\n        all_our_indices.extend(ours_df.index.tolist())\n        all_differentiation_scores.extend(differentiation_scores.cpu().numpy().tolist())\n\n    # 3. Assign the calculated features back to the original DataFrame using their indices.\n    # This is a safe and efficient way to update the main frame after processing.\n    if all_our_indices:\n        df.loc[all_our_indices, feature_name] = all_differentiation_scores\n\n    return df",
  "analysis": {
    "correlation": 0.13393062490464774,
    "p_value": 0.19568222566948637,
    "interpretation": "약한 양의 상관관계(0.1339)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.1957)."
  }
}{
  "timestamp": "2025-07-22T08:29:18.545493",
  "attempt": 1,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'title_body_cohesion_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# Use a global variable to lazy-load the model, avoiding re-initialization on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a well-regarded multilingual model suitable for Korean text.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic cohesion between a post's title and body and compares it\n    to the average cohesion of competitors for the same representative query.\n\n    Args:\n        df (pd.DataFrame): The input dataframe with columns 'post_title', 'post_body',\n                           'source', and 'representative_query'.\n\n    Returns:\n        pd.DataFrame: The dataframe with the new feature column 'title_body_cohesion_vs_competitors'.\n    \"\"\"\n    # 1. Handle empty DataFrame input gracefully.\n    if df.empty:\n        df['title_body_cohesion_vs_competitors'] = pd.Series(dtype=float)\n        return df\n\n    # Preserve the original index to ensure correct final assignment.\n    original_index = df.index\n    df_copy = df.copy()\n\n    # 2. Get the pre-trained model.\n    try:\n        model = get_model()\n    except Exception as e:\n        # If model loading fails, return the original df with a default column.\n        print(f\"Error loading SentenceTransformer model: {e}\")\n        df['title_body_cohesion_vs_competitors'] = 1.0\n        return df\n\n    # 3. Prepare text data, ensuring no NaN values.\n    titles = df_copy['post_title'].fillna('').astype(str).tolist()\n    bodies = df_copy['post_body'].fillna('').astype(str).tolist()\n\n    # 4. Efficiently generate embeddings for all titles and bodies in batches.\n    title_embeddings = model.encode(titles, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # 5. Calculate the title-body cohesion score for every post.\n    # The diagonal of the cosine similarity matrix gives the sim(title_i, body_i).\n    cohesion_scores = util.cos_sim(title_embeddings, body_embeddings).diag()\n    df_copy['internal_cohesion'] = cohesion_scores.cpu().numpy()\n\n    # 6. Calculate the benchmark: the average cohesion for competitors in each query group.\n    competitors_df = df_copy[df_copy['source'] == 'competitors']\n    if not competitors_df.empty:\n        competitor_avg_cohesion = competitors_df.groupby('representative_query')['internal_cohesion'].mean().rename('competitor_avg_cohesion')\n        # Merge the benchmark back into the main dataframe.\n        df_copy = df_copy.merge(competitor_avg_cohesion, on='representative_query', how='left')\n    else:\n        # If there are no competitors at all in the dataframe, create an empty benchmark column.\n        df_copy['competitor_avg_cohesion'] = np.nan\n\n    # 7. Calculate the final feature.\n    # Default to 1.0 (neutral) for competitors and for 'ours' posts without a benchmark.\n    feature_series = pd.Series(1.0, index=df_copy.index)\n\n    # Identify 'ours' posts that have a valid competitor benchmark to compare against.\n    # A valid benchmark is non-null and greater than zero to avoid division issues.\n    mask_ours_with_benchmark = (\n        (df_copy['source'] == 'ours') & \n        (df_copy['competitor_avg_cohesion'].notna()) & \n        (df_copy['competitor_avg_cohesion'] > 0)\n    )\n    \n    # For the valid 'ours' posts, calculate the relative score.\n    if mask_ours_with_benchmark.any():\n        feature_series.loc[mask_ours_with_benchmark] = \\\n            df_copy.loc[mask_ours_with_benchmark, 'internal_cohesion'] / df_copy.loc[mask_ours_with_benchmark, 'competitor_avg_cohesion']\n\n    # 8. Assign the new feature to the original DataFrame using the preserved index.\n    # This is a robust way to handle potential reordering during merges.\n    df['title_body_cohesion_vs_competitors'] = pd.Series(feature_series.values, index=df_copy.index).reindex(original_index)\n    \n    # Handle any potential inf/nan values that might have slipped through, setting them to a neutral 1.0\n    df['title_body_cohesion_vs_competitors'].replace([np.inf, -np.inf], np.nan, inplace=True)\n    df['title_body_cohesion_vs_competitors'].fillna(1.0, inplace=True)\n\n    return df"
}{
  "timestamp": "2025-07-22T08:31:32.569379",
  "attempt": 2,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'title_body_cohesion_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a CPU device explicitly to avoid potential CUDA issues in diverse environments\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device='cpu')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the title-body semantic cohesion for each post and compares it to the competitor average for the same query.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with columns including 'post_title', 'post_body', 'source', and 'representative_query'.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the new feature column 'title_body_cohesion_vs_competitors'.\n                      The feature value is a ratio: (our post's cohesion) / (competitors' average cohesion).\n                      - A value > 1.0 means our post has higher cohesion than the competitor average.\n                      - A value < 1.0 means our post has lower cohesion.\n                      - A value of 1.0 is the default/neutral value, assigned to competitor posts or when no competitor data is available for a query.\n    \"\"\"\n    feature_name = 'title_body_cohesion_vs_competitors'\n\n    if df.empty:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n\n    try:\n        model = get_model()\n\n        # 2. Pre-process text data and encode in batches for efficiency\n        titles = df['post_title'].fillna('').astype(str).tolist()\n        bodies = df['post_body'].fillna('').astype(str).tolist()\n\n        title_embeddings = model.encode(titles, convert_to_tensor=True)\n        body_embeddings = model.encode(bodies, convert_to_tensor=True)\n\n        # 3. Calculate cohesion score for every single post\n        # The .diag() method correctly pairs the i-th title with the i-th body\n        cohesion_scores = util.cos_sim(title_embeddings, body_embeddings).diag()\n        df['temp_cohesion_score'] = cohesion_scores.cpu().numpy()\n\n        # 4. Calculate the average cohesion for competitors for each query group\n        competitors_df = df[df['source'] == 'competitors']\n        competitor_avg_cohesion = competitors_df.groupby('representative_query')['temp_cohesion_score'].mean()\n        \n        # 5. Map the competitor average back to each row in the original dataframe\n        df['competitor_benchmark'] = df['representative_query'].map(competitor_avg_cohesion)\n\n        # 6. Calculate the relative score for 'ours' posts, which is the core of the feature\n        # Initialize the final column with a neutral value of 1.0 for all posts first.\n        df[feature_name] = 1.0\n        \n        ours_mask = df['source'] == 'ours'\n        \n        # Isolate scores and benchmarks specifically for 'ours' posts\n        our_cohesion = df.loc[ours_mask, 'temp_cohesion_score']\n        our_benchmark = df.loc[ours_mask, 'competitor_benchmark']\n\n        # Create a mask for valid benchmarks (not NaN and not zero) to avoid division errors\n        valid_benchmark_mask = our_benchmark.notna() & (our_benchmark != 0)\n\n        # Calculate the relative score only for 'ours' posts where a valid benchmark exists.\n        # This comparison of an *individual* post's score against a *group* average ensures that\n        # 'ours' posts within the same query group can have different feature values,\n        # thus creating the variance needed for correlation analysis.\n        relative_scores = our_cohesion[valid_benchmark_mask] / our_benchmark[valid_benchmark_mask]\n        \n        # Update the feature column for 'ours' posts where the calculation was possible\n        df.loc[relative_scores.index, feature_name] = relative_scores\n\n        # Clean up temporary columns\n        df.drop(columns=['temp_cohesion_score', 'competitor_benchmark'], inplace=True)\n\n    except Exception as e:\n        # In case of any error, fall back to a neutral value to ensure stability.\n        df[feature_name] = 1.0\n\n    return df\n"
}{
  "timestamp": "2025-07-22T08:32:27.612699",
  "attempt": 3,
  "status": "success",
  "feature_name": "body_solution_orientation_score",
  "hypothesis": "포스트 본문이 '해결책', '방법', '가이드' 등 해결 지향적 개념을 대표하는 이상적인 벡터에 의미론적으로 가까울수록, 즉 해결 지향성 점수가 높을수록 `non_brand_average_ctr`이 높을 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a pre-trained SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a well-regarded Korean-capable multilingual model\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a 'solution orientation' score for each post body.\n    \n    This feature measures the semantic similarity of each post's body to a predefined set of\n    \"solution-oriented\" phrases. The hypothesis is that content which is more aligned with\n    providing solutions, guides, or clear methods will have a higher CTR. This approach\n    avoids the 'constant value' error from previous attempts by calculating an absolute\n    score for each post, rather than a relative score against a group average, thus\n    ensuring variance across the dataset.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing post data. Must include 'post_body'.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the new 'body_solution_orientation_score' column added.\n    \"\"\"\n    # 2. Check for an empty DataFrame to prevent errors.\n    feature_name = 'body_solution_orientation_score'\n    if df.empty:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n\n    try:\n        model = get_model()\n        \n        # 3. Define a set of \"solution-oriented\" keywords and phrases.\n        # These phrases represent an \"ideal\" practical, helpful document.\n        solution_phrases = [\n            \"해결 방법\", \"자세히 알아보기\", \"단계별 가이드\", \"문제를 해결하는 법\",\n            \"팁과 요령\", \"원인과 해결책\", \"어떻게 해야 하나요?\", \"A to Z 정리\",\n            \"핵심 요약\", \"빠른 해결\"\n        ]\n\n        # 4. Create the 'ideal' solution vector by encoding the phrases and averaging them.\n        # This creates a single benchmark vector representing \"solution orientation\".\n        solution_embeddings = model.encode(solution_phrases, convert_to_tensor=True)\n        ideal_solution_vector = torch.mean(solution_embeddings, dim=0, keepdim=True)\n\n        # 5. Prepare and encode post bodies in a single batch for efficiency.\n        post_bodies = df['post_body'].fillna('').astype(str).tolist()\n        body_embeddings = model.encode(post_bodies, convert_to_tensor=True, show_progress_bar=False)\n\n        # 6. Calculate cosine similarity between each post body and the ideal solution vector.\n        # This gives each post an absolute score based on its content.\n        # The result is a (num_posts x 1) tensor.\n        cosine_scores = util.cos_sim(body_embeddings, ideal_solution_vector)\n        \n        # 7. Assign the new feature column. .flatten() converts the Nx1 tensor to a 1D array.\n        df[feature_name] = cosine_scores.cpu().numpy().flatten()\n\n    except Exception as e:\n        # In case of any error (e.g., model loading, encoding), fill with a neutral value.\n        print(f\"An error occurred during feature generation for '{feature_name}': {e}\")\n        df[feature_name] = 0.0\n\n    return df\n",
  "analysis": {
    "correlation": -0.10846781107879715,
    "p_value": 0.29541859696881917,
    "interpretation": "약한 음의 상관관계(-0.1085)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.2954)."
  }
}{
  "timestamp": "2025-07-22T10:19:26.754876",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_semantic_expertise_score",
  "hypothesis": "우리 포스트의 본문이 '전문의', '진단', '연구', '과학적 근거' 등 전문적이고 신뢰도를 높이는 어휘와 의미적으로 더 가까울수록, 그리고 이러한 전문성 점수가 경쟁 포스트 그룹의 평균보다 높을수록, 사용자는 해당 콘텐츠를 더 신뢰하고 클릭할 가능성이 높아져 `non_brand_average_ctr`이 향상될 것입니다. 이 피처는 콘텐츠의 권위성을 정량화하고 경쟁 우위를 측정합니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a model that is strong for Korean and general semantics.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a relative semantic expertise score for 'ours' posts compared to competitors.\n\n    The score measures how closely a post's body text aligns with a predefined set of\n    'expertise' keywords, relative to the average expertise score of competitors\n    within the same representative query.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with post data.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the new feature column 'relative_semantic_expertise_score'.\n    \"\"\"\n    feature_name = 'relative_semantic_expertise_score'\n\n    # Handle empty or invalid input by returning the df with a default column\n    if df.empty or not all(col in df.columns for col in ['post_body', 'source', 'representative_query']):\n        df[feature_name] = 1.0\n        return df\n\n    # Make a copy to safely add temporary columns\n    df_copy = df.copy()\n\n    model = get_model()\n\n    # Define expertise-related keywords to create a semantic benchmark for professionalism\n    expertise_keywords = [\n        '전문의', '진단', '연구', '과학적', '논문', '정확한', '치료 계획',\n        '분석', '의료진', '데이터', '검증된', '원리', '원칙', '학술적'\n    ]\n\n    # Encode keywords and post bodies in efficient batches\n    with torch.no_grad():\n        expertise_embeddings = model.encode(expertise_keywords, convert_to_tensor=True)\n        # Create a single 'ideal expertise' vector by averaging the keyword embeddings\n        ideal_expertise_vector = torch.mean(expertise_embeddings, dim=0, keepdim=True)\n\n        post_bodies = df_copy['post_body'].fillna('').tolist()\n        body_embeddings = model.encode(post_bodies, convert_to_tensor=True, show_progress_bar=False)\n\n        # Calculate cosine similarity between each post and the ideal expertise vector\n        expertise_scores_tensor = util.cos_sim(body_embeddings, ideal_expertise_vector)\n        df_copy['expertise_score'] = expertise_scores_tensor.cpu().numpy().flatten()\n\n    # --- Benchmarking Logic ---\n    is_ours = df_copy['source'] == 'ours'\n    is_competitor = df_copy['source'] == 'competitors'\n\n    # Fallback 1: Global average expertise of all competitors in the dataset\n    if is_competitor.any():\n        global_competitor_avg = df_copy.loc[is_competitor, 'expertise_score'].mean()\n    # Fallback 2: If no competitors at all, use our own average (ratio will be ~1)\n    elif is_ours.any():\n        global_competitor_avg = df_copy.loc[is_ours, 'expertise_score'].mean()\n    # Fallback 3: If no data at all, use 1.0\n    else:\n        global_competitor_avg = 1.0\n\n    # Primary Benchmark: Average expertise score of competitors per query group\n    competitor_avg_by_query = df_copy[is_competitor].groupby('representative_query')['expertise_score'].mean()\n\n    # Map the per-query average to all posts. This creates the primary benchmark.\n    df_copy['competitor_avg_score'] = df_copy['representative_query'].map(competitor_avg_by_query)\n    # Apply fallback for posts in query groups that have no competitors\n    df_copy['competitor_avg_score'].fillna(global_competitor_avg, inplace=True)\n\n    # Avoid division by zero. A small epsilon is robust.\n    epsilon = 1e-9\n    denominator = df_copy['competitor_avg_score'].replace(0, epsilon)\n\n    # --- Feature Calculation ---\n    # Initialize the feature column with a default value of 1.0\n    df[feature_name] = 1.0\n\n    # Calculate the relative score only for 'ours' posts\n    if is_ours.any():\n        our_scores = df_copy.loc[is_ours, 'expertise_score']\n        our_benchmarks = denominator[is_ours]\n        relative_scores = our_scores / our_benchmarks\n        df.loc[is_ours, feature_name] = relative_scores\n\n    # Final cleanup to ensure data integrity for correlation analysis\n    df[feature_name].replace([np.inf, -np.inf], np.nan, inplace=True)\n    df[feature_name].fillna(1.0, inplace=True)\n\n    return df",
  "analysis": {
    "correlation": 0.03258355785485443,
    "p_value": 0.7539280041708545,
    "interpretation": "약한 양의 상관관계(0.0326)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.7539)."
  }
}