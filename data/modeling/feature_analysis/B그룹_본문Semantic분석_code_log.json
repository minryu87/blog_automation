[
  {
    "timestamp": "2025-07-21T19:40:17.553289",
    "iteration": 1,
    "initial_agent_response": {
      "feature_name": "semantic_similarity_to_success",
      "hypothesis": "Within each topic category ('top_category_name'), there exists an ideal 'semantic centroid' calculated from the content of proven high-performing posts (all competitor posts + our top 30% posts based on 'non_brand_inflow'). A post's semantic similarity to this success centroid is a strong predictor of its own performance. Posts that are more semantically aligned with the success centroid will achieve higher 'non_brand_inflow'.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Safe, lazy-loading of the Sentence Transformer model.\n# This global variable ensures the model is loaded only once.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the ML model, loading it only on the first call.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean and other languages.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of each post to a 'success centroid' \n    defined for its topic category.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with the schema provided in the brief.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with the new 'semantic_similarity_to_success' feature column.\n    \"\"\"\n    # 0. Handle empty DataFrame case to prevent errors.\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        # Add the column with a default type if it doesn't exist, to ensure schema consistency.\n        if 'semantic_similarity_to_success' not in df.columns:\n            df['semantic_similarity_to_success'] = pd.Series(dtype=float)\n        return df\n\n    # Preserve the original index to prevent misalignment issues upon feature assignment.\n    original_index = df.index\n\n    # 1. Get the pre-loaded model.\n    model = get_model()\n\n    # 2. Prepare text for embedding. Combine body and morphemes for richer semantic context.\n    # Fill NaN values to prevent errors during string concatenation.\n    df['semantic_text'] = df['post_body'].fillna('') + ' ' + df['morpheme_words'].fillna('')\n\n    # 3. Efficiently create embeddings for all posts in a single batch.\n    # This is much faster than using .apply() row by row.\n    embeddings = model.encode(\n        df['semantic_text'].tolist(),\n        convert_to_tensor=True,\n        show_progress_bar=False\n    )\n\n    # 4. Calculate the 'Success Centroid' for each topic category.\n    # A centroid is the average embedding of all high-performing posts in a category.\n    category_centroids = {}\n    # The grouping key is 'top_category_name' as 'representative_keyword' is not in the schema.\n    for name, group in df.groupby('top_category_name'):\n        our_posts = group[group['source'] == 'ours']\n        competitor_posts = group[group['source'] == 'competitor']\n\n        # Define the high-performing group for this category.\n        high_perf_indices = competitor_posts.index.tolist()\n\n        # Identify our top-performing posts if they exist.\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            # Calculate the 70th percentile threshold for 'non_brand_inflow'.\n            inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7)\n            # Posts with inflow >= threshold are considered top-performing (Top 30%).\n            our_top_performers = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold]\n            high_perf_indices.extend(our_top_performers.index.tolist())\n        \n        # Remove duplicates just in case.\n        high_perf_indices = list(set(high_perf_indices))\n\n        if not high_perf_indices:\n            # If no high-performers are found in this category, skip it.\n            continue\n\n        # CRITICAL: Convert DataFrame index labels to integer positions for tensor slicing.\n        # This prevents the logical error of mixing up index labels and tensor positions.\n        tensor_positions = df.index.get_indexer(high_perf_indices)\n        high_perf_embeddings = embeddings[tensor_positions]\n\n        # Calculate the centroid (mean vector) for the high-performing group.\n        category_centroids[name] = torch.mean(high_perf_embeddings, dim=0)\n\n    # 5. Calculate the similarity of each post to its category's success centroid.\n    similarities = []\n    for idx, row in df.iterrows():\n        category = row['top_category_name']\n        # Use the integer position corresponding to the DataFrame index.\n        row_position = df.index.get_loc(idx)\n        post_embedding = embeddings[row_position]\n\n        if category in category_centroids:\n            centroid = category_centroids[category]\n            # Calculate cosine similarity and extract the scalar value.\n            sim_score = util.cos_sim(post_embedding, centroid).item()\n            similarities.append(sim_score)\n        else:\n            # If no centroid for the category, append a neutral/null value.\n            similarities.append(np.nan)\n    \n    # 6. Add the new feature to the DataFrame, ensuring the index is aligned.\n    # Using the preserved original index guarantees correct assignment.\n    df['semantic_similarity_to_success'] = pd.Series(similarities, index=df.index)\n\n    # Clean up the temporary column.\n    df = df.drop(columns=['semantic_text'])\n\n    return df\n"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": -0.07101942180175005,
          "p_value": 0.49403052248243484,
          "interpretation": "Found a weak, negative correlation (-0.0710). However, the result is not statistically significant (p-value: 0.4940), so we cannot confidently rule out random chance."
        },
        "non_brand_average_ctr": {
          "correlation": -0.03648633086929399,
          "p_value": 0.72556296412709,
          "interpretation": "Found a weak, negative correlation (-0.0365). However, the result is not statistically significant (p-value: 0.7256), so we cannot confidently rule out random chance."
        }
      },
      "overall_conclusion": "Analysis completed. See interpretations for each target.",
      "hypothesis": "Within each topic category ('top_category_name'), there exists an ideal 'semantic centroid' calculated from the content of proven high-performing posts (all competitor posts + our top 30% posts based on 'non_brand_inflow'). A post's semantic similarity to this success centroid is a strong predictor of its own performance. Posts that are more semantically aligned with the success centroid will achieve higher 'non_brand_inflow'.",
      "feature_created": "semantic_similarity_to_success"
    }
  },
  {
    "timestamp": "2025-07-21T19:54:56.282568",
    "iteration": 1,
    "initial_agent_response": {
      "feature_name": "semantic_contrast_score",
      "hypothesis": "Within each topic category ('top_category_name'), there exist distinct semantic centroids for high-performing content (all competitors + our top 30% posts) and low-performing content (our bottom 30% posts). A post's performance is predicted by its relative semantic position. Posts that are semantically closer to the 'high-performance' centroid and farther from the 'low-performance' centroid will achieve higher engagement. This feature calculates `(similarity_to_high_centroid - similarity_to_low_centroid)`, expecting a positive correlation with performance metrics like 'non_brand_inflow'.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model that is effective for Korean and general purpose.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic contrast score for each post based on its similarity to\n    high-performing and low-performing content centroids within its category.\n    \"\"\"\n    # 2. Check for empty DataFrame to prevent errors.\n    if df.empty:\n        df['semantic_contrast_score'] = pd.Series(dtype=np.float64)\n        return df\n\n    model = get_model()\n\n    # 3. Create a combined text field for a richer semantic representation.\n    # Using title, body, and morphemes as per the analysis goal.\n    df['temp_combined_text'] = df['post_title'].fillna('') + ' ' + \\\n                              df['post_body'].fillna('') + ' ' + \\\n                              df['morpheme_words'].fillna('')\n\n    # 4. Use efficient batch processing for all texts at once.\n    # This is significantly faster than row-by-row .apply().\n    # Preserve the original index to ensure correct mapping later.\n    original_index = df.index\n    texts = df['temp_combined_text'].tolist()\n    embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n\n    # Create a mapping from original DataFrame index to tensor's integer position.\n    # This is a critical step to prevent index misalignment errors.\n    index_to_position = {idx: pos for pos, idx in enumerate(original_index)}\n\n    # 5. Initialize the new feature column with NaN.\n    df['semantic_contrast_score'] = np.nan\n\n    # 6. Process each category group to calculate its unique centroids.\n    for category, group in df.groupby('top_category_name'):\n        if len(group) < 3:  # Not enough data to form meaningful groups\n            continue\n\n        # Define groups based on performance metrics\n        ours_in_group = group[group['source'] == 'ours']\n        competitors_in_group = group[group['source'] == 'competitor']\n\n        # Need at least one of our posts to define low-performers\n        if ours_in_group.empty:\n            continue\n\n        # Define performance quantiles on 'non_brand_inflow'\n        # Use dropna() to handle NaN values gracefully\n        inflow_data = ours_in_group['non_brand_inflow'].dropna()\n        if len(inflow_data) < 2:\n            continue # Not enough data to calculate quantiles\n\n        high_perf_quantile = inflow_data.quantile(0.7)\n        low_perf_quantile = inflow_data.quantile(0.3)\n\n        # Get indices for each group\n        high_perf_ours_indices = ours_in_group[ours_in_group['non_brand_inflow'] >= high_perf_quantile].index\n        low_perf_indices = ours_in_group[ours_in_group['non_brand_inflow'] <= low_perf_quantile].index\n        competitor_indices = competitors_in_group.index\n\n        # Combine competitors and high-performing 'ours' posts for the success group\n        high_perf_total_indices = competitor_indices.union(high_perf_ours_indices)\n        \n        # Ensure groups are not empty before proceeding\n        if high_perf_total_indices.empty or low_perf_indices.empty:\n            continue\n\n        # 7. Safely get embedding vectors using the position map.\n        high_perf_positions = [index_to_position[idx] for idx in high_perf_total_indices]\n        low_perf_positions = [index_to_position[idx] for idx in low_perf_indices]\n        group_positions = [index_to_position[idx] for idx in group.index]\n\n        high_perf_embeddings = embeddings[high_perf_positions]\n        low_perf_embeddings = embeddings[low_perf_positions]\n        group_embeddings = embeddings[group_positions]\n        \n        # 8. Calculate centroids (mean vector) for each group.\n        high_centroid = high_perf_embeddings.mean(axis=0, keepdim=True)\n        low_centroid = low_perf_embeddings.mean(axis=0, keepdim=True)\n\n        # 9. Calculate similarity of all posts in the group to each centroid.\n        sim_to_high = util.cos_sim(group_embeddings, high_centroid)\n        sim_to_low = util.cos_sim(group_embeddings, low_centroid)\n\n        # The contrast score is the difference in similarities.\n        # A higher score means closer to 'high-perf' and farther from 'low-perf'.\n        contrast_scores = (sim_to_high - sim_to_low).flatten().cpu().numpy()\n\n        # 10. Assign the calculated scores back to the original DataFrame using .loc.\n        df.loc[group.index, 'semantic_contrast_score'] = contrast_scores\n\n    # 11. Clean up the temporary column and return the full DataFrame.\n    df = df.drop(columns=['temp_combined_text'])\n    return df\n"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.23987680014396542,
          "p_value": 0.019212942115811902,
          "interpretation": "약한 양의 상관관계(0.2399)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0192), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.10329578473677403,
          "p_value": 0.31918104208496456,
          "interpretation": "약한 양의 상관관계(0.1033)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.3192), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic category ('top_category_name'), there exist distinct semantic centroids for high-performing content (all competitors + our top 30% posts) and low-performing content (our bottom 30% posts). A post's performance is predicted by its relative semantic position. Posts that are semantically closer to the 'high-performance' centroid and farther from the 'low-performance' centroid will achieve higher engagement. This feature calculates `(similarity_to_high_centroid - similarity_to_low_centroid)`, expecting a positive correlation with performance metrics like 'non_brand_inflow'.",
      "feature_created": "semantic_contrast_score"
    }
  },
  {
    "timestamp": "2025-07-21T19:55:53.762066",
    "iteration": 2,
    "initial_agent_response": {
      "feature_name": "semantic_gap_coverage_score",
      "hypothesis": "Within each topic category ('top_category_name'), a specific set of 'opportunity keywords' can be identified. These keywords are highly prevalent in the content of top-performing posts (all competitors + our top 30%) but are conspicuously absent or underutilized in low-performing posts (our bottom 30%). A post's performance is therefore positively correlated with its coverage of these 'opportunity keywords'. A higher score, indicating the post successfully incorporates these differentiating topics, should predict higher performance metrics like 'non_brand_inflow'.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport ast\n\ndef _safe_parse_list(s):\n    \"\"\"Safely parses a string that should be a list of strings.\"\"\"\n    try:\n        parsed = ast.literal_eval(s)\n        if isinstance(parsed, list):\n            return [str(item) for item in parsed]\n    except (ValueError, SyntaxError, TypeError):\n        # Fallback for simple comma-separated strings or malformed data\n        if isinstance(s, str):\n            return [item.strip() for item in s.split(',') if item.strip()]\n    return []\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score based on the coverage of 'opportunity keywords'.\n    These keywords are common in high-performing posts but absent in low-performing ones within the same category.\n    \"\"\"\n    if df.empty:\n        return df\n\n    # Initialize the feature column\n    feature_name = 'semantic_gap_coverage_score'\n    df[feature_name] = 0.0\n\n    # 1. Prepare keyword data by combining and safely parsing keyword columns\n    df['temp_keywords'] = df['category_keywords'].fillna('') + ',' + df['morpheme_words'].fillna('')\n    df['keyword_list'] = df['temp_keywords'].apply(_safe_parse_list)\n\n    # 2. Group by category to perform analysis within a relevant context\n    grouped = df.groupby('top_category_name')\n\n    for name, group_df in grouped:\n        # Preserve the original index for accurate assignment later\n        group_indices = group_df.index\n\n        our_posts = group_df[group_df['source'] == 'ours']\n        competitor_posts = group_df[group_df['source'] == 'competitor']\n\n        # Need our posts to define performance tiers\n        if our_posts.empty or our_posts['non_brand_inflow'].dropna().empty:\n            continue\n\n        # 3. Define High-performing and Low-performing groups based on the brief\n        inflow_threshold_low = our_posts['non_brand_inflow'].quantile(0.3, interpolation='lower')\n        inflow_threshold_high = our_posts['non_brand_inflow'].quantile(0.7, interpolation='higher')\n\n        low_perf_ours = our_posts[our_posts['non_brand_inflow'] <= inflow_threshold_low]\n        high_perf_ours = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold_high]\n        \n        high_perf_group = pd.concat([high_perf_ours, competitor_posts])\n\n        if high_perf_group.empty or low_perf_ours.empty:\n            continue\n\n        # 4. Extract vocabularies for each performance group\n        high_perf_words = [word for sublist in high_perf_group['keyword_list'] for word in sublist]\n        low_perf_words = [word for sublist in low_perf_ours['keyword_list'] for word in sublist]\n\n        if not high_perf_words or not low_perf_words:\n            continue\n\n        # 5. Identify 'Gap Keywords': frequent in high-perf, but not frequent in low-perf\n        high_counts = Counter(high_perf_words)\n        low_counts = Counter(low_perf_words)\n\n        # Using asymmetric sets: find what's common in high-performers that isn't common in low-performers\n        top_n_high = 50\n        top_n_low = 100 \n        top_high_set = set(word for word, count in high_counts.most_common(top_n_high))\n        top_low_set = set(word for word, count in low_counts.most_common(top_n_low))\n\n        gap_keywords = top_high_set - top_low_set\n\n        if not gap_keywords:\n            continue\n\n        # 6. Calculate coverage score for each post in the group\n        scores = {}\n        for idx, row in group_df.iterrows():\n            post_word_set = set(row['keyword_list'])\n            covered_keywords = post_word_set.intersection(gap_keywords)\n            # Score is the percentage of gap keywords covered by the post\n            score = len(covered_keywords) / len(gap_keywords)\n            scores[idx] = score\n        \n        # 7. Assign scores back to the original DataFrame using preserved indices\n        df.loc[list(scores.keys()), feature_name] = pd.Series(scores)\n\n    # Clean up temporary column and fill any NaNs that might have occurred\n    df.drop(columns=['temp_keywords', 'keyword_list'], inplace=True)\n    df[feature_name].fillna(0.0, inplace=True)\n\n    return df"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": null,
          "p_value": null,
          "interpretation": "상관관계를 계산할 수 없습니다. 이는 보통 생성된 피처의 분산이 0일 때 (모든 값이 동일할 때) 발생합니다."
        },
        "non_brand_average_ctr": {
          "correlation": null,
          "p_value": null,
          "interpretation": "상관관계를 계산할 수 없습니다. 이는 보통 생성된 피처의 분산이 0일 때 (모든 값이 동일할 때) 발생합니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic category ('top_category_name'), a specific set of 'opportunity keywords' can be identified. These keywords are highly prevalent in the content of top-performing posts (all competitors + our top 30%) but are conspicuously absent or underutilized in low-performing posts (our bottom 30%). A post's performance is therefore positively correlated with its coverage of these 'opportunity keywords'. A higher score, indicating the post successfully incorporates these differentiating topics, should predict higher performance metrics like 'non_brand_inflow'.",
      "feature_created": "semantic_gap_coverage_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:01:32.877135",
    "iteration": 1,
    "initial_agent_response": {
      "feature_name": "semantic_outlier_score",
      "hypothesis": "For any given topic ('top_category_name'), high-performing posts (all competitors + our top 30%) form a 'success cluster' in semantic space. Low-performing posts are often semantic outliers relative to this cluster. This feature quantifies how much of an outlier a post is by calculating its semantic distance from the success cluster's centroid, normalized by the cluster's own internal tightness (average pairwise distance). A lower score indicates the post is semantically aligned with the successful consensus, and we predict this will negatively correlate with performance (i.e., a lower outlier score leads to higher 'non_brand_inflow').",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model as the content language is not specified.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic outlier score for each post within its topic category.\n\n    The score is the distance from the 'high-performers' centroid, normalized by the\n    high-performers' own semantic cluster tightness.\n    \"\"\"\n    feature_name = 'semantic_outlier_score'\n\n    if df.empty:\n        df[feature_name] = pd.Series(dtype=np.float64)\n        return df\n\n    # Make a copy to avoid SettingWithCopyWarning and preserve the original df\n    df_copy = df.copy()\n    df_copy[feature_name] = np.nan\n\n    # Ensure text columns are string type and handle potential NaN values\n    df_copy['post_body'] = df_copy['post_body'].fillna('').astype(str)\n    df_copy['top_category_name'] = df_copy['top_category_name'].fillna('Uncategorized').astype(str)\n    \n    # Pre-compute all embeddings in one batch for efficiency\n    model = get_model()\n    all_embeddings = model.encode(\n        df_copy['post_body'].tolist(), \n        convert_to_tensor=True, \n        show_progress_bar=False\n    )\n\n    # Define performance quantiles for 'ours' posts\n    our_posts = df_copy[df_copy['source'] == 'ours']\n    if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n        inflow_q_low = our_posts['non_brand_inflow'].quantile(0.3)\n        inflow_q_high = our_posts['non_brand_inflow'].quantile(0.7)\n    else:\n        # If no 'ours' posts with performance data, we can't define our high/low performers\n        inflow_q_low = -1\n        inflow_q_high = -1\n\n    # Process each topic category group\n    for category, group in df_copy.groupby('top_category_name'):\n        # Define the High-Performing (HP) group for this category\n        is_competitor = group['source'] == 'competitor'\n        is_our_high_performer = (group['source'] == 'ours') & (group['non_brand_inflow'] >= inflow_q_high)\n        hp_mask = is_competitor | is_our_high_performer\n        \n        hp_indices = group[hp_mask].index\n        \n        # Need at least 2 HP posts to measure dispersion\n        if len(hp_indices) < 2:\n            continue\n\n        # Get integer positions for tensor slicing, preserving original index integrity\n        hp_positions = [df_copy.index.get_loc(i) for i in hp_indices]\n        hp_embeddings = all_embeddings[hp_positions]\n\n        # --- Calculate HP Centroid and Dispersion ---\n        hp_centroid = hp_embeddings.mean(axis=0)\n\n        # Calculate pairwise cosine similarity within the HP cluster\n        hp_sim_matrix = util.cos_sim(hp_embeddings, hp_embeddings)\n        \n        # Convert similarity to distance (1 - sim)\n        hp_dist_matrix = 1 - hp_sim_matrix\n        \n        # Extract unique pairwise distances (upper triangle) to calculate average dispersion\n        # This measures the 'tightness' of the HP cluster\n        upper_triangle_indices = np.triu_indices_from(hp_dist_matrix, k=1)\n        hp_dispersion = hp_dist_matrix[upper_triangle_indices].mean().item()\n\n        # Avoid division by zero if all HP posts are identical\n        if hp_dispersion == 0:\n            hp_dispersion = 1e-9 # Use a small epsilon\n\n        # --- Calculate Score for all posts in the current category ---\n        group_indices = group.index\n        group_positions = [df_copy.index.get_loc(i) for i in group_indices]\n        group_embeddings = all_embeddings[group_positions]\n        \n        # Calculate cosine similarity of each post to the HP centroid\n        sim_to_centroid = util.cos_sim(group_embeddings, hp_centroid.unsqueeze(0))\n        \n        # Convert similarity to distance\n        dist_to_centroid = 1 - sim_to_centroid.flatten().cpu().numpy()\n\n        # The final score is the normalized distance\n        outlier_scores = dist_to_centroid / hp_dispersion\n\n        # Assign scores back to the corresponding rows in the copied dataframe\n        df_copy.loc[group_indices, feature_name] = outlier_scores\n\n    return df_copy"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.07101941795670873,
          "p_value": 0.49403054592452045,
          "interpretation": "약한 양의 상관관계(0.0710)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.4940), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.03648638530424874,
          "p_value": 0.7255625709679984,
          "interpretation": "약한 양의 상관관계(0.0365)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.7256), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "For any given topic ('top_category_name'), high-performing posts (all competitors + our top 30%) form a 'success cluster' in semantic space. Low-performing posts are often semantic outliers relative to this cluster. This feature quantifies how much of an outlier a post is by calculating its semantic distance from the success cluster's centroid, normalized by the cluster's own internal tightness (average pairwise distance). A lower score indicates the post is semantically aligned with the successful consensus, and we predict this will negatively correlate with performance (i.e., a lower outlier score leads to higher 'non_brand_inflow').",
      "feature_created": "semantic_outlier_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:02:54.788074",
    "iteration": 2,
    "initial_agent_response": {
      "feature_name": "relative_semantic_proximity_score",
      "hypothesis": "A post's performance is not just about its proximity to the *average* of successful content, but its relationship to the *nearest* examples of success and failure. This feature measures a post's semantic similarity to its single closest high-performing peer and its single closest low-performing peer within its topic category (`top_category_name`). The final score is calculated as `(similarity_to_nearest_high_performer) - (similarity_to_nearest_low_performer)`. A high positive score indicates the post is very similar to a specific successful example while being distinct from any particular failure, predicting better performance metrics like `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for various content types.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the relative semantic proximity score for each post.\n\n    This score is the difference between the similarity to the nearest high-performing post\n    and the similarity to the nearest low-performing post within the same topic category.\n    \"\"\"\n    # 2. Check for empty DataFrame and required columns.\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        return df\n\n    feature_name = 'relative_semantic_proximity_score'\n    df[feature_name] = np.nan  # Initialize the feature column.\n\n    required_cols = ['source', 'top_category_name', 'non_brand_inflow', 'post_body']\n    if not all(col in df.columns for col in required_cols):\n        print(f\"[Error] DataFrame is missing one or more required columns: {required_cols}\")\n        return df\n\n    model = get_model()\n\n    # 3. Prepare data for embedding, ensuring original index is preserved.\n    # Using a copy to prevent SettingWithCopyWarning.\n    df_copy = df.copy()\n    df_copy['text_to_embed'] = df_copy['post_body'].fillna('').astype(str)\n    \n    # Map original DataFrame index to integer position for tensor slicing.\n    original_indices = df_copy.index\n    index_to_pos = {idx: i for i, idx in enumerate(original_indices)}\n\n    # 4. Encode all texts in a single, efficient batch.\n    embeddings = model.encode(\n        df_copy['text_to_embed'].tolist(),\n        convert_to_tensor=True,\n        show_progress_bar=False\n    )\n\n    # 5. Process each topic category separately.\n    for category, group_df in df_copy.groupby('top_category_name'):\n        \n        # Define performance groups for the current category.\n        ours_df = group_df[group_df['source'] == 'ours']\n        \n        # Skip category if there's not enough data to define performance tiers.\n        inflow_values = ours_df['non_brand_inflow'].dropna()\n        if len(inflow_values) < 4: # Need a few data points to make quantiles meaningful.\n            continue\n\n        # Define performance thresholds using quantiles.\n        q_low = inflow_values.quantile(0.3)\n        q_high = inflow_values.quantile(0.7)\n\n        # Handle cases where all values are the same, making quantiles equal.\n        if q_low == q_high:\n            continue\n\n        # Get indices for high and low performers.\n        high_perf_our_indices = ours_df[ours_df['non_brand_inflow'] >= q_high].index\n        competitor_indices = group_df[group_df['source'] == 'competitor'].index\n        high_perf_indices = high_perf_our_indices.union(competitor_indices)\n        \n        low_perf_indices = ours_df[ours_df['non_brand_inflow'] <= q_low].index\n\n        # Get integer positions from the original DataFrame for tensor slicing.\n        high_perf_pos = [index_to_pos[i] for i in high_perf_indices if i in index_to_pos]\n        low_perf_pos = [index_to_pos[i] for i in low_perf_indices if i in index_to_pos]\n\n        # Skip this category if either performance group is empty.\n        if not high_perf_pos or not low_perf_pos:\n            continue\n            \n        # Get the embedding tensors for the performance groups.\n        high_perf_embeddings = embeddings[high_perf_pos]\n        low_perf_embeddings = embeddings[low_perf_pos]\n\n        # Iterate through each post in the current category to calculate its score.\n        for idx, row in group_df.iterrows():\n            post_pos = index_to_pos.get(idx)\n            if post_pos is None:\n                continue\n            \n            post_embedding = embeddings[post_pos].unsqueeze(0) # Keep 2D for cos_sim.\n\n            # --- Calculate similarity to nearest high-performer ---\n            sim_to_high = util.cos_sim(post_embedding, high_perf_embeddings)[0]\n            # Exclude self-similarity if the post is in the high-perf group.\n            if idx in high_perf_indices:\n                try:\n                    self_idx_in_high = high_perf_indices.get_loc(idx)\n                    sim_to_high[self_idx_in_high] = -1.0 # Set to a value that won't be the max.\n                except KeyError:\n                    pass # Should not happen, but safe to have.\n            max_sim_high = sim_to_high.max().item() if len(sim_to_high) > 0 else 0.0\n\n            # --- Calculate similarity to nearest low-performer ---\n            sim_to_low = util.cos_sim(post_embedding, low_perf_embeddings)[0]\n            # Exclude self-similarity if the post is in the low-perf group.\n            if idx in low_perf_indices:\n                try:\n                    self_idx_in_low = low_perf_indices.get_loc(idx)\n                    sim_to_low[self_idx_in_low] = -1.0\n                except KeyError:\n                    pass\n            max_sim_low = sim_to_low.max().item() if len(sim_to_low) > 0 else 0.0\n\n            # Calculate final feature score and assign it to the original DataFrame.\n            score = max_sim_high - max_sim_low\n            df.loc[idx, feature_name] = score\n\n    return df\n"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.08648991331797197,
          "p_value": 0.4046175875294581,
          "interpretation": "약한 양의 상관관계(0.0865)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.4046), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.0333314150546968,
          "p_value": 0.7484659422852351,
          "interpretation": "약한 양의 상관관계(0.0333)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.7485), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "A post's performance is not just about its proximity to the *average* of successful content, but its relationship to the *nearest* examples of success and failure. This feature measures a post's semantic similarity to its single closest high-performing peer and its single closest low-performing peer within its topic category (`top_category_name`). The final score is calculated as `(similarity_to_nearest_high_performer) - (similarity_to_nearest_low_performer)`. A high positive score indicates the post is very similar to a specific successful example while being distinct from any particular failure, predicting better performance metrics like `non_brand_inflow`.",
      "feature_created": "relative_semantic_proximity_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:06:26.438968",
    "iteration": 1,
    "initial_agent_response": {
      "feature_name": "semantic_theme_coverage_score",
      "hypothesis": "Within each topic ('top_category_name'), high-performing posts (all competitors + our top 30%) do not just align with a single average 'success point', but rather cover a diverse range of 'success themes'—the primary axes of semantic variation within that successful group. Conversely, low-performing posts (our bottom 30%) align with distinct 'failure themes'. This feature uses Principal Component Analysis (PCA) to identify these semantic themes for both groups. It then scores each post based on how strongly its content projects onto the success themes, while penalizing it for projecting onto the failure themes. A high score, indicating broad alignment with successful content diversity and avoidance of common failure patterns, is predicted to positively correlate with performance metrics like 'non_brand_inflow'.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.decomposition import PCA\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model that is effective for Korean and general purpose\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic theme coverage score for each post.\n\n    This score measures how well a post's content covers the primary semantic themes\n    of high-performing content while avoiding the themes of low-performing content\n    within its topic category.\n    \"\"\"\n    # 0. Handle empty DataFrame\n    if df.empty:\n        return df\n\n    # Ensure the target column exists and initialize with a default value (NaN)\n    df['semantic_theme_coverage_score'] = np.nan\n\n    model = get_model()\n\n    # 1. Prepare Text Data for Embedding\n    # Combine title, body, and morphemes for a richer semantic representation.\n    # Fill NaNs to prevent errors during concatenation.\n    df['combined_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).astype(str)\n\n    # 2. Efficient Batch Encoding\n    # Create embeddings for all posts at once for maximum efficiency.\n    # Keep as a tensor for GPU acceleration if available.\n    all_embeddings = model.encode(df['combined_text'].tolist(), convert_to_tensor=True, show_progress_bar=False)\n\n    # Create a mapping from the DataFrame's index to the embedding's positional index.\n    # This is crucial for correctly linking group data to embeddings later.\n    index_to_pos = {idx: i for i, idx in enumerate(df.index)}\n\n    # The number of principal components to extract as 'themes'.\n    N_COMPONENTS = 2\n\n    # 3. Group by category and analyze themes\n    for category, group_df in df.groupby('top_category_name'):\n        ours_df = group_df[group_df['source'] == 'ours']\n        competitor_indices = group_df[group_df['source'] == 'competitor'].index.tolist()\n\n        # Define High and Low performing groups for 'ours' posts\n        if not ours_df.empty and ours_df['non_brand_inflow'].notna().any():\n            # Use quantiles to define performance tiers\n            low_quantile = ours_df['non_brand_inflow'].quantile(0.3)\n            high_quantile = ours_df['non_brand_inflow'].quantile(0.7)\n\n            low_perf_ours_indices = ours_df[ours_df['non_brand_inflow'] <= low_quantile].index.tolist()\n            high_perf_ours_indices = ours_df[ours_df['non_brand_inflow'] >= high_quantile].index.tolist()\n        else:\n            # If no 'ours' posts or no performance data, these are empty\n            low_perf_ours_indices = []\n            high_perf_ours_indices = []\n\n        # Combine groups: High-performers = all competitors + top 'ours' posts\n        high_perf_indices = competitor_indices + high_perf_ours_indices\n        low_perf_indices = low_perf_ours_indices # Low-performers are only from 'ours'\n\n        # 4. Perform PCA only if there's enough data in both groups\n        # PCA requires n_samples >= n_components.\n        if len(high_perf_indices) < N_COMPONENTS or len(low_perf_indices) < N_COMPONENTS:\n            continue # Skip this category if there's not enough data to model themes\n\n        # Get embeddings for each group using the positional mapping\n        hp_positions = [index_to_pos[i] for i in high_perf_indices]\n        lp_positions = [index_to_pos[i] for i in low_perf_indices]\n        hp_embeddings = all_embeddings[hp_positions].cpu().numpy()\n        lp_embeddings = all_embeddings[lp_positions].cpu().numpy()\n\n        # Fit PCA to find the primary 'themes' (principal components)\n        hp_pca = PCA(n_components=N_COMPONENTS).fit(hp_embeddings)\n        lp_pca = PCA(n_components=N_COMPONENTS).fit(lp_embeddings)\n\n        # 5. Calculate score for every post in the current category group\n        for idx in group_df.index:\n            pos = index_to_pos[idx]\n            post_embedding = all_embeddings[pos].cpu().numpy().reshape(1, -1)\n\n            # Project the post's embedding onto the high/low-performance themes\n            hp_projection = hp_pca.transform(post_embedding)\n            lp_projection = lp_pca.transform(post_embedding)\n\n            # The score is the magnitude (L2 norm) of the projection.\n            # A larger norm means the post aligns more strongly with the identified themes.\n            hp_score = np.linalg.norm(hp_projection)\n            lp_score = np.linalg.norm(lp_projection)\n\n            # The final score is the alignment with success themes minus alignment with failure themes.\n            final_score = hp_score - lp_score\n            df.loc[idx, 'semantic_theme_coverage_score'] = final_score\n\n    # Clean up the temporary column\n    df.drop(columns=['combined_text'], inplace=True)\n\n    return df\n"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.2010886539072582,
          "p_value": 0.05069735727481306,
          "interpretation": "약한 양의 상관관계(0.2011)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.0507), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.11489818561246616,
          "p_value": 0.26754226685344257,
          "interpretation": "약한 양의 상관관계(0.1149)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.2675), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic ('top_category_name'), high-performing posts (all competitors + our top 30%) do not just align with a single average 'success point', but rather cover a diverse range of 'success themes'—the primary axes of semantic variation within that successful group. Conversely, low-performing posts (our bottom 30%) align with distinct 'failure themes'. This feature uses Principal Component Analysis (PCA) to identify these semantic themes for both groups. It then scores each post based on how strongly its content projects onto the success themes, while penalizing it for projecting onto the failure themes. A high score, indicating broad alignment with successful content diversity and avoidance of common failure patterns, is predicted to positively correlate with performance metrics like 'non_brand_inflow'.",
      "feature_created": "semantic_theme_coverage_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:07:47.028829",
    "iteration": 2,
    "initial_agent_response": {
      "feature_name": "optimal_topic_focus_score",
      "hypothesis": "Within a given topic category (`top_category_name`), high-performing content (competitors + our top 30%) consistently covers a specific set of core 'sub-themes'. These sub-themes, identified via clustering on content chunks, represent the ideal thematic distribution for success. A post's performance is predicted by how well its own content chunks align with these identified success themes. A higher score, indicating the post's content is thematically similar to the established patterns of high-performing articles, should positively correlate with performance metrics like `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a model that is good for semantic search and clustering\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef split_text_into_chunks(text, chunk_size=100):\n    \"\"\"Splits a single string of text into chunks of a specified number of words.\"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return []\n    words = text.split()\n    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the Optimal Topic Focus Score for each post.\n\n    This score measures how well a post's content aligns with the key 'sub-themes'\n    derived from high-performing content within the same category.\n    \"\"\"\n    # 0. Initial Setup & DataFrame Checks\n    if df.empty:\n        return df\n\n    model = get_model()\n    feature_name = 'optimal_topic_focus_score'\n    df[feature_name] = 0.0\n\n    # Preserve original index for safe lookups later\n    df_reset = df.reset_index()\n\n    # 1. Efficiently prepare and embed all content chunks in one go\n    all_chunks = []\n    doc_to_chunk_map = [[] for _ in range(len(df))]\n    \n    # Combine text sources for a richer representation\n    texts_to_process = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).tolist()\n\n    for i, text in enumerate(texts_to_process):\n        post_chunks = split_text_into_chunks(text, chunk_size=100)\n        if post_chunks:\n            start_index = len(all_chunks)\n            all_chunks.extend(post_chunks)\n            end_index = len(all_chunks)\n            doc_to_chunk_map[i] = list(range(start_index, end_index))\n\n    if not all_chunks:\n        return df\n\n    # Encode all chunks in a single, efficient batch operation\n    chunk_embeddings = model.encode(\n        all_chunks, \n        convert_to_tensor=True, \n        show_progress_bar=False, \n        batch_size=128\n    )\n\n    # 2. Process each category to find success themes and score posts\n    for category, group_df in df.groupby('top_category_name'):\n        # Define high-performing group for the category\n        our_posts = group_df[group_df['source'] == 'ours']\n        competitor_posts = group_df[group_df['source'] == 'competitor']\n\n        high_perf_our_indices = pd.Index([])\n        if not our_posts.empty and our_posts['non_brand_inflow'].notna().any():\n            inflow_threshold_top = our_posts['non_brand_inflow'].quantile(0.7, interpolation='higher')\n            high_perf_our = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold_top]\n            if not high_perf_our.empty:\n                 high_perf_our_indices = high_perf_our.index\n\n        high_perf_indices = competitor_posts.index.union(high_perf_our_indices)\n\n        if high_perf_indices.empty:\n            continue\n\n        # Gather embeddings for all chunks from the high-performing group\n        high_perf_chunk_global_indices = []\n        for idx in high_perf_indices:\n            # Use the preserved integer position from the reset index\n            row_pos = df.index.get_loc(idx)\n            high_perf_chunk_global_indices.extend(doc_to_chunk_map[row_pos])\n\n        if not high_perf_chunk_global_indices:\n            continue\n\n        high_perf_embeddings = chunk_embeddings[high_perf_chunk_global_indices]\n\n        # 3. Cluster high-performer chunks to find 'Success Themes'\n        num_samples = high_perf_embeddings.shape[0]\n        # Dynamically determine a reasonable number of clusters\n        k = min(5, max(2, num_samples // 10))\n        if num_samples <= k:\n            continue # Not enough data to cluster\n\n        try:\n            kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n            kmeans.fit(high_perf_embeddings.cpu().numpy())\n            success_theme_centroids = torch.tensor(kmeans.cluster_centers_, device=model.device)\n        except ValueError:\n            continue # KMeans failed, skip this group\n\n        # 4. Score each post in the category based on its alignment with success themes\n        for idx in group_df.index:\n            row_pos = df.index.get_loc(idx)\n            post_chunk_global_indices = doc_to_chunk_map[row_pos]\n\n            if not post_chunk_global_indices:\n                # Post has no content chunks, score is 0\n                df.loc[idx, feature_name] = 0.0\n                continue\n\n            post_chunk_embeddings = chunk_embeddings[post_chunk_global_indices]\n            \n            # Calculate similarity of each chunk to the nearest success theme\n            sim_matrix = util.cos_sim(post_chunk_embeddings, success_theme_centroids)\n            max_sim_per_chunk, _ = torch.max(sim_matrix, dim=1)\n            \n            # The final score is the average of these maximum similarities\n            post_score = max_sim_per_chunk.mean().item()\n            df.loc[idx, feature_name] = post_score\n            \n    return df"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": -0.01325226362221591,
          "p_value": 0.898574175348341,
          "interpretation": "약한 음의 상관관계(-0.0133)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.8986), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.04435220829515002,
          "p_value": 0.6695401673094615,
          "interpretation": "약한 양의 상관관계(0.0444)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.6695), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within a given topic category (`top_category_name`), high-performing content (competitors + our top 30%) consistently covers a specific set of core 'sub-themes'. These sub-themes, identified via clustering on content chunks, represent the ideal thematic distribution for success. A post's performance is predicted by how well its own content chunks align with these identified success themes. A higher score, indicating the post's content is thematically similar to the established patterns of high-performing articles, should positively correlate with performance metrics like `non_brand_inflow`.",
      "feature_created": "optimal_topic_focus_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:08:42.487215",
    "iteration": 3,
    "initial_agent_response": {
      "feature_name": "semantic_archetype_alignment_score",
      "hypothesis": "Within each topic category ('top_category_name'), high-performing content (competitors + our top 30%) is not monolithic but often conforms to one of several distinct 'success archetypes' (e.g., in-depth review, quick guide, user story). These archetypes can be identified by clustering the content of high-performing posts in semantic space. A post's performance is predicted by its ability to strongly align with the *closest* of these success archetypes, rather than its proximity to the average of all successful posts. This feature calculates a post's maximum semantic similarity to any of the archetype centroids. A higher score, indicating a strong alignment with a proven success pattern, is expected to positively correlate with performance metrics like 'non_brand_inflow'.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a model that is good for semantic search and clustering.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic alignment of each post with high-performance archetypes within its category.\n    \"\"\"\n    feature_name = 'semantic_archetype_alignment_score'\n\n    if df.empty:\n        df[feature_name] = pd.Series(dtype=np.float64)\n        return df\n\n    # Ensure the feature column exists with a default value.\n    df[feature_name] = np.nan\n\n    model = get_model()\n\n    # Combine text sources for a comprehensive semantic representation.\n    # Fill NaN values to prevent errors during concatenation.\n    text_to_encode = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).tolist()\n\n    # Generate embeddings for all posts in a single, efficient batch.\n    embeddings = model.encode(text_to_encode, convert_to_tensor=True, show_progress_bar=False)\n\n    # Process each category independently.\n    for category in df['top_category_name'].dropna().unique():\n        category_mask = df['top_category_name'] == category\n        df_cat = df[category_mask]\n\n        # Define groups within the category\n        our_posts_mask = df_cat['source'] == 'ours'\n        competitor_posts_mask = df_cat['source'] == 'competitor'\n        df_our_cat = df_cat[our_posts_mask]\n\n        # Identify the high-performing group\n        high_perf_indices = df_cat[competitor_posts_mask].index.tolist()\n        if not df_our_cat.empty and df_our_cat['non_brand_inflow'].notna().any():\n            inflow_quantile_70 = df_our_cat['non_brand_inflow'].quantile(0.7, interpolation='higher')\n            # Avoid adding posts with zero inflow to the high-performance group unless it's the only option\n            if inflow_quantile_70 > 0:\n                high_perf_our_indices = df_our_cat[df_our_cat['non_brand_inflow'] >= inflow_quantile_70].index.tolist()\n                high_perf_indices.extend(high_perf_our_indices)\n        \n        high_perf_indices = list(set(high_perf_indices)) # Remove duplicates\n\n        if not high_perf_indices:\n            continue # Skip category if no high-performers are found.\n\n        # Get integer positions for tensor slicing from DataFrame indices.\n        try:\n            high_perf_locs = df.index.get_indexer_for(high_perf_indices)\n        except KeyError:\n            # This can happen if indices somehow don't align, skip the category.\n            continue\n        \n        high_perf_embeddings = embeddings[high_perf_locs]\n        \n        # Determine the number of archetypes (clusters) adaptively.\n        # We need at least 3 samples to find 2 clusters.\n        if high_perf_embeddings.shape[0] < 3:\n            # If too few samples, the only archetype is the mean.\n            archetype_centroids = high_perf_embeddings.mean(axis=0, keepdim=True)\n        else:\n            # Adapt k: from 2 to 5 clusters, with one cluster per 5-10 posts.\n            n_clusters = max(2, min(5, high_perf_embeddings.shape[0] // 5))\n            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n            kmeans.fit(high_perf_embeddings.cpu().numpy())\n            archetype_centroids = torch.tensor(kmeans.cluster_centers_, device=embeddings.device)\n\n        # Calculate similarity of all posts in the category to the identified archetypes.\n        category_locs = df.index.get_indexer_for(df_cat.index)\n        category_embeddings = embeddings[category_locs]\n        \n        sim_matrix = util.cos_sim(category_embeddings, archetype_centroids)\n        \n        # The score is the maximum similarity to any archetype.\n        scores = sim_matrix.max(axis=1).values.cpu().numpy()\n        \n        # Assign scores back to the main DataFrame.\n        df.loc[df_cat.index, feature_name] = scores\n\n    # Fill any categories that couldn't be processed with a neutral value.\n    df[feature_name] = df[feature_name].fillna(0.0)\n\n    return df\n"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": -0.030993569517502897,
          "p_value": 0.7655806894023406,
          "interpretation": "약한 음의 상관관계(-0.0310)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.7656), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        },
        "non_brand_average_ctr": {
          "correlation": -0.05652974724475645,
          "p_value": 0.5863553397723282,
          "interpretation": "약한 음의 상관관계(-0.0565)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.5864), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic category ('top_category_name'), high-performing content (competitors + our top 30%) is not monolithic but often conforms to one of several distinct 'success archetypes' (e.g., in-depth review, quick guide, user story). These archetypes can be identified by clustering the content of high-performing posts in semantic space. A post's performance is predicted by its ability to strongly align with the *closest* of these success archetypes, rather than its proximity to the average of all successful posts. This feature calculates a post's maximum semantic similarity to any of the archetype centroids. A higher score, indicating a strong alignment with a proven success pattern, is expected to positively correlate with performance metrics like 'non_brand_inflow'.",
      "feature_created": "semantic_archetype_alignment_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:10:31.831161",
    "iteration": 4,
    "initial_agent_response": {
      "feature_name": "archetypal_contrast_score",
      "hypothesis": "Within each topic category ('top_category_name'), high-performing content (competitors + our top 30%) clusters into several distinct 'Success Archetypes' (e.g., 'detailed guide', 'comparative review'), while low-performing content (our bottom 30%) clusters into 'Failure Archetypes'. A post's success is predicted by its ability to strongly align with a winning pattern while simultaneously distancing itself from common failure patterns. This feature calculates the difference between a post's maximum similarity to any Success Archetype and its maximum similarity to any Failure Archetype. A higher score is expected to positively correlate with performance (`non_brand_inflow`).",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\nimport warnings\n\n# Suppress KMeans warning about memory leaks on Windows with MKL, which can be noisy.\nwarnings.filterwarnings('ignore', category=UserWarning, module='sklearn.cluster._kmeans')\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a multilingual model as it's a robust choice for various languages.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the Archetypal Contrast Score for each post.\n\n    This score measures how well a post aligns with successful content archetypes\n    while avoiding failure archetypes within its topic category.\n    \"\"\"\n    feature_name = 'archetypal_contrast_score'\n    \n    if df.empty:\n        df[feature_name] = pd.Series(dtype=np.float64)\n        return df\n\n    model = get_model()\n\n    # Work on a copy to avoid SettingWithCopyWarning\n    df_copy = df.copy()\n    df_copy['combined_text'] = (\n        df_copy['post_title'].fillna('') + ' ' +\n        df_copy['post_body'].fillna('') + ' ' +\n        df_copy['morpheme_words'].fillna('')\n    ).str.strip()\n\n    # Initialize the feature column in the original DataFrame\n    df[feature_name] = np.nan\n\n    # 2. Use efficient batch processing for all posts at once.\n    texts_to_encode = df_copy['combined_text'].tolist()\n    all_embeddings = model.encode(texts_to_encode, convert_to_tensor=True, show_progress_bar=False)\n\n    # Group by the main topic category to find archetypes within each topic.\n    grouped = df_copy.groupby('top_category_name')\n\n    for _, group in grouped:\n        # Define high and low-performing groups within the category\n        ours_posts = group[group['source'] == 'ours']\n        competitor_posts = group[group['source'] == 'competitor']\n\n        # Determine performance thresholds only from 'ours' posts that have the metric\n        low_perf_ours = pd.DataFrame(columns=df_copy.columns)\n        high_perf_ours = pd.DataFrame(columns=df_copy.columns)\n        if not ours_posts.empty and ours_posts['non_brand_inflow'].notna().any():\n            inflow_data = ours_posts['non_brand_inflow'].dropna()\n            if len(inflow_data) > 0:\n                low_quantile = inflow_data.quantile(0.3)\n                high_quantile = inflow_data.quantile(0.7)\n                low_perf_ours = ours_posts[ours_posts['non_brand_inflow'] <= low_quantile]\n                high_perf_ours = ours_posts[ours_posts['non_brand_inflow'] >= high_quantile]\n\n        # High-performing group: All competitors + our top performers\n        high_perf_group = pd.concat([competitor_posts, high_perf_ours])\n        # Low-performing group: Our bottom performers\n        low_perf_group = low_perf_ours\n\n        # Get original DataFrame integer positions for slicing the embedding tensor.\n        # Must filter for -1, which get_indexer_for returns for non-existent labels.\n        group_locs = df.index.get_indexer_for(group.index)\n        high_perf_locs = df.index.get_indexer_for(high_perf_group.index)\n        low_perf_locs = df.index.get_indexer_for(low_perf_group.index)\n\n        valid_group_locs = group_locs[group_locs != -1]\n        valid_high_perf_locs = high_perf_locs[high_perf_locs != -1]\n        valid_low_perf_locs = low_perf_locs[low_perf_locs != -1]\n        \n        high_perf_centroids = torch.Tensor().to(all_embeddings.device)\n        low_perf_centroids = torch.Tensor().to(all_embeddings.device)\n\n        # --- Cluster High-Performing Posts to find Success Archetypes ---\n        if len(valid_high_perf_locs) > 0:\n            high_perf_embeddings = all_embeddings[valid_high_perf_locs]\n            n_clusters_high = min(3, len(valid_high_perf_locs))\n            if n_clusters_high > 0:\n                kmeans_high = KMeans(n_clusters=n_clusters_high, random_state=42, n_init='auto')\n                kmeans_high.fit(high_perf_embeddings.cpu().numpy())\n                high_perf_centroids = torch.tensor(kmeans_high.cluster_centers_, device=all_embeddings.device)\n\n        # --- Cluster Low-Performing Posts to find Failure Archetypes ---\n        if len(valid_low_perf_locs) > 0:\n            low_perf_embeddings = all_embeddings[valid_low_perf_locs]\n            n_clusters_low = min(3, len(valid_low_perf_locs))\n            if n_clusters_low > 0:\n                kmeans_low = KMeans(n_clusters=n_clusters_low, random_state=42, n_init='auto')\n                kmeans_low.fit(low_perf_embeddings.cpu().numpy())\n                low_perf_centroids = torch.tensor(kmeans_low.cluster_centers_, device=all_embeddings.device)\n        \n        # --- Score every post in the current group ---\n        if len(valid_group_locs) > 0:\n            group_embeddings = all_embeddings[valid_group_locs]\n            \n            if high_perf_centroids.numel() > 0:\n                sim_to_high = util.cos_sim(group_embeddings, high_perf_centroids)\n                max_sim_high, _ = torch.max(sim_to_high, dim=1)\n            else:\n                max_sim_high = torch.zeros(len(valid_group_locs), device=all_embeddings.device)\n\n            if low_perf_centroids.numel() > 0:\n                sim_to_low = util.cos_sim(group_embeddings, low_perf_centroids)\n                max_sim_low, _ = torch.max(sim_to_low, dim=1)\n            else:\n                max_sim_low = torch.zeros(len(valid_group_locs), device=all_embeddings.device)\n            \n            contrast_scores = (max_sim_high - max_sim_low).cpu().tolist()\n            \n            df.loc[group.index, feature_name] = contrast_scores\n\n    # Fill any NaNs for groups that couldn't be processed with a neutral 0\n    df[feature_name].fillna(0, inplace=True)\n\n    return df"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.3405989787564634,
          "p_value": 0.0007318021804215847,
          "interpretation": "중간 정도의 양의 상관관계(0.3406)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0007), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.12089014109634857,
          "p_value": 0.2432190554425769,
          "interpretation": "약한 양의 상관관계(0.1209)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.2432), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic category ('top_category_name'), high-performing content (competitors + our top 30%) clusters into several distinct 'Success Archetypes' (e.g., 'detailed guide', 'comparative review'), while low-performing content (our bottom 30%) clusters into 'Failure Archetypes'. A post's success is predicted by its ability to strongly align with a winning pattern while simultaneously distancing itself from common failure patterns. This feature calculates the difference between a post's maximum similarity to any Success Archetype and its maximum similarity to any Failure Archetype. A higher score is expected to positively correlate with performance (`non_brand_inflow`).",
      "feature_created": "archetypal_contrast_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:11:50.170485",
    "iteration": 5,
    "initial_agent_response": {
      "feature_name": "thematic_alignment_score",
      "hypothesis": "High-performing content within a topic (`top_category_name`) forms a relatively cohesive 'success theme,' while low-performing content forms a distinct 'failure theme.' A post's performance is predicted by its alignment with the success theme and its divergence from the failure theme. This feature measures a post's average semantic similarity to all posts in the high-performing group (all competitors + our top 30% by 'non_brand_inflow') and its average similarity to all posts in the low-performing group (our bottom 30%). The final score is `avg_sim_high - avg_sim_low`. A higher score signifies a better alignment with successful content patterns and is expected to correlate positively with performance metrics like `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Safely initializes and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a model known for strong performance on semantic tasks.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\" \n    Calculates the thematic alignment score for each post.\n\n    This score represents how much more semantically similar a post is to the cluster\n    of high-performing content versus the cluster of low-performing content within\n    its specific topic category.\n    \"\"\"\n    # 2. Check for an empty DataFrame to prevent errors.\n    if df.empty:\n        return df\n\n    model = get_model()\n    feature_name = 'thematic_alignment_score'\n    df[feature_name] = np.nan # Initialize the feature column.\n\n    # 3. Preprocess text columns to ensure they are strings and handle potential NaNs.\n    df['post_title'] = df['post_title'].fillna('').astype(str)\n    df['post_body'] = df['post_body'].fillna('').astype(str)\n    df['top_category_name'] = df['top_category_name'].fillna('Uncategorized').astype(str)\n\n    # Create a combined text field for a richer semantic representation.\n    texts = (df['post_title'] + ' ' + df['post_body']).tolist()\n\n    # 4. Encode all texts in a single, efficient batch.\n    # This is significantly faster than row-by-row processing.\n    try:\n        embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n    except RuntimeError as e:\n        if 'CUDA out of memory' in str(e):\n            print(\"CUDA out of memory. Falling back to CPU for encoding.\")\n            torch.cuda.empty_cache()\n            device = 'cpu'\n            model.to(device)\n            embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=False, device=device)\n        else:\n            raise e\n\n    # 5. Process each topic category independently.\n    for category, group_df in df.groupby('top_category_name'):\n        \n        # --- Define High and Low Performance Groups for the current category ---\n        ours_df = group_df[group_df['source'] == 'ours']\n        competitor_df = group_df[group_df['source'] == 'competitor']\n\n        # Skip if there's not enough data in 'ours' to define performance tiers.\n        if ours_df.shape[0] < 3: \n            continue\n            \n        # Calculate performance thresholds based on 'non_brand_inflow'.\n        low_perf_threshold = ours_df['non_brand_inflow'].quantile(0.3, interpolation='lower')\n        high_perf_threshold = ours_df['non_brand_inflow'].quantile(0.7, interpolation='higher')\n\n        # Define high-performing posts: all competitors + our top 30%.\n        our_high_perf_df = ours_df[ours_df['non_brand_inflow'] >= high_perf_threshold]\n        high_perf_df = pd.concat([competitor_df, our_high_perf_df])\n\n        # Define low-performing posts: our bottom 30%.\n        low_perf_df = ours_df[ours_df['non_brand_inflow'] <= low_perf_threshold]\n\n        # Ensure both comparison groups are non-empty to avoid errors.\n        if high_perf_df.empty or low_perf_df.empty:\n            continue\n            \n        # --- Vectorized Semantic Calculation ---\n        # 6. [CRITICAL] Get integer locations (iloc) of group members in the original dataframe.\n        # This correctly maps from the DataFrame's index to the tensor's positional index, \n        # preventing the common error of mixing up labels and positions.\n        group_locs = df.index.get_indexer(group_df.index)\n        high_perf_locs = df.index.get_indexer(high_perf_df.index)\n        low_perf_locs = df.index.get_indexer(low_perf_df.index)\n        \n        # Slice the master embeddings tensor using the correct integer locations.\n        group_embeddings = embeddings[group_locs]\n        high_embeddings = embeddings[high_perf_locs]\n        low_embeddings = embeddings[low_perf_locs]\n        \n        # Calculate cosine similarity between every post in the category and the performance groups.\n        sim_to_high = util.cos_sim(group_embeddings, high_embeddings)\n        sim_to_low = util.cos_sim(group_embeddings, low_embeddings)\n        \n        # Average the similarities for each post to get its alignment with each group.\n        avg_sim_high = sim_to_high.mean(dim=1)\n        avg_sim_low = sim_to_low.mean(dim=1)\n\n        # The final score is the difference in alignment.\n        scores = avg_sim_high - avg_sim_low\n        \n        # 7. Assign the calculated scores back to the original DataFrame using the group's index.\n        df.loc[group_df.index, feature_name] = scores.cpu().numpy()\n\n    # 8. Return the full DataFrame with the new feature column.\n    return df"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.24396614817566295,
          "p_value": 0.017194204885550363,
          "interpretation": "약한 양의 상관관계(0.2440)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0172), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.12469212296102598,
          "p_value": 0.22860323088842682,
          "interpretation": "약한 양의 상관관계(0.1247)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.2286), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "High-performing content within a topic (`top_category_name`) forms a relatively cohesive 'success theme,' while low-performing content forms a distinct 'failure theme.' A post's performance is predicted by its alignment with the success theme and its divergence from the failure theme. This feature measures a post's average semantic similarity to all posts in the high-performing group (all competitors + our top 30% by 'non_brand_inflow') and its average similarity to all posts in the low-performing group (our bottom 30%). The final score is `avg_sim_high - avg_sim_low`. A higher score signifies a better alignment with successful content patterns and is expected to correlate positively with performance metrics like `non_brand_inflow`.",
      "feature_created": "thematic_alignment_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:12:55.531730",
    "iteration": 6,
    "initial_agent_response": {
      "feature_name": "competitive_positioning_score",
      "hypothesis": "Within each topic category ('top_category_name'), high-performing content clusters into distinct 'Success Archetypes', and low-performing content forms 'Failure Archetypes'. A post's potential is best measured not by its absolute similarity to any single group, but by its relative positioning between the nearest success and failure patterns. This feature calculates a normalized score representing how much closer a post is to its nearest Success Archetype versus its nearest Failure Archetype. A score approaching 1.0 indicates strong alignment with a winning pattern and distance from losing ones, predicting higher `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a multilingual model as it's a good general-purpose starting point.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score based on a post's semantic proximity to the nearest 'Success Archetype' \n    relative to its proximity to the nearest 'Failure Archetype' within its topic category.\n    \"\"\"\n    # 2. Check for empty DataFrame.\n    if df.empty:\n        df['competitive_positioning_score'] = pd.Series(dtype=float)\n        return df\n\n    model = get_model()\n\n    # 3. Initialize the feature column with a default value.\n    df['competitive_positioning_score'] = np.nan\n\n    # 4. Create a combined text field for embedding to ensure consistency.\n    # Handle potential missing values in titles or bodies.\n    texts_to_embed = (df['post_title'].fillna('') + ' ' + df['post_body'].fillna('')).tolist()\n    all_embeddings = model.encode(texts_to_embed, convert_to_tensor=True, show_progress_bar=False)\n\n    # 5. Group by the main category for analysis.\n    for category, group_df in df.groupby('top_category_name'):\n        # Preserve original indices for final assignment\n        group_indices = group_df.index\n\n        # Define 'ours' posts within the category\n        ours_df = group_df[group_df['source'] == 'ours']\n\n        # Need at least 10 'ours' posts to have a meaningful percentile split\n        if len(ours_df) < 10:\n            continue\n\n        # Define performance thresholds for 'ours' posts\n        low_perf_threshold = ours_df['non_brand_inflow'].quantile(0.3, interpolation='lower')\n        high_perf_threshold = ours_df['non_brand_inflow'].quantile(0.7, interpolation='higher')\n\n        # Define high and low performance groups based on indices\n        high_perf_indices = group_df[\n            (group_df['source'] == 'competitor') |\n            ((group_df['source'] == 'ours') & (group_df['non_brand_inflow'] >= high_perf_threshold))\n        ].index\n\n        low_perf_indices = ours_df[ours_df['non_brand_inflow'] <= low_perf_threshold].index\n\n        # Ensure we have members in both groups to create archetypes\n        if high_perf_indices.empty or low_perf_indices.empty:\n            continue\n\n        # Map DataFrame indices to integer positions for tensor slicing\n        # This correctly handles non-sequential or shuffled indices.\n        high_perf_tensor_indices = df.index.get_indexer(high_perf_indices)\n        low_perf_tensor_indices = df.index.get_indexer(low_perf_indices)\n        group_tensor_indices = df.index.get_indexer(group_indices)\n\n        high_embeddings = all_embeddings[high_perf_tensor_indices]\n        low_embeddings = all_embeddings[low_perf_tensor_indices]\n        group_embeddings = all_embeddings[group_tensor_indices]\n\n        # 6. Cluster to find archetypes (centroids)\n        # Dynamically set k to avoid errors if a group is smaller than k\n        k_high = min(3, len(high_embeddings))\n        k_low = min(3, len(low_embeddings))\n\n        if k_high == 0 or k_low == 0:\n            continue\n\n        kmeans_high = KMeans(n_clusters=k_high, random_state=42, n_init='auto').fit(high_embeddings.cpu().numpy())\n        high_archetypes = torch.tensor(kmeans_high.cluster_centers_).to(model.device)\n\n        kmeans_low = KMeans(n_clusters=k_low, random_state=42, n_init='auto').fit(low_embeddings.cpu().numpy())\n        low_archetypes = torch.tensor(kmeans_low.cluster_centers_).to(model.device)\n\n        # 7. Calculate scores for each post in the current category\n        # Cosine similarity is used; distance is 1 - similarity.\n        sim_to_high_archetypes = util.cos_sim(group_embeddings, high_archetypes)\n        sim_to_low_archetypes = util.cos_sim(group_embeddings, low_archetypes)\n\n        # Find the minimum distance (i.e., maximum similarity) to any archetype in each set\n        max_sim_high = torch.max(sim_to_high_archetypes, dim=1).values\n        max_sim_low = torch.max(sim_to_low_archetypes, dim=1).values\n        \n        # Convert to distance: lower value is better (closer)\n        min_dist_high = 1 - max_sim_high\n        min_dist_low = 1 - max_sim_low\n\n        # Calculate the final normalized score\n        # The score represents proximity to a success archetype relative to the failure ones.\n        # Epsilon (1e-9) prevents division by zero.\n        score = min_dist_low / (min_dist_high + min_dist_low + 1e-9)\n\n        # 8. Assign the calculated scores back to the original DataFrame using preserved indices.\n        df.loc[group_indices, 'competitive_positioning_score'] = score.cpu().numpy()\n\n    return df\n"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.32866992593929834,
          "p_value": 0.0011463189206592445,
          "interpretation": "중간 정도의 양의 상관관계(0.3287)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0011), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.15960869843150388,
          "p_value": 0.1223459566190262,
          "interpretation": "약한 양의 상관관계(0.1596)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.1223), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic category ('top_category_name'), high-performing content clusters into distinct 'Success Archetypes', and low-performing content forms 'Failure Archetypes'. A post's potential is best measured not by its absolute similarity to any single group, but by its relative positioning between the nearest success and failure patterns. This feature calculates a normalized score representing how much closer a post is to its nearest Success Archetype versus its nearest Failure Archetype. A score approaching 1.0 indicates strong alignment with a winning pattern and distance from losing ones, predicting higher `non_brand_inflow`.",
      "feature_created": "competitive_positioning_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:14:57.772609",
    "iteration": 7,
    "initial_agent_response": {
      "feature_name": "enriched_archetypal_contrast_score",
      "hypothesis": "By creating an 'enriched' text representation that combines the `post_title` (for focus), `post_body` (for semantic context), and `morpheme_words` (for explicit keyword signals), we can generate more distinct semantic archetypes for high- and low-performing content. A post's performance is predicted by its ability to align with a dominant 'Success Archetype' while simultaneously distancing itself from any 'Failure Archetype' within its topic category. The final score, calculated as `(max similarity to success archetypes) - (max similarity to failure archetypes)`, is expected to have a strong positive correlation with `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a model that is strong for Korean and multilingual contexts\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generates the enriched_archetypal_contrast_score feature.\n\n    This feature works by:\n    1. Creating an 'enriched' text field combining title, body, and morphemes.\n    2. For each 'top_category_name', identifying high and low performing posts.\n    3. Using KMeans clustering to find 'Success Archetypes' from high-performers\n       and 'Failure Archetypes' from low-performers.\n    4. Scoring each post based on its maximum similarity to a success archetype\n       minus its maximum similarity to a failure archetype.\n    \"\"\"\n    if df.empty:\n        df['enriched_archetypal_contrast_score'] = []\n        return df\n\n    model = get_model()\n    feature_name = 'enriched_archetypal_contrast_score'\n    df[feature_name] = 0.0\n\n    # Create the enriched text representation\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).str.strip()\n\n    # 2. Use efficient batch processing for all posts at once\n    # This is critical for performance and avoids re-computing embeddings in a loop.\n    all_embeddings = model.encode(\n        df['enriched_text'].tolist(),\n        convert_to_tensor=True, \n        show_progress_bar=False\n    )\n\n    # Group by the main topic category to perform analysis within relevant contexts\n    for category, group_df in df.groupby('top_category_name'):\n        # Identify our posts within the category to define performance tiers\n        ours_df = group_df[group_df['source'] == 'ours']\n\n        if ours_df.empty or ours_df['non_brand_inflow'].isnull().all() or ours_df['non_brand_inflow'].nunique() < 2:\n            # Not enough data to define high/low performers, skip this category\n            continue\n\n        # Define performance thresholds\n        low_quantile = ours_df['non_brand_inflow'].quantile(0.3, interpolation='lower')\n        high_quantile = ours_df['non_brand_inflow'].quantile(0.7, interpolation='higher')\n\n        # Get indices for each group, ensuring we keep the original DataFrame index\n        low_perf_ours_indices = ours_df[ours_df['non_brand_inflow'] <= low_quantile].index\n        high_perf_ours_indices = ours_df[ours_df['non_brand_inflow'] >= high_quantile].index\n        competitor_indices = group_df[group_df['source'] == 'competitor'].index\n        \n        # Define the complete high and low performance groups\n        high_perf_indices = competitor_indices.union(high_perf_ours_indices)\n        low_perf_indices = low_perf_ours_indices\n\n        if high_perf_indices.empty or low_perf_indices.empty:\n            continue\n\n        # --- CRITICAL: Use integer locations to slice the master embedding tensor --- #\n        # Get the integer positions of our indices relative to the original full DataFrame\n        # This prevents misalignment errors when slicing the pre-computed tensor.\n        try:\n            group_locs = df.index.get_indexer_for(group_df.index)\n            high_perf_locs = df.index.get_indexer_for(high_perf_indices)\n            low_perf_locs = df.index.get_indexer_for(low_perf_indices)\n        except KeyError:\n            # This can happen in rare edge cases with multi-index, though unlikely here.\n            continue\n\n        group_embeddings = all_embeddings[group_locs]\n        high_perf_embeddings = all_embeddings[high_perf_locs]\n        low_perf_embeddings = all_embeddings[low_perf_locs]\n\n        # Determine number of archetypes (clusters). Fixed k is more robust.\n        k_high = min(3, len(high_perf_embeddings))\n        k_low = min(3, len(low_perf_embeddings))\n\n        if k_high == 0 or k_low == 0:\n            continue\n\n        # Find Success Archetypes\n        kmeans_high = KMeans(n_clusters=k_high, random_state=42, n_init='auto').fit(high_perf_embeddings.cpu().numpy())\n        success_archetypes = util.normalize_embeddings(model.encode(kmeans_high.cluster_centers_, convert_to_tensor=True))\n\n        # Find Failure Archetypes\n        kmeans_low = KMeans(n_clusters=k_low, random_state=42, n_init='auto').fit(low_perf_embeddings.cpu().numpy())\n        failure_archetypes = util.normalize_embeddings(model.encode(kmeans_low.cluster_centers_, convert_to_tensor=True))\n        \n        # Calculate similarity scores for every post in the current category\n        sim_to_success_archetypes = util.cos_sim(group_embeddings, success_archetypes)\n        sim_to_failure_archetypes = util.cos_sim(group_embeddings, failure_archetypes)\n\n        # For each post, find its similarity to the CLOSEST archetype in each group\n        max_sim_high = sim_to_success_archetypes.max(axis=1).values\n        max_sim_low = sim_to_failure_archetypes.max(axis=1).values\n\n        # The final score is the contrast between aligning with success and aligning with failure\n        contrast_scores = max_sim_high - max_sim_low\n\n        # Assign scores back to the original DataFrame using the correct index\n        df.loc[group_df.index, feature_name] = contrast_scores.cpu().numpy()\n\n    # Clean up temporary column\n    df.drop(columns=['enriched_text'], inplace=True)\n    \n    return df\n"
    },
    "correction_attempts_made": 1,
    "correction_history": [
      {
        "attempt": 1,
        "response_raw": "```json\n{\n  \"feature_name\": \"enriched_archetypal_contrast_score\",\n  \"hypothesis\": \"By creating an 'enriched' text representation that combines the `post_title` (for focus), `post_body` (for semantic context), and `morpheme_words` (for explicit keyword signals), we can generate more distinct semantic archetypes for high- and low-performing content. A post's performance is predicted by its ability to align with a dominant 'Success Archetype' while simultaneously distancing itself from any 'Failure Archetype' within its topic category. The final score, calculated as `(max similarity to success archetypes) - (max similarity to failure archetypes)`, is expected to have a strong positive correlation with `non_brand_inflow`.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nfrom sentence_transformers import SentenceTransformer, util\\nfrom sklearn.cluster import KMeans\\nimport torch\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    global _model\\n    if _model is None:\\n        # Using a model that is strong for Korean and multilingual contexts\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Generates the enriched_archetypal_contrast_score feature.\\n\\n    This feature works by:\\n    1. Creating an 'enriched' text field combining title, body, and morphemes.\\n    2. For each 'top_category_name', identifying high and low performing posts.\\n    3. Using KMeans clustering to find 'Success Archetypes' from high-performers\\n       and 'Failure Archetypes' from low-performers.\\n    4. Scoring each post based on its maximum similarity to a success archetype\\n       minus its maximum similarity to a failure archetype.\\n    \\\"\\\"\\\"\\n    if df.empty:\\n        df['enriched_archetypal_contrast_score'] = pd.Series(dtype=float)\\n        return df\\n\\n    model = get_model()\\n    feature_name = 'enriched_archetypal_contrast_score'\\n    df[feature_name] = 0.0\\n\\n    # Create the enriched text representation\\n    df['enriched_text'] = (\\n        df['post_title'].fillna('') + ' ' + \\n        df['post_body'].fillna('') + ' ' + \\n        df['morpheme_words'].fillna('')\\n    ).str.strip()\\n\\n    # 2. Use efficient batch processing for all posts at once\\n    all_embeddings = model.encode(\\n        df['enriched_text'].tolist(),\\n        convert_to_tensor=True, \\n        show_progress_bar=False\\n    )\\n\\n    # Group by the main topic category to perform analysis within relevant contexts\\n    for category, group_df in df.groupby('top_category_name'):\\n        # Identify our posts within the category to define performance tiers\\n        ours_df = group_df[group_df['source'] == 'ours']\\n\\n        if ours_df.empty or ours_df['non_brand_inflow'].isnull().all() or ours_df['non_brand_inflow'].nunique() < 2:\\n            continue\\n\\n        # Define performance thresholds\\n        low_quantile = ours_df['non_brand_inflow'].quantile(0.3, interpolation='lower')\\n        high_quantile = ours_df['non_brand_inflow'].quantile(0.7, interpolation='higher')\\n\\n        # Get indices for each group, ensuring we keep the original DataFrame index\\n        low_perf_ours_indices = ours_df[ours_df['non_brand_inflow'] <= low_quantile].index\\n        high_perf_ours_indices = ours_df[ours_df['non_brand_inflow'] >= high_quantile].index\\n        competitor_indices = group_df[group_df['source'] == 'competitor'].index\\n        \\n        high_perf_indices = competitor_indices.union(high_perf_ours_indices)\\n        low_perf_indices = low_perf_ours_indices\\n\\n        if high_perf_indices.empty or low_perf_indices.empty:\\n            continue\\n\\n        # Use integer locations to slice the master embedding tensor\\n        try:\\n            group_locs = df.index.get_indexer_for(group_df.index)\\n            high_perf_locs = df.index.get_indexer_for(high_perf_indices)\\n            low_perf_locs = df.index.get_indexer_for(low_perf_indices)\\n        except KeyError:\\n            continue\\n\\n        # Filter out any -1s which indicate an index wasn't found\\n        group_locs = group_locs[group_locs != -1]\\n        high_perf_locs = high_perf_locs[high_perf_locs != -1]\\n        low_perf_locs = low_perf_locs[low_perf_locs != -1]\\n        \\n        if len(high_perf_locs) == 0 or len(low_perf_locs) == 0:\\n             continue\\n\\n        group_embeddings = all_embeddings[group_locs]\\n        high_perf_embeddings = all_embeddings[high_perf_locs]\\n        low_perf_embeddings = all_embeddings[low_perf_locs]\\n\\n        k_high = min(3, len(high_perf_embeddings))\\n        k_low = min(3, len(low_perf_embeddings))\\n\\n        # Find Success Archetypes\\n        kmeans_high = KMeans(n_clusters=k_high, random_state=42, n_init='auto').fit(high_perf_embeddings.cpu().numpy())\\n        # SELF-CORRECTION: The cluster centers ARE the archetypes in the embedding space.\\n        # DO NOT re-encode them. Just convert the numpy vectors to tensors.\\n        success_archetypes_np = kmeans_high.cluster_centers_\\n        success_archetypes = util.normalize_embeddings(torch.from_numpy(success_archetypes_np).to(all_embeddings.device))\\n\\n        # Find Failure Archetypes\\n        kmeans_low = KMeans(n_clusters=k_low, random_state=42, n_init='auto').fit(low_perf_embeddings.cpu().numpy())\\n        # SELF-CORRECTION: Same logic as above. The centroids are already vectors.\\n        failure_archetypes_np = kmeans_low.cluster_centers_\\n        failure_archetypes = util.normalize_embeddings(torch.from_numpy(failure_archetypes_np).to(all_embeddings.device))\\n        \\n        # Calculate similarity scores for every post in the current category\\n        sim_to_success_archetypes = util.cos_sim(group_embeddings, success_archetypes)\\n        sim_to_failure_archetypes = util.cos_sim(group_embeddings, failure_archetypes)\\n\\n        max_sim_high = sim_to_success_archetypes.max(axis=1).values\\n        max_sim_low = sim_to_failure_archetypes.max(axis=1).values\\n\\n        contrast_scores = max_sim_high - max_sim_low\\n\\n        df.loc[group_df.index, feature_name] = contrast_scores.cpu().numpy()\\n\\n    df.drop(columns=['enriched_text'], inplace=True)\\n    \\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "enriched_archetypal_contrast_score",
          "hypothesis": "By creating an 'enriched' text representation that combines the `post_title` (for focus), `post_body` (for semantic context), and `morpheme_words` (for explicit keyword signals), we can generate more distinct semantic archetypes for high- and low-performing content. A post's performance is predicted by its ability to align with a dominant 'Success Archetype' while simultaneously distancing itself from any 'Failure Archetype' within its topic category. The final score, calculated as `(max similarity to success archetypes) - (max similarity to failure archetypes)`, is expected to have a strong positive correlation with `non_brand_inflow`.",
          "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a model that is strong for Korean and multilingual contexts\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generates the enriched_archetypal_contrast_score feature.\n\n    This feature works by:\n    1. Creating an 'enriched' text field combining title, body, and morphemes.\n    2. For each 'top_category_name', identifying high and low performing posts.\n    3. Using KMeans clustering to find 'Success Archetypes' from high-performers\n       and 'Failure Archetypes' from low-performers.\n    4. Scoring each post based on its maximum similarity to a success archetype\n       minus its maximum similarity to a failure archetype.\n    \"\"\"\n    if df.empty:\n        df['enriched_archetypal_contrast_score'] = pd.Series(dtype=float)\n        return df\n\n    model = get_model()\n    feature_name = 'enriched_archetypal_contrast_score'\n    df[feature_name] = 0.0\n\n    # Create the enriched text representation\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).str.strip()\n\n    # 2. Use efficient batch processing for all posts at once\n    all_embeddings = model.encode(\n        df['enriched_text'].tolist(),\n        convert_to_tensor=True, \n        show_progress_bar=False\n    )\n\n    # Group by the main topic category to perform analysis within relevant contexts\n    for category, group_df in df.groupby('top_category_name'):\n        # Identify our posts within the category to define performance tiers\n        ours_df = group_df[group_df['source'] == 'ours']\n\n        if ours_df.empty or ours_df['non_brand_inflow'].isnull().all() or ours_df['non_brand_inflow'].nunique() < 2:\n            continue\n\n        # Define performance thresholds\n        low_quantile = ours_df['non_brand_inflow'].quantile(0.3, interpolation='lower')\n        high_quantile = ours_df['non_brand_inflow'].quantile(0.7, interpolation='higher')\n\n        # Get indices for each group, ensuring we keep the original DataFrame index\n        low_perf_ours_indices = ours_df[ours_df['non_brand_inflow'] <= low_quantile].index\n        high_perf_ours_indices = ours_df[ours_df['non_brand_inflow'] >= high_quantile].index\n        competitor_indices = group_df[group_df['source'] == 'competitor'].index\n        \n        high_perf_indices = competitor_indices.union(high_perf_ours_indices)\n        low_perf_indices = low_perf_ours_indices\n\n        if high_perf_indices.empty or low_perf_indices.empty:\n            continue\n\n        # Use integer locations to slice the master embedding tensor\n        try:\n            group_locs = df.index.get_indexer_for(group_df.index)\n            high_perf_locs = df.index.get_indexer_for(high_perf_indices)\n            low_perf_locs = df.index.get_indexer_for(low_perf_indices)\n        except KeyError:\n            continue\n\n        # Filter out any -1s which indicate an index wasn't found\n        group_locs = group_locs[group_locs != -1]\n        high_perf_locs = high_perf_locs[high_perf_locs != -1]\n        low_perf_locs = low_perf_locs[low_perf_locs != -1]\n        \n        if len(high_perf_locs) == 0 or len(low_perf_locs) == 0:\n             continue\n\n        group_embeddings = all_embeddings[group_locs]\n        high_perf_embeddings = all_embeddings[high_perf_locs]\n        low_perf_embeddings = all_embeddings[low_perf_locs]\n\n        k_high = min(3, len(high_perf_embeddings))\n        k_low = min(3, len(low_perf_embeddings))\n\n        # Find Success Archetypes\n        kmeans_high = KMeans(n_clusters=k_high, random_state=42, n_init='auto').fit(high_perf_embeddings.cpu().numpy())\n        # SELF-CORRECTION: The cluster centers ARE the archetypes in the embedding space.\n        # DO NOT re-encode them. Just convert the numpy vectors to tensors.\n        success_archetypes_np = kmeans_high.cluster_centers_\n        success_archetypes = util.normalize_embeddings(torch.from_numpy(success_archetypes_np).to(all_embeddings.device))\n\n        # Find Failure Archetypes\n        kmeans_low = KMeans(n_clusters=k_low, random_state=42, n_init='auto').fit(low_perf_embeddings.cpu().numpy())\n        # SELF-CORRECTION: Same logic as above. The centroids are already vectors.\n        failure_archetypes_np = kmeans_low.cluster_centers_\n        failure_archetypes = util.normalize_embeddings(torch.from_numpy(failure_archetypes_np).to(all_embeddings.device))\n        \n        # Calculate similarity scores for every post in the current category\n        sim_to_success_archetypes = util.cos_sim(group_embeddings, success_archetypes)\n        sim_to_failure_archetypes = util.cos_sim(group_embeddings, failure_archetypes)\n\n        max_sim_high = sim_to_success_archetypes.max(axis=1).values\n        max_sim_low = sim_to_failure_archetypes.max(axis=1).values\n\n        contrast_scores = max_sim_high - max_sim_low\n\n        df.loc[group_df.index, feature_name] = contrast_scores.cpu().numpy()\n\n    df.drop(columns=['enriched_text'], inplace=True)\n    \n    return df"
        }
      }
    ],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.3420276110301501,
          "p_value": 0.0006926739081232632,
          "interpretation": "중간 정도의 양의 상관관계(0.3420)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0007), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.11042405211980323,
          "p_value": 0.28674279333712677,
          "interpretation": "약한 양의 상관관계(0.1104)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.2867), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "By creating an 'enriched' text representation that combines the `post_title` (for focus), `post_body` (for semantic context), and `morpheme_words` (for explicit keyword signals), we can generate more distinct semantic archetypes for high- and low-performing content. A post's performance is predicted by its ability to align with a dominant 'Success Archetype' while simultaneously distancing itself from any 'Failure Archetype' within its topic category. The final score, calculated as `(max similarity to success archetypes) - (max similarity to failure archetypes)`, is expected to have a strong positive correlation with `non_brand_inflow`.",
      "feature_created": "enriched_archetypal_contrast_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:16:00.820095",
    "iteration": 8,
    "initial_agent_response": {
      "feature_name": "archetype_resonance_score",
      "hypothesis": "Within each topic category ('top_category_name'), there are multiple distinct 'Success Archetypes' (from high-performers: all competitors + our top 30%) and 'Failure Archetypes' (from low-performers: our bottom 30%). A post's performance is driven by its holistic alignment with the entire suite of successful patterns, not just its proximity to a single one. This feature identifies these archetypes via clustering on an enriched text representation (`title` + `body` + `morphemes`). It then scores each post by calculating `(AvgSim_Success - AvgSim_Failure) / (AvgSim_Success + AvgSim_Failure)`, where `AvgSim` is the average similarity to all archetypes in a group. A high score signifies strong resonance with successful patterns and is expected to positively correlate with `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a model that performs well on Korean and general text.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the Archetype Resonance Score for each post.\n\n    This score measures a post's holistic alignment with clusters of successful content\n    versus clusters of unsuccessful content within its topic category.\n    \"\"\"\n    # 2. Check for empty DataFrame to prevent errors.\n    if df.empty:\n        return df\n\n    feature_name = 'archetype_resonance_score'\n    df[feature_name] = np.nan\n\n    model = get_model()\n\n    # 3. Create an 'enriched' text field for more nuanced embeddings.\n    # This combines high-level focus (title), core content (body), and explicit keywords (morphemes).\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' +\n        df['post_body'].fillna('') + ' ' +\n        df['morpheme_words'].fillna('')\n    ).str.strip()\n\n    # 4. Efficiently encode all texts in one batch, preserving the original index.\n    # This is critical for correctly mapping results back to the DataFrame.\n    texts = df['enriched_text'].tolist()\n    all_embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n\n    # Group by the main topic category to perform analysis within relevant contexts.\n    for category, group in df.groupby('top_category_name'):\n        \n        # Define performance groups based on the provided strategy.\n        ours_df = group[group['source'] == 'ours']\n\n        # Skip category if there's not enough data from our posts to define performance tiers.\n        if ours_df.empty or ours_df['non_brand_inflow'].nunique() < 2:\n            continue\n\n        # Define performance thresholds. Using dropna() is crucial for robustness.\n        inflow = ours_df['non_brand_inflow'].dropna()\n        if inflow.empty:\n            continue\n        high_perf_thresh = inflow.quantile(0.7, interpolation='higher')\n        low_perf_thresh = inflow.quantile(0.3, interpolation='lower')\n\n        # Get the original DataFrame indices for each group.\n        high_perf_ours_idx = ours_df[ours_df['non_brand_inflow'] >= high_perf_thresh].index\n        low_perf_ours_idx = ours_df[ours_df['non_brand_inflow'] <= low_perf_thresh].index\n        competitor_idx = group[group['source'] == 'competitor'].index\n\n        high_perf_group_idx = competitor_idx.union(high_perf_ours_idx)\n        low_perf_group_idx = low_perf_ours_idx\n        \n        # 5. Use get_indexer_for to safely find integer positions for slicing the embeddings tensor.\n        # This prevents the critical error of mixing up label-based and integer-based indexing.\n        high_perf_locs = df.index.get_indexer_for(high_perf_group_idx)\n        low_perf_locs = df.index.get_indexer_for(low_perf_group_idx)\n\n        # Set the number of archetypes (clusters) to find.\n        k = 3 \n\n        # Proceed only if both groups have enough members to form clusters.\n        if len(high_perf_locs) < k or len(low_perf_locs) < k:\n            continue\n\n        high_embeddings = all_embeddings[high_perf_locs]\n        low_embeddings = all_embeddings[low_perf_locs]\n\n        # 6. Identify 'Success Archetypes' by clustering high-performing content.\n        try:\n            high_kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto', verbose=0)\n            high_kmeans.fit(high_embeddings.cpu().numpy())\n            high_centroids = torch.tensor(high_kmeans.cluster_centers_, device=model.device)\n\n            # Identify 'Failure Archetypes' by clustering low-performing content.\n            low_kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto', verbose=0)\n            low_kmeans.fit(low_embeddings.cpu().numpy())\n            low_centroids = torch.tensor(low_kmeans.cluster_centers_, device=model.device)\n        except Exception:\n            # If clustering fails for any reason, skip the category.\n            continue\n\n        # Get embeddings for all posts within the current category.\n        category_locs = df.index.get_indexer_for(group.index)\n        category_embeddings = all_embeddings[category_locs]\n\n        # 7. Calculate similarity of each post to all identified archetypes.\n        sim_to_high_centroids = util.cos_sim(category_embeddings, high_centroids)\n        sim_to_low_centroids = util.cos_sim(category_embeddings, low_centroids)\n\n        # Calculate 'Success Resonance' (average similarity to all success archetypes).\n        avg_sim_high = sim_to_high_centroids.mean(dim=1)\n        # Calculate 'Failure Resonance' (average similarity to all failure archetypes).\n        avg_sim_low = sim_to_low_centroids.mean(dim=1)\n\n        # Calculate the final normalized score.\n        numerator = avg_sim_high - avg_sim_low\n        denominator = avg_sim_high + avg_sim_low\n\n        # Add a small epsilon to the denominator to prevent division by zero.\n        final_scores = torch.div(numerator, denominator + 1e-9)\n\n        # 8. Assign the calculated scores back to the original DataFrame using .loc and the group's index.\n        df.loc[group.index, feature_name] = final_scores.cpu().numpy()\n\n    # Drop the temporary column before returning.\n    df = df.drop(columns=['enriched_text'])\n\n    return df\n"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.056712760022495666,
          "p_value": 0.5851418320887928,
          "interpretation": "약한 양의 상관관계(0.0567)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.5851), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.1820148171695939,
          "p_value": 0.07750434910337808,
          "interpretation": "약한 양의 상관관계(0.1820)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.0775), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic category ('top_category_name'), there are multiple distinct 'Success Archetypes' (from high-performers: all competitors + our top 30%) and 'Failure Archetypes' (from low-performers: our bottom 30%). A post's performance is driven by its holistic alignment with the entire suite of successful patterns, not just its proximity to a single one. This feature identifies these archetypes via clustering on an enriched text representation (`title` + `body` + `morphemes`). It then scores each post by calculating `(AvgSim_Success - AvgSim_Failure) / (AvgSim_Success + AvgSim_Failure)`, where `AvgSim` is the average similarity to all archetypes in a group. A high score signifies strong resonance with successful patterns and is expected to positively correlate with `non_brand_inflow`.",
      "feature_created": "archetype_resonance_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:18:01.378923",
    "iteration": 9,
    "initial_agent_response": {
      "feature_name": "sub_topic_contrast_score",
      "hypothesis": "Within each topic category (`top_category_name`), successful content (all competitors + our top 30%) and unsuccessful content (our bottom 30%) are not just monolithic blocks, but are themselves composed of distinct sub-topics or 'themes'. High-performing posts excel by aligning with the most potent 'success themes' while simultaneously avoiding the 'failure themes' common in low-performing content. This feature identifies these sub-topic themes via clustering on an enriched text representation (`title`+`body`+`morphemes`). It then calculates a score as `(max_similarity_to_success_themes) - (max_similarity_to_failure_themes)`. A higher score, indicating a strategic focus on winning sub-topics and avoidance of losing ones, is expected to positively correlate with performance (`non_brand_inflow`).",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a model that is effective for Korean and general semantic tasks.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score based on a post's alignment with successful sub-topics\n    and its divergence from unsuccessful sub-topics.\n    \"\"\"\n    # 0. Handle empty DataFrame\n    if df.empty:\n        return df\n\n    feature_name = 'sub_topic_contrast_score'\n    df[feature_name] = np.nan\n    model = get_model()\n\n    # 1. Create an 'enriched' text field for more robust embeddings\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).str.strip()\n\n    # 2. Efficiently encode all texts in a single batch\n    # This preserves the original DataFrame's order.\n    all_embeddings = model.encode(\n        df['enriched_text'].tolist(),\n        convert_to_tensor=True, \n        show_progress_bar=False\n    )\n\n    # 3. Process each category separately\n    for category, group in df.groupby('top_category_name'):\n        if len(group) < 10: # Skip small categories\n            continue\n\n        # 4. Define High-Performing and Low-Performing groups\n        our_posts = group[group['source'] == 'ours']\n        if our_posts.empty or our_posts['non_brand_inflow'].isnull().all():\n            continue\n\n        # Define performance thresholds safely\n        high_inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n        low_inflow_threshold = our_posts['non_brand_inflow'].quantile(0.3, interpolation='higher')\n\n        high_perf_ours = our_posts[our_posts['non_brand_inflow'] >= high_inflow_threshold]\n        low_perf_ours = our_posts[our_posts['non_brand_inflow'] <= low_inflow_threshold]\n        competitor_posts = group[group['source'] == 'competitor']\n\n        high_perf_group = pd.concat([high_perf_ours, competitor_posts])\n        low_perf_group = low_perf_ours\n\n        # 5. Get embeddings for each group using integer positions\n        # CRITICAL: Use df.index.get_indexer to safely get integer positions for the tensor\n        high_perf_indices = df.index.get_indexer(high_perf_group.index)\n        low_perf_indices = df.index.get_indexer(low_perf_group.index)\n        category_indices = df.index.get_indexer(group.index)\n\n        high_perf_embeddings = all_embeddings[high_perf_indices]\n        low_perf_embeddings = all_embeddings[low_perf_indices]\n        category_embeddings = all_embeddings[category_indices]\n\n        # 6. Cluster to find sub-topic centroids, with safety checks\n        K = 3 # Number of sub-topic clusters\n        if len(high_perf_embeddings) < K or len(low_perf_embeddings) < K:\n            continue # Not enough data to form meaningful clusters\n\n        try:\n            # Cluster high-performers to find 'Success Themes'\n            kmeans_high = KMeans(n_clusters=K, random_state=42, n_init='auto')\n            kmeans_high.fit(high_perf_embeddings.cpu().numpy())\n            high_centroids = kmeans_high.cluster_centers_\n\n            # Cluster low-performers to find 'Failure Themes'\n            kmeans_low = KMeans(n_clusters=K, random_state=42, n_init='auto')\n            kmeans_low.fit(low_perf_embeddings.cpu().numpy())\n            low_centroids = kmeans_low.cluster_centers_\n        except ValueError:\n            continue # KMeans can fail if inputs are strange\n\n        # 7. Calculate similarity scores for every post in the category\n        # Similarity to 'Success Themes'\n        sim_to_high = util.cos_sim(category_embeddings, high_centroids)\n        max_sim_to_high, _ = sim_to_high.max(dim=1)\n\n        # Similarity to 'Failure Themes'\n        sim_to_low = util.cos_sim(category_embeddings, low_centroids)\n        max_sim_to_low, _ = sim_to_low.max(dim=1)\n        \n        # 8. Calculate the final contrast score\n        contrast_score = max_sim_to_high - max_sim_to_low\n\n        # 9. Assign scores back to the original DataFrame using the group's index\n        df.loc[group.index, feature_name] = contrast_score.cpu().numpy()\n\n    # Clean up temporary column\n    df.drop(columns=['enriched_text'], inplace=True)\n\n    return df\n"
    },
    "correction_attempts_made": 1,
    "correction_history": [
      {
        "attempt": 1,
        "response_raw": "```json\n{\n  \"feature_name\": \"sub_topic_contrast_score\",\n  \"hypothesis\": \"Within each topic category (`top_category_name`), successful content (all competitors + our top 30%) and unsuccessful content (our bottom 30%) are not just monolithic blocks, but are themselves composed of distinct sub-topics or 'themes'. High-performing posts excel by aligning with the most potent 'success themes' while simultaneously avoiding the 'failure themes' common in low-performing content. This feature identifies these sub-topic themes via clustering on an enriched text representation (`title`+`body`+`morphemes`). It then calculates a score as `(max_similarity_to_success_themes) - (max_similarity_to_failure_themes)`. A higher score, indicating a strategic focus on winning sub-topics and avoidance of losing ones, is expected to positively correlate with performance (`non_brand_inflow`).\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nfrom sentence_transformers import SentenceTransformer, util\\nfrom sklearn.cluster import KMeans\\nimport torch\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    global _model\\n    if _model is None:\\n        # Using a model that is effective for Korean and general semantic tasks.\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates a score based on a post's alignment with successful sub-topics\\n    and its divergence from unsuccessful sub-topics.\\n    \\\"\\\"\\\"\\n    # 0. Handle empty DataFrame\\n    if df.empty:\\n        return df\\n\\n    feature_name = 'sub_topic_contrast_score'\\n    df[feature_name] = np.nan\\n    model = get_model()\\n\\n    # 1. Create an 'enriched' text field for more robust embeddings\\n    df['enriched_text'] = (\\n        df['post_title'].fillna('') + ' ' + \\n        df['post_body'].fillna('') + ' ' + \\n        df['morpheme_words'].fillna('')\\n    ).str.strip()\\n\\n    # 2. Efficiently encode all texts in a single batch\\n    # This preserves the original DataFrame's order.\\n    all_embeddings = model.encode(\\n        df['enriched_text'].tolist(),\\n        convert_to_tensor=True, \\n        show_progress_bar=False\\n    )\\n\\n    # 3. Process each category separately\\n    for category, group in df.groupby('top_category_name'):\\n        if len(group) < 10: # Skip small categories\\n            continue\\n\\n        # 4. Define High-Performing and Low-Performing groups\\n        our_posts = group[group['source'] == 'ours']\\n        if our_posts.empty or our_posts['non_brand_inflow'].isnull().all():\\n            continue\\n\\n        # Define performance thresholds safely\\n        high_inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\\n        low_inflow_threshold = our_posts['non_brand_inflow'].quantile(0.3, interpolation='higher')\\n\\n        high_perf_ours = our_posts[our_posts['non_brand_inflow'] >= high_inflow_threshold]\\n        low_perf_ours = our_posts[our_posts['non_brand_inflow'] <= low_inflow_threshold]\\n        competitor_posts = group[group['source'] == 'competitor']\\n\\n        high_perf_group = pd.concat([high_perf_ours, competitor_posts])\\n        low_perf_group = low_perf_ours\\n\\n        # 5. Get embeddings for each group using integer positions\\n        # CRITICAL: Use df.index.get_indexer to safely get integer positions for the tensor\\n        high_perf_indices = df.index.get_indexer(high_perf_group.index)\\n        low_perf_indices = df.index.get_indexer(low_perf_group.index)\\n        category_indices = df.index.get_indexer(group.index)\\n\\n        high_perf_embeddings = all_embeddings[high_perf_indices]\\n        low_perf_embeddings = all_embeddings[low_perf_indices]\\n        category_embeddings = all_embeddings[category_indices]\\n\\n        # 6. Cluster to find sub-topic centroids, with safety checks\\n        K = 3 # Number of sub-topic clusters\\n        if len(high_perf_embeddings) < K or len(low_perf_embeddings) < K:\\n            continue # Not enough data to form meaningful clusters\\n\\n        try:\\n            # Cluster high-performers to find 'Success Themes'\\n            kmeans_high = KMeans(n_clusters=K, random_state=42, n_init='auto')\\n            kmeans_high.fit(high_perf_embeddings.cpu().numpy())\\n            high_centroids_np = kmeans_high.cluster_centers_\\n\\n            # Cluster low-performers to find 'Failure Themes'\\n            kmeans_low = KMeans(n_clusters=K, random_state=42, n_init='auto')\\n            kmeans_low.fit(low_perf_embeddings.cpu().numpy())\\n            low_centroids_np = kmeans_low.cluster_centers_\\n        except ValueError:\\n            continue # KMeans can fail if inputs are strange\\n\\n        # 7. [FIX] Convert NumPy centroids to Tensors and move to the correct device\\n        # This resolves the CPU/GPU device mismatch error.\\n        device = category_embeddings.device\\n        high_centroids_tensor = torch.tensor(high_centroids_np, device=device)\\n        low_centroids_tensor = torch.tensor(low_centroids_np, device=device)\\n\\n        # 8. Calculate similarity scores for every post in the category\\n        # Similarity to 'Success Themes'\\n        sim_to_high = util.cos_sim(category_embeddings, high_centroids_tensor)\\n        max_sim_to_high, _ = sim_to_high.max(dim=1)\\n\\n        # Similarity to 'Failure Themes'\\n        sim_to_low = util.cos_sim(category_embeddings, low_centroids_tensor)\\n        max_sim_to_low, _ = sim_to_low.max(dim=1)\\n        \\n        # 9. Calculate the final contrast score\\n        contrast_score = max_sim_to_high - max_sim_to_low\\n\\n        # 10. Assign scores back to the original DataFrame using the group's index\\n        df.loc[group.index, feature_name] = contrast_score.cpu().numpy()\\n\\n    # Clean up temporary column\\n    df.drop(columns=['enriched_text'], inplace=True)\\n\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "sub_topic_contrast_score",
          "hypothesis": "Within each topic category (`top_category_name`), successful content (all competitors + our top 30%) and unsuccessful content (our bottom 30%) are not just monolithic blocks, but are themselves composed of distinct sub-topics or 'themes'. High-performing posts excel by aligning with the most potent 'success themes' while simultaneously avoiding the 'failure themes' common in low-performing content. This feature identifies these sub-topic themes via clustering on an enriched text representation (`title`+`body`+`morphemes`). It then calculates a score as `(max_similarity_to_success_themes) - (max_similarity_to_failure_themes)`. A higher score, indicating a strategic focus on winning sub-topics and avoidance of losing ones, is expected to positively correlate with performance (`non_brand_inflow`).",
          "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a model that is effective for Korean and general semantic tasks.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score based on a post's alignment with successful sub-topics\n    and its divergence from unsuccessful sub-topics.\n    \"\"\"\n    # 0. Handle empty DataFrame\n    if df.empty:\n        return df\n\n    feature_name = 'sub_topic_contrast_score'\n    df[feature_name] = np.nan\n    model = get_model()\n\n    # 1. Create an 'enriched' text field for more robust embeddings\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).str.strip()\n\n    # 2. Efficiently encode all texts in a single batch\n    # This preserves the original DataFrame's order.\n    all_embeddings = model.encode(\n        df['enriched_text'].tolist(),\n        convert_to_tensor=True, \n        show_progress_bar=False\n    )\n\n    # 3. Process each category separately\n    for category, group in df.groupby('top_category_name'):\n        if len(group) < 10: # Skip small categories\n            continue\n\n        # 4. Define High-Performing and Low-Performing groups\n        our_posts = group[group['source'] == 'ours']\n        if our_posts.empty or our_posts['non_brand_inflow'].isnull().all():\n            continue\n\n        # Define performance thresholds safely\n        high_inflow_threshold = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n        low_inflow_threshold = our_posts['non_brand_inflow'].quantile(0.3, interpolation='higher')\n\n        high_perf_ours = our_posts[our_posts['non_brand_inflow'] >= high_inflow_threshold]\n        low_perf_ours = our_posts[our_posts['non_brand_inflow'] <= low_inflow_threshold]\n        competitor_posts = group[group['source'] == 'competitor']\n\n        high_perf_group = pd.concat([high_perf_ours, competitor_posts])\n        low_perf_group = low_perf_ours\n\n        # 5. Get embeddings for each group using integer positions\n        # CRITICAL: Use df.index.get_indexer to safely get integer positions for the tensor\n        high_perf_indices = df.index.get_indexer(high_perf_group.index)\n        low_perf_indices = df.index.get_indexer(low_perf_group.index)\n        category_indices = df.index.get_indexer(group.index)\n\n        high_perf_embeddings = all_embeddings[high_perf_indices]\n        low_perf_embeddings = all_embeddings[low_perf_indices]\n        category_embeddings = all_embeddings[category_indices]\n\n        # 6. Cluster to find sub-topic centroids, with safety checks\n        K = 3 # Number of sub-topic clusters\n        if len(high_perf_embeddings) < K or len(low_perf_embeddings) < K:\n            continue # Not enough data to form meaningful clusters\n\n        try:\n            # Cluster high-performers to find 'Success Themes'\n            kmeans_high = KMeans(n_clusters=K, random_state=42, n_init='auto')\n            kmeans_high.fit(high_perf_embeddings.cpu().numpy())\n            high_centroids_np = kmeans_high.cluster_centers_\n\n            # Cluster low-performers to find 'Failure Themes'\n            kmeans_low = KMeans(n_clusters=K, random_state=42, n_init='auto')\n            kmeans_low.fit(low_perf_embeddings.cpu().numpy())\n            low_centroids_np = kmeans_low.cluster_centers_\n        except ValueError:\n            continue # KMeans can fail if inputs are strange\n\n        # 7. [FIX] Convert NumPy centroids to Tensors and move to the correct device\n        # This resolves the CPU/GPU device mismatch error.\n        device = category_embeddings.device\n        high_centroids_tensor = torch.tensor(high_centroids_np, device=device)\n        low_centroids_tensor = torch.tensor(low_centroids_np, device=device)\n\n        # 8. Calculate similarity scores for every post in the category\n        # Similarity to 'Success Themes'\n        sim_to_high = util.cos_sim(category_embeddings, high_centroids_tensor)\n        max_sim_to_high, _ = sim_to_high.max(dim=1)\n\n        # Similarity to 'Failure Themes'\n        sim_to_low = util.cos_sim(category_embeddings, low_centroids_tensor)\n        max_sim_to_low, _ = sim_to_low.max(dim=1)\n        \n        # 9. Calculate the final contrast score\n        contrast_score = max_sim_to_high - max_sim_to_low\n\n        # 10. Assign scores back to the original DataFrame using the group's index\n        df.loc[group.index, feature_name] = contrast_score.cpu().numpy()\n\n    # Clean up temporary column\n    df.drop(columns=['enriched_text'], inplace=True)\n\n    return df"
        }
      }
    ],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.3429047101486894,
          "p_value": 0.0006696088506905236,
          "interpretation": "중간 정도의 양의 상관관계(0.3429)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0007), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.11403939957662518,
          "p_value": 0.2711585748833745,
          "interpretation": "약한 양의 상관관계(0.1140)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.2712), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic category (`top_category_name`), successful content (all competitors + our top 30%) and unsuccessful content (our bottom 30%) are not just monolithic blocks, but are themselves composed of distinct sub-topics or 'themes'. High-performing posts excel by aligning with the most potent 'success themes' while simultaneously avoiding the 'failure themes' common in low-performing content. This feature identifies these sub-topic themes via clustering on an enriched text representation (`title`+`body`+`morphemes`). It then calculates a score as `(max_similarity_to_success_themes) - (max_similarity_to_failure_themes)`. A higher score, indicating a strategic focus on winning sub-topics and avoidance of losing ones, is expected to positively correlate with performance (`non_brand_inflow`).",
      "feature_created": "sub_topic_contrast_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:19:19.743474",
    "iteration": 10,
    "initial_agent_response": {
      "feature_name": "archetypal_purity_score",
      "hypothesis": "A post's performance is predicted by its strong, 'pure' alignment with a single, clear success archetype, while being free from the 'contamination' of any failure archetypes. The score is calculated as `max_similarity_to_success_archetypes * (1 - max_similarity_to_failure_archetypes)`. A higher score signifies a focused, uncontaminated alignment with a winning pattern and is expected to positively correlate with performance metrics like `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the 'Archetypal Purity Score' for each post.\n\n    The score measures how purely a post aligns with successful content patterns\n    while avoiding unsuccessful ones within its topic category.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame.\")\n\n    feature_name = 'archetypal_purity_score'\n    df[feature_name] = 0.0\n\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # Create an enriched text representation for more accurate embeddings\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' +\n        df['post_body'].fillna('') + ' ' +\n        df['morpheme_words'].fillna('')\n    ).str.strip()\n\n    # Create a map from the DataFrame's original index to integer positions (0, 1, 2...)\n    # This is crucial for correctly slicing the embedding tensor later.\n    index_to_pos = {idx: i for i, idx in enumerate(df.index)}\n\n    # Batch encode all texts for efficiency\n    all_embeddings = model.encode(\n        df['enriched_text'].tolist(),\n        convert_to_tensor=True,\n        show_progress_bar=False\n    )\n\n    # Process each topic category independently\n    for category, group_df in df.groupby('top_category_name'):\n        if len(group_df) < 5:  # Skip categories too small for meaningful analysis\n            continue\n\n        # Identify our posts within the category to define performance thresholds\n        ours_in_group = group_df[group_df['source'] == 'ours']\n        if ours_in_group.empty or ours_in_group['non_brand_inflow'].nunique() < 2:\n            continue\n\n        # Define high and low performance groups based on our data\n        low_perf_threshold = ours_in_group['non_brand_inflow'].quantile(0.3, interpolation='lower')\n        high_perf_threshold = ours_in_group['non_brand_inflow'].quantile(0.7, interpolation='higher')\n\n        high_perf_docs_indices = group_df[\n            (group_df['source'] == 'competitor') |\n            ((group_df['source'] == 'ours') & (group_df['non_brand_inflow'] >= high_perf_threshold))\n        ].index\n\n        low_perf_docs_indices = group_df[\n            (group_df['source'] == 'ours') & (group_df['non_brand_inflow'] <= low_perf_threshold)\n        ].index\n\n        # Get integer positions for tensor slicing\n        high_perf_positions = [index_to_pos[idx] for idx in high_perf_docs_indices]\n        low_perf_positions = [index_to_pos[idx] for idx in low_perf_docs_indices]\n\n        high_perf_embeddings = all_embeddings[high_perf_positions]\n        low_perf_embeddings = all_embeddings[low_perf_positions]\n\n        # Cluster high-performers to find 'Success Archetypes'\n        success_archetypes = None\n        if len(high_perf_embeddings) >= 3:\n            num_clusters = min(3, len(high_perf_embeddings))\n            kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init='auto')\n            kmeans.fit(high_perf_embeddings.cpu().numpy())\n            success_archetypes = torch.tensor(kmeans.cluster_centers_, device=all_embeddings.device)\n\n        # Cluster low-performers to find 'Failure Archetypes'\n        failure_archetypes = None\n        if len(low_perf_embeddings) >= 3:\n            num_clusters = min(3, len(low_perf_embeddings))\n            kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init='auto')\n            kmeans.fit(low_perf_embeddings.cpu().numpy())\n            failure_archetypes = torch.tensor(kmeans.cluster_centers_, device=all_embeddings.device)\n\n        # Get embeddings for all posts in the current category\n        current_group_indices = group_df.index\n        current_group_positions = [index_to_pos[idx] for idx in current_group_indices]\n        current_embeddings = all_embeddings[current_group_positions]\n\n        # Calculate max similarity to success archetypes\n        max_sim_success = pd.Series(0.0, index=current_group_indices)\n        if success_archetypes is not None:\n            sim_matrix = util.cos_sim(current_embeddings, success_archetypes)\n            max_sim_success = pd.Series(sim_matrix.max(dim=1).values.cpu().numpy(), index=current_group_indices)\n\n        # Calculate max similarity to failure archetypes\n        max_sim_failure = pd.Series(0.0, index=current_group_indices)\n        if failure_archetypes is not None:\n            sim_matrix = util.cos_sim(current_embeddings, failure_archetypes)\n            max_sim_failure = pd.Series(sim_matrix.max(dim=1).values.cpu().numpy(), index=current_group_indices)\n\n        # Calculate the final score and assign it safely using original indices\n        purity_score = max_sim_success * (1 - max_sim_failure.clip(0, 1))\n        df.loc[current_group_indices, feature_name] = purity_score\n\n    # Clean up and finalize\n    df = df.drop(columns=['enriched_text'])\n    df[feature_name] = df[feature_name].fillna(0.0)\n\n    return df\n"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.3580329390385438,
          "p_value": 0.00036757465982824186,
          "interpretation": "중간 정도의 양의 상관관계(0.3580)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0004), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.18258494768250386,
          "p_value": 0.07656435057228472,
          "interpretation": "약한 양의 상관관계(0.1826)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.0766), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "A post's performance is predicted by its strong, 'pure' alignment with a single, clear success archetype, while being free from the 'contamination' of any failure archetypes. The score is calculated as `max_similarity_to_success_archetypes * (1 - max_similarity_to_failure_archetypes)`. A higher score signifies a focused, uncontaminated alignment with a winning pattern and is expected to positively correlate with performance metrics like `non_brand_inflow`.",
      "feature_created": "archetypal_purity_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:21:06.391424",
    "iteration": 11,
    "initial_agent_response": {
      "feature_name": "dominant_archetype_adherence_score",
      "hypothesis": "Within each topic category ('top_category_name'), not all 'Success Archetypes' (clusters of high-performing content) are equally valuable. Some are more 'dominant' or 'central' based on the number of high-performing posts they contain. A post's performance is predicted by its ability to adhere strongly to one of these dominant success archetypes while simultaneously avoiding all identified 'Failure Archetypes'. This feature calculates a score as `(max similarity to a dominant success archetype) - (max similarity to any failure archetype)`. A higher score, indicating alignment with a popular, proven content strategy and avoidance of failure patterns, is expected to positively correlate with performance metrics like `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a powerful, multilingual model suitable for Korean text\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the dominant archetype adherence score for each post.\n    \"\"\"\n    # 0. Initial check for empty DataFrame\n    if df.empty:\n        df['dominant_archetype_adherence_score'] = pd.Series(dtype=np.float64)\n        return df\n\n    model = get_model()\n\n    # 1. Prepare Text Data and Feature Column\n    # Combine title, body, and morphemes for a richer semantic representation\n    df['enriched_text'] = df['post_title'].fillna('') + ' ' + \\\n                          df['post_body'].fillna('') + ' ' + \\\n                          df['morpheme_words'].fillna('')\n    \n    # Preserve the original index to ensure correct assignment later\n    original_index = df.index\n\n    # Initialize the feature column with a default value (NaN)\n    df['dominant_archetype_adherence_score'] = np.nan\n\n    # 2. Batch-encode all texts for efficiency\n    # This creates a tensor where the i-th row corresponds to the i-th row of the DataFrame\n    all_embeddings = model.encode(df['enriched_text'].tolist(), convert_to_tensor=True, show_progress_bar=False)\n\n    # 3. Group by category and perform analysis\n    for category, group in df.groupby('top_category_name'):\n        if len(group) < 10:  # Skip categories with too few posts for meaningful analysis\n            continue\n\n        # Define performance groups within the category\n        our_posts = group[group['source'] == 'ours']\n        if our_posts.empty or our_posts['non_brand_inflow'].isnull().all():\n            continue\n\n        # Define performance thresholds based on our posts\n        inflow = our_posts['non_brand_inflow'].dropna()\n        if len(inflow) < 5:\n            continue\n            \n        low_quantile = inflow.quantile(0.3)\n        high_quantile = inflow.quantile(0.7)\n\n        # Get indices for high and low performing groups\n        competitor_indices = group[group['source'] == 'competitor'].index\n        our_high_indices = our_posts[our_posts['non_brand_inflow'] >= high_quantile].index\n        our_low_indices = our_posts[our_posts['non_brand_inflow'] <= low_quantile].index\n\n        high_perf_indices = competitor_indices.union(our_high_indices)\n        low_perf_indices = our_low_indices\n\n        # Ensure we have enough data to form clusters\n        MIN_CLUSTER_SAMPLES = 3\n        if len(high_perf_indices) < MIN_CLUSTER_SAMPLES or len(low_perf_indices) < MIN_CLUSTER_SAMPLES:\n            continue\n            \n        # Get integer positions of these indices in the original DataFrame\n        high_perf_positions = np.where(original_index.isin(high_perf_indices))[0]\n        low_perf_positions = np.where(original_index.isin(low_perf_indices))[0]\n        group_positions = np.where(original_index.isin(group.index))[0]\n\n        # Select corresponding embeddings\n        high_perf_embeddings = all_embeddings[high_perf_positions]\n        low_perf_embeddings = all_embeddings[low_perf_positions]\n        group_embeddings = all_embeddings[group_positions]\n\n        # --- 4. Identify Success and Failure Archetypes via Clustering ---\n        k_success = min(MIN_CLUSTER_SAMPLES, len(high_perf_indices))\n        kmeans_success = KMeans(n_clusters=k_success, random_state=42, n_init='auto').fit(high_perf_embeddings.cpu().numpy())\n        success_centroids = util.tensor(kmeans_success.cluster_centers_)\n        \n        # Identify DOMINANT success archetypes\n        labels, counts = np.unique(kmeans_success.labels_, return_counts=True)\n        dominance_threshold = np.median(counts) # A cluster is dominant if its size is >= median size\n        dominant_cluster_indices = [i for i, count in enumerate(counts) if count >= dominance_threshold]\n        if not dominant_cluster_indices:\n            dominant_cluster_indices = [np.argmax(counts)] # Fallback to the largest cluster\n        dominant_success_centroids = success_centroids[dominant_cluster_indices]\n\n        k_failure = min(MIN_CLUSTER_SAMPLES, len(low_perf_indices))\n        kmeans_failure = KMeans(n_clusters=k_failure, random_state=42, n_init='auto').fit(low_perf_embeddings.cpu().numpy())\n        failure_centroids = util.tensor(kmeans_failure.cluster_centers_)\n\n        # --- 5. Calculate Scores for Each Post in the Category ---\n        # Calculate similarities to all archetype centroids in batch\n        sim_to_dominant_success = util.cos_sim(group_embeddings, dominant_success_centroids)\n        sim_to_failure = util.cos_sim(group_embeddings, failure_centroids)\n\n        # Find the max similarity to any success/failure archetype\n        max_sim_success, _ = sim_to_dominant_success.max(dim=1)\n        max_sim_failure, _ = sim_to_failure.max(dim=1)\n\n        # Calculate final contrast score\n        category_scores = (max_sim_success - max_sim_failure).cpu().numpy()\n\n        # Safely assign the calculated scores back to the main DataFrame\n        df.loc[group.index, 'dominant_archetype_adherence_score'] = category_scores\n\n    # Clean up the temporary column\n    df.drop(columns=['enriched_text'], inplace=True)\n    \n    # Fill any remaining NaNs (for skipped categories) with a neutral value like 0\n    df['dominant_archetype_adherence_score'].fillna(0, inplace=True)\n\n    return df"
    },
    "correction_attempts_made": 1,
    "correction_history": [
      {
        "attempt": 1,
        "response_raw": "```json\n{\n  \"feature_name\": \"dominant_archetype_adherence_score\",\n  \"hypothesis\": \"Within each topic category ('top_category_name'), not all 'Success Archetypes' (clusters of high-performing content) are equally valuable. Some are more 'dominant' or 'central' based on the number of high-performing posts they contain. A post's performance is predicted by its ability to adhere strongly to one of these dominant success archetypes while simultaneously avoiding all identified 'Failure Archetypes'. This feature calculates a score as `(max similarity to a dominant success archetype) - (max similarity to any failure archetype)`. A higher score, indicating alignment with a popular, proven content strategy and avoidance of failure patterns, is expected to positively correlate with performance metrics like `non_brand_inflow`.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nfrom sentence_transformers import SentenceTransformer, util\\nfrom sklearn.cluster import KMeans\\nimport torch\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    global _model\\n    if _model is None:\\n        # Using a powerful, multilingual model suitable for Korean text\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates the dominant archetype adherence score for each post.\\n    \\\"\\\"\\\"\\n    # 0. Initial check for empty DataFrame\\n    if not df.empty:\\n        pass\\n    else:\\n        df['dominant_archetype_adherence_score'] = pd.Series(dtype=np.float64)\\n        return df\\n\\n    model = get_model()\\n\\n    # 1. Prepare Text Data and Feature Column\\n    # Combine title, body, and morphemes for a richer semantic representation\\n    df['enriched_text'] = df['post_title'].fillna('') + ' ' + \\\\\\n                          df['post_body'].fillna('') + ' ' + \\\\\\n                          df['morpheme_words'].fillna('')\\n    \\n    # Preserve the original index to ensure correct assignment later\\n    original_index = df.index\\n\\n    # Initialize the feature column with a default value (NaN)\\n    df['dominant_archetype_adherence_score'] = np.nan\\n\\n    # 2. Batch-encode all texts for efficiency\\n    # This creates a tensor where the i-th row corresponds to the i-th row of the DataFrame\\n    all_embeddings = model.encode(df['enriched_text'].tolist(), convert_to_tensor=True, show_progress_bar=False)\\n\\n    # 3. Group by category and perform analysis\\n    for category, group in df.groupby('top_category_name'):\\n        if len(group) < 10:  # Skip categories with too few posts for meaningful analysis\\n            continue\\n\\n        # Define performance groups within the category\\n        our_posts = group[group['source'] == 'ours']\\n        if our_posts.empty or our_posts['non_brand_inflow'].isnull().all():\\n            continue\\n\\n        # Define performance thresholds based on our posts\\n        inflow = our_posts['non_brand_inflow'].dropna()\\n        if len(inflow) < 5:\\n            continue\\n            \\n        low_quantile = inflow.quantile(0.3)\\n        high_quantile = inflow.quantile(0.7)\\n\\n        # Get indices for high and low performing groups\\n        competitor_indices = group[group['source'] == 'competitor'].index\\n        our_high_indices = our_posts[our_posts['non_brand_inflow'] >= high_quantile].index\\n        our_low_indices = our_posts[our_posts['non_brand_inflow'] <= low_quantile].index\\n\\n        high_perf_indices = competitor_indices.union(our_high_indices)\\n        low_perf_indices = our_low_indices\\n\\n        # Ensure we have enough data to form clusters\\n        MIN_CLUSTER_SAMPLES = 3\\n        if len(high_perf_indices) < MIN_CLUSTER_SAMPLES or len(low_perf_indices) < MIN_CLUSTER_SAMPLES:\\n            continue\\n            \\n        # Get integer positions of these indices in the original DataFrame\\n        high_perf_positions = np.where(original_index.isin(high_perf_indices))[0]\\n        low_perf_positions = np.where(original_index.isin(low_perf_indices))[0]\\n        group_positions = np.where(original_index.isin(group.index))[0]\\n\\n        # Select corresponding embeddings\\n        high_perf_embeddings = all_embeddings[high_perf_positions]\\n        low_perf_embeddings = all_embeddings[low_perf_positions]\\n        group_embeddings = all_embeddings[group_positions]\\n\\n        # --- 4. Identify Success and Failure Archetypes via Clustering ---\\n        k_success = min(MIN_CLUSTER_SAMPLES, len(high_perf_indices))\\n        kmeans_success = KMeans(n_clusters=k_success, random_state=42, n_init='auto').fit(high_perf_embeddings.cpu().numpy())\\n        # FIX: Use torch.tensor instead of the non-existent util.tensor\\n        # Ensure tensor is on the same device (CPU/GPU) as embeddings\\n        success_centroids = torch.tensor(kmeans_success.cluster_centers_, device=all_embeddings.device)\\n        \\n        # Identify DOMINANT success archetypes\\n        labels, counts = np.unique(kmeans_success.labels_, return_counts=True)\\n        dominance_threshold = np.median(counts) # A cluster is dominant if its size is >= median size\\n        dominant_cluster_indices = [i for i, count in enumerate(counts) if count >= dominance_threshold]\\n        if not dominant_cluster_indices:\\n            dominant_cluster_indices = [np.argmax(counts)] # Fallback to the largest cluster\\n        dominant_success_centroids = success_centroids[dominant_cluster_indices]\\n\\n        k_failure = min(MIN_CLUSTER_SAMPLES, len(low_perf_indices))\\n        kmeans_failure = KMeans(n_clusters=k_failure, random_state=42, n_init='auto').fit(low_perf_embeddings.cpu().numpy())\\n        # FIX: Use torch.tensor instead of the non-existent util.tensor\\n        failure_centroids = torch.tensor(kmeans_failure.cluster_centers_, device=all_embeddings.device)\\n\\n        # --- 5. Calculate Scores for Each Post in the Category ---\\n        # Calculate similarities to all archetype centroids in batch\\n        sim_to_dominant_success = util.cos_sim(group_embeddings, dominant_success_centroids)\\n        sim_to_failure = util.cos_sim(group_embeddings, failure_centroids)\\n\\n        # Find the max similarity to any success/failure archetype\\n        max_sim_success, _ = sim_to_dominant_success.max(dim=1)\\n        max_sim_failure, _ = sim_to_failure.max(dim=1)\\n\\n        # Calculate final contrast score\\n        category_scores = (max_sim_success - max_sim_failure).cpu().numpy()\\n\\n        # Safely assign the calculated scores back to the main DataFrame\\n        df.loc[group.index, 'dominant_archetype_adherence_score'] = category_scores\\n\\n    # Clean up the temporary column\\n    if 'enriched_text' in df.columns:\\n        df.drop(columns=['enriched_text'], inplace=True)\\n    \\n    # Fill any remaining NaNs (for skipped categories) with a neutral value like 0\\n    df['dominant_archetype_adherence_score'].fillna(0, inplace=True)\\n\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "dominant_archetype_adherence_score",
          "hypothesis": "Within each topic category ('top_category_name'), not all 'Success Archetypes' (clusters of high-performing content) are equally valuable. Some are more 'dominant' or 'central' based on the number of high-performing posts they contain. A post's performance is predicted by its ability to adhere strongly to one of these dominant success archetypes while simultaneously avoiding all identified 'Failure Archetypes'. This feature calculates a score as `(max similarity to a dominant success archetype) - (max similarity to any failure archetype)`. A higher score, indicating alignment with a popular, proven content strategy and avoidance of failure patterns, is expected to positively correlate with performance metrics like `non_brand_inflow`.",
          "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a powerful, multilingual model suitable for Korean text\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the dominant archetype adherence score for each post.\n    \"\"\"\n    # 0. Initial check for empty DataFrame\n    if not df.empty:\n        pass\n    else:\n        df['dominant_archetype_adherence_score'] = pd.Series(dtype=np.float64)\n        return df\n\n    model = get_model()\n\n    # 1. Prepare Text Data and Feature Column\n    # Combine title, body, and morphemes for a richer semantic representation\n    df['enriched_text'] = df['post_title'].fillna('') + ' ' + \\\n                          df['post_body'].fillna('') + ' ' + \\\n                          df['morpheme_words'].fillna('')\n    \n    # Preserve the original index to ensure correct assignment later\n    original_index = df.index\n\n    # Initialize the feature column with a default value (NaN)\n    df['dominant_archetype_adherence_score'] = np.nan\n\n    # 2. Batch-encode all texts for efficiency\n    # This creates a tensor where the i-th row corresponds to the i-th row of the DataFrame\n    all_embeddings = model.encode(df['enriched_text'].tolist(), convert_to_tensor=True, show_progress_bar=False)\n\n    # 3. Group by category and perform analysis\n    for category, group in df.groupby('top_category_name'):\n        if len(group) < 10:  # Skip categories with too few posts for meaningful analysis\n            continue\n\n        # Define performance groups within the category\n        our_posts = group[group['source'] == 'ours']\n        if our_posts.empty or our_posts['non_brand_inflow'].isnull().all():\n            continue\n\n        # Define performance thresholds based on our posts\n        inflow = our_posts['non_brand_inflow'].dropna()\n        if len(inflow) < 5:\n            continue\n            \n        low_quantile = inflow.quantile(0.3)\n        high_quantile = inflow.quantile(0.7)\n\n        # Get indices for high and low performing groups\n        competitor_indices = group[group['source'] == 'competitor'].index\n        our_high_indices = our_posts[our_posts['non_brand_inflow'] >= high_quantile].index\n        our_low_indices = our_posts[our_posts['non_brand_inflow'] <= low_quantile].index\n\n        high_perf_indices = competitor_indices.union(our_high_indices)\n        low_perf_indices = our_low_indices\n\n        # Ensure we have enough data to form clusters\n        MIN_CLUSTER_SAMPLES = 3\n        if len(high_perf_indices) < MIN_CLUSTER_SAMPLES or len(low_perf_indices) < MIN_CLUSTER_SAMPLES:\n            continue\n            \n        # Get integer positions of these indices in the original DataFrame\n        high_perf_positions = np.where(original_index.isin(high_perf_indices))[0]\n        low_perf_positions = np.where(original_index.isin(low_perf_indices))[0]\n        group_positions = np.where(original_index.isin(group.index))[0]\n\n        # Select corresponding embeddings\n        high_perf_embeddings = all_embeddings[high_perf_positions]\n        low_perf_embeddings = all_embeddings[low_perf_positions]\n        group_embeddings = all_embeddings[group_positions]\n\n        # --- 4. Identify Success and Failure Archetypes via Clustering ---\n        k_success = min(MIN_CLUSTER_SAMPLES, len(high_perf_indices))\n        kmeans_success = KMeans(n_clusters=k_success, random_state=42, n_init='auto').fit(high_perf_embeddings.cpu().numpy())\n        # FIX: Use torch.tensor instead of the non-existent util.tensor\n        # Ensure tensor is on the same device (CPU/GPU) as embeddings\n        success_centroids = torch.tensor(kmeans_success.cluster_centers_, device=all_embeddings.device)\n        \n        # Identify DOMINANT success archetypes\n        labels, counts = np.unique(kmeans_success.labels_, return_counts=True)\n        dominance_threshold = np.median(counts) # A cluster is dominant if its size is >= median size\n        dominant_cluster_indices = [i for i, count in enumerate(counts) if count >= dominance_threshold]\n        if not dominant_cluster_indices:\n            dominant_cluster_indices = [np.argmax(counts)] # Fallback to the largest cluster\n        dominant_success_centroids = success_centroids[dominant_cluster_indices]\n\n        k_failure = min(MIN_CLUSTER_SAMPLES, len(low_perf_indices))\n        kmeans_failure = KMeans(n_clusters=k_failure, random_state=42, n_init='auto').fit(low_perf_embeddings.cpu().numpy())\n        # FIX: Use torch.tensor instead of the non-existent util.tensor\n        failure_centroids = torch.tensor(kmeans_failure.cluster_centers_, device=all_embeddings.device)\n\n        # --- 5. Calculate Scores for Each Post in the Category ---\n        # Calculate similarities to all archetype centroids in batch\n        sim_to_dominant_success = util.cos_sim(group_embeddings, dominant_success_centroids)\n        sim_to_failure = util.cos_sim(group_embeddings, failure_centroids)\n\n        # Find the max similarity to any success/failure archetype\n        max_sim_success, _ = sim_to_dominant_success.max(dim=1)\n        max_sim_failure, _ = sim_to_failure.max(dim=1)\n\n        # Calculate final contrast score\n        category_scores = (max_sim_success - max_sim_failure).cpu().numpy()\n\n        # Safely assign the calculated scores back to the main DataFrame\n        df.loc[group.index, 'dominant_archetype_adherence_score'] = category_scores\n\n    # Clean up the temporary column\n    if 'enriched_text' in df.columns:\n        df.drop(columns=['enriched_text'], inplace=True)\n    \n    # Fill any remaining NaNs (for skipped categories) with a neutral value like 0\n    df['dominant_archetype_adherence_score'].fillna(0, inplace=True)\n\n    return df"
        }
      }
    ],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.3420276110301501,
          "p_value": 0.0006926739081232632,
          "interpretation": "중간 정도의 양의 상관관계(0.3420)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0007), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.11042405211980323,
          "p_value": 0.28674279333712677,
          "interpretation": "약한 양의 상관관계(0.1104)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.2867), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic category ('top_category_name'), not all 'Success Archetypes' (clusters of high-performing content) are equally valuable. Some are more 'dominant' or 'central' based on the number of high-performing posts they contain. A post's performance is predicted by its ability to adhere strongly to one of these dominant success archetypes while simultaneously avoiding all identified 'Failure Archetypes'. This feature calculates a score as `(max similarity to a dominant success archetype) - (max similarity to any failure archetype)`. A higher score, indicating alignment with a popular, proven content strategy and avoidance of failure patterns, is expected to positively correlate with performance metrics like `non_brand_inflow`.",
      "feature_created": "dominant_archetype_adherence_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:22:38.199464",
    "iteration": 12,
    "initial_agent_response": {
      "feature_name": "semantic_trajectory_score",
      "hypothesis": "Within a topic category (`top_category_name`), there exists an 'ideal semantic trajectory' pointing from common failure patterns to successful content patterns. This trajectory is defined by the vector from a post's nearest 'Failure Archetype' (cluster of low-performing content) to its nearest 'Success Archetype' (cluster of high-performing content). A post's performance is predicted by how well its own semantic position aligns with this ideal trajectory. A higher score, indicating the post is semantically positioned 'on the path to success', is expected to positively correlate with performance metrics like `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a model that is effective for Korean and general semantic tasks\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the Semantic Trajectory Score for each post.\n\n    This score measures how well a post's semantic position aligns with the ideal\n    trajectory from failure patterns to success patterns within its topic category.\n    \"\"\"\n    feature_name = 'semantic_trajectory_score'\n    df[feature_name] = np.nan\n\n    if df.empty:\n        return df\n\n    # Ensure required columns are present\n    required_cols = ['post_title', 'post_body', 'morpheme_words', 'source', 'non_brand_inflow', 'top_category_name']\n    if not all(col in df.columns for col in required_cols):\n        # If essential columns are missing, return the DataFrame as is.\n        return df\n\n    model = get_model()\n\n    # Create an enriched text representation for more accurate semantic analysis\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).str.strip()\n    \n    # Preserve original index for correct assignment later\n    original_index = df.index\n\n    # Create a mapping from DataFrame index to tensor position (0, 1, 2, ...)\n    idx_to_tensor_pos = {label: i for i, label in enumerate(original_index)}\n\n    # 2. Use efficient batch processing for all posts at once\n    all_embeddings = model.encode(\n        df['enriched_text'].tolist(), \n        convert_to_tensor=True, \n        show_progress_bar=False\n    )\n\n    # Group by the main topic category\n    grouped = df.groupby('top_category_name')\n\n    for category, group_df in grouped:\n        if len(group_df) < 10: # Skip categories with too few posts for meaningful analysis\n            continue\n\n        our_posts = group_df[group_df['source'] == 'ours']\n        competitor_posts = group_df[group_df['source'] == 'competitor']\n\n        if our_posts.empty or competitor_posts.empty:\n            continue\n\n        # Define High and Low performing groups based on the hypothesis\n        inflow_quantile_30 = our_posts['non_brand_inflow'].quantile(0.30, interpolation='lower')\n        inflow_quantile_70 = our_posts['non_brand_inflow'].quantile(0.70, interpolation='higher')\n\n        low_perf_mask = (our_posts['non_brand_inflow'] <= inflow_quantile_30)\n        high_perf_ours_mask = (our_posts['non_brand_inflow'] >= inflow_quantile_70)\n\n        low_perf_indices = our_posts[low_perf_mask].index\n        high_perf_indices = competitor_posts.index.union(our_posts[high_perf_ours_mask].index)\n\n        # Map DataFrame indices to tensor positions\n        try:\n            low_perf_tensor_pos = [idx_to_tensor_pos[i] for i in low_perf_indices]\n            high_perf_tensor_pos = [idx_to_tensor_pos[i] for i in high_perf_indices]\n        except KeyError:\n            # This can happen if there's an index mismatch, though unlikely with this setup.\n            continue\n\n        if not low_perf_tensor_pos or not high_perf_tensor_pos:\n            continue\n\n        low_embeddings = all_embeddings[low_perf_tensor_pos]\n        high_embeddings = all_embeddings[high_perf_tensor_pos]\n\n        # Determine a dynamic number of clusters (archetypes)\n        n_clusters_low = max(1, min(5, len(low_embeddings) // 3))\n        n_clusters_high = max(1, min(5, len(high_embeddings) // 3))\n\n        if len(low_embeddings) < n_clusters_low or len(high_embeddings) < n_clusters_high:\n            continue\n\n        # Cluster to find Failure and Success Archetypes\n        kmeans_low = KMeans(n_clusters=n_clusters_low, random_state=42, n_init='auto').fit(low_embeddings.cpu().numpy())\n        F_centroids = torch.from_numpy(kmeans_low.cluster_centers_).to(all_embeddings.device)\n\n        kmeans_high = KMeans(n_clusters=n_clusters_high, random_state=42, n_init='auto').fit(high_embeddings.cpu().numpy())\n        S_centroids = torch.from_numpy(kmeans_high.cluster_centers_).to(all_embeddings.device)\n\n        # Get embeddings for all posts in the current category\n        group_tensor_pos = [idx_to_tensor_pos[i] for i in group_df.index]\n        group_embeddings = all_embeddings[group_tensor_pos]\n        C_all = group_embeddings.mean(dim=0)\n\n        # Calculate score for each post in the category\n        for i, post_index in enumerate(group_df.index):\n            e_p = group_embeddings[i]\n\n            # Find nearest failure and success archetypes\n            sims_to_F = util.cos_sim(e_p, F_centroids)[0]\n            closest_F_idx = torch.argmax(sims_to_F)\n            F_closest = F_centroids[closest_F_idx]\n\n            sims_to_S = util.cos_sim(e_p, S_centroids)[0]\n            closest_S_idx = torch.argmax(sims_to_S)\n            S_closest = S_centroids[closest_S_idx]\n\n            # Define the ideal trajectory and the post's position vectors\n            V_traj = S_closest - F_closest\n            V_pos = e_p - C_all\n\n            # Calculate the score as the cosine similarity (projection)\n            # Add a small epsilon to avoid division by zero for zero-norm vectors\n            score = util.cos_sim(V_pos, V_traj)[0][0].item()\n            \n            # Assign score back to the original DataFrame using its preserved index\n            df.loc[post_index, feature_name] = score\n\n    # Fill any remaining NaNs (e.g., from small categories) with a neutral value\n    df[feature_name].fillna(0.0, inplace=True)\n    \n    # Clean up temporary column\n    df.drop(columns=['enriched_text'], inplace=True)\n\n    # 3. Return the full DataFrame with the new column.\n    return df"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.36381045338358636,
          "p_value": 0.00029001211557318376,
          "interpretation": "중간 정도의 양의 상관관계(0.3638)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0003), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.1959486721036451,
          "p_value": 0.057029993250693814,
          "interpretation": "약한 양의 상관관계(0.1959)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.0570), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within a topic category (`top_category_name`), there exists an 'ideal semantic trajectory' pointing from common failure patterns to successful content patterns. This trajectory is defined by the vector from a post's nearest 'Failure Archetype' (cluster of low-performing content) to its nearest 'Success Archetype' (cluster of high-performing content). A post's performance is predicted by how well its own semantic position aligns with this ideal trajectory. A higher score, indicating the post is semantically positioned 'on the path to success', is expected to positively correlate with performance metrics like `non_brand_inflow`.",
      "feature_created": "semantic_trajectory_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:25:11.885453",
    "iteration": 13,
    "initial_agent_response": {
      "feature_name": "semantic_focus_score",
      "hypothesis": "High-performing content is not only aligned with a success archetype but is also semantically *focused*. Low-performing content is often semantically scattered, showing moderate similarity to multiple, often conflicting, archetypes. This feature quantifies this 'semantic focus' by calculating the ratio of a post's similarity to its single best-fit success archetype versus the sum of its similarities to all other archetypes (both success and failure). A high score, indicating a clear and unambiguous alignment with a single winning pattern, is expected to positively correlate with performance metrics like `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for various content types.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic_focus_score for each post.\n\n    This score measures how focused a post's content is on a single 'success archetype'\n    relative to its similarity to all other identified archetypes (both success and failure).\n    \"\"\"\n    # 0. Handle empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        return df\n\n    feature_name = 'semantic_focus_score'\n    df[feature_name] = np.nan\n\n    # 1. Prepare data and embeddings\n    try:\n        model = get_model()\n        \n        # Create an enriched text representation for more robust semantic analysis\n        df['enriched_text'] = (\n            df['post_title'].fillna('').astype(str) + ' ' + \n            df['post_body'].fillna('').astype(str) + ' ' + \n            df['morpheme_words'].fillna('').astype(str)\n        ).str.strip()\n\n        # Ensure performance metric is numeric for quantile calculations\n        df['non_brand_inflow_numeric'] = pd.to_numeric(df['non_brand_inflow'], errors='coerce')\n\n        # 2. Generate embeddings for all posts in a single batch for efficiency\n        texts = df['enriched_text'].tolist()\n        all_embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n\n    except Exception as e:\n        # If model loading or embedding fails, return the original DataFrame with the empty feature column\n        print(f\"Error during model loading or embedding: {e}\")\n        if 'non_brand_inflow_numeric' in df.columns:\n            df = df.drop(columns=['non_brand_inflow_numeric'])\n        if 'enriched_text' in df.columns:\n            df = df.drop(columns=['enriched_text'])\n        return df\n\n    # 3. Process each topic category separately\n    for category, group in df.groupby('top_category_name'):\n        if len(group) < 5:  # Skip categories with too few posts for meaningful analysis\n            continue\n\n        # Get original integer positions of group members for slicing the tensor\n        group_indices = df.index.get_indexer(group.index)\n        group_embeddings = all_embeddings[group_indices]\n\n        # Define high and low-performing posts within the category\n        ours_posts = group[group['source'] == 'ours']\n        competitor_posts = group[group['source'] == 'competitor']\n\n        # Define performance thresholds for 'ours' posts\n        if not ours_posts.empty and ours_posts['non_brand_inflow_numeric'].notna().sum() > 1:\n            low_quantile = ours_posts['non_brand_inflow_numeric'].quantile(0.3, interpolation='lower')\n            high_quantile = ours_posts['non_brand_inflow_numeric'].quantile(0.7, interpolation='higher')\n            \n            ours_low_idx = ours_posts[ours_posts['non_brand_inflow_numeric'] <= low_quantile].index\n            ours_high_idx = ours_posts[ours_posts['non_brand_inflow_numeric'] >= high_quantile].index\n        else:\n            ours_low_idx = pd.Index([])\n            ours_high_idx = pd.Index([])\n\n        success_group_idx = competitor_posts.index.union(ours_high_idx)\n        failure_group_idx = ours_low_idx\n\n        # 4. Identify archetypes (clusters) for success and failure groups\n        try:\n            # Get integer positions for slicing the main embedding tensor\n            success_indices = df.index.get_indexer(success_group_idx)\n            failure_indices = df.index.get_indexer(failure_group_idx)\n\n            # Filter out potential -1s from get_indexer if an index wasn't found\n            success_indices = success_indices[success_indices != -1]\n            failure_indices = failure_indices[failure_indices != -1]\n\n            # Ensure there are enough posts to form clusters\n            min_success_clusters = 3\n            min_failure_clusters = 2\n\n            if len(success_indices) < min_success_clusters or len(failure_indices) < min_failure_clusters:\n                continue\n\n            success_embeddings = all_embeddings[success_indices]\n            failure_embeddings = all_embeddings[failure_indices]\n\n            # Determine the optimal number of clusters (archetypes)\n            num_success_archetypes = min(min_success_clusters, len(np.unique(success_embeddings, axis=0)))\n            num_failure_archetypes = min(min_failure_clusters, len(np.unique(failure_embeddings, axis=0)))\n            \n            if num_success_archetypes == 0 or num_failure_archetypes == 0:\n                continue\n\n            # Find centroids of success and failure archetypes\n            kmeans_success = KMeans(n_clusters=num_success_archetypes, random_state=42, n_init='auto').fit(success_embeddings.cpu().numpy())\n            success_centroids = torch.tensor(kmeans_success.cluster_centers_, device=model.device)\n\n            kmeans_failure = KMeans(n_clusters=num_failure_archetypes, random_state=42, n_init='auto').fit(failure_embeddings.cpu().numpy())\n            failure_centroids = torch.tensor(kmeans_failure.cluster_centers_, device=model.device)\n\n            # 5. Calculate the semantic focus score for each post in the group\n            sim_to_success = util.cos_sim(group_embeddings, success_centroids)\n            sim_to_failure = util.cos_sim(group_embeddings, failure_centroids)\n\n            # Combine all similarities into one tensor for total sum calculation\n            all_sims = torch.cat((sim_to_success, sim_to_failure), dim=1)\n            # Clamp negative similarities to 0 as they are artifacts of the model\n            all_sims[all_sims < 0] = 0\n            sim_to_success[sim_to_success < 0] = 0\n\n            # Signal: The similarity to the single best-fit success archetype\n            max_success_sim = torch.max(sim_to_success, dim=1).values\n            \n            # Noise: The sum of similarities to all other archetypes\n            total_sum_of_sims = torch.sum(all_sims, dim=1)\n            sum_of_other_sims = total_sum_of_sims - max_success_sim\n\n            # Score: Ratio of signal to noise. Add epsilon for numerical stability.\n            focus_score = max_success_sim / (sum_of_other_sims + 1e-9)\n\n            df.loc[group.index, feature_name] = focus_score.cpu().numpy()\n\n        except Exception as e:\n            # print(f\"Skipping category '{category}' due to error: {e}\")\n            continue\n            \n    # 6. Clean up and return\n    df = df.drop(columns=['enriched_text', 'non_brand_inflow_numeric'])\n    \n    # Fill scores for posts in skipped categories or where calculation failed with the median of calculated scores\n    median_score = df[feature_name].median()\n    if pd.notna(median_score):\n        df[feature_name].fillna(median_score, inplace=True)\n    else:\n        df[feature_name].fillna(0, inplace=True) # Fallback if no scores were calculated\n\n    return df\n"
    },
    "correction_attempts_made": 1,
    "correction_history": [
      {
        "attempt": 1,
        "response_raw": "An analysis of the previous attempt reveals that the feature generation process failed, resulting in a column with only a single unique value. This indicates that the core calculation logic inside the `try...except` block was likely skipped for every group due to overly strict conditions, causing the final `fillna()` step to assign a default value (e.g., 0) to all rows.\n\nThe primary points of failure were:\n1.  **Overly Complex Archetypes**: The code attempted to create multiple \"success\" and \"failure\" archetypes using KMeans clustering within each `top_category_name`.\n2.  **Brittle Preconditions**: The process required each category to have a sufficient number of high-performing, low-performing, and competitor posts simultaneously. It is highly probable that no single category met all these criteria, leading to the calculation being skipped entirely.\n3.  **Logical Contradiction**: The complex \"signal-to-noise\" ratio was unnecessary. A simpler, more direct measure of alignment with a success model is more robust and less prone to calculation errors.\n\nThe corrected code implements a more robust and direct strategy:\n1.  **Simplified Success Archetype**: Instead of complex clustering, it now calculates a single \"ideal success centroid\" for each category. This centroid is the semantic average of all high-performing content in that category (all competitor posts plus the top-performing 'ours' posts).\n2.  **Robust Preconditions**: The logic now only requires a small number of success posts (a minimum of 3) to create a stable centroid, a condition far more likely to be met.\n3.  **Direct Similarity Score**: The feature is now a direct cosine similarity score between each post and its category's success centroid. This directly measures alignment with the \"winning pattern\" and is guaranteed to produce a range of values as long as at least one category can be processed.\n\nThis revised approach directly resolves the zero-variance error by ensuring calculations can proceed under realistic data conditions, while still fulfilling the original strategic goal of identifying content that aligns with proven success patterns.\n\n```json\n{\n  \"feature_name\": \"semantic_focus_score\",\n  \"hypothesis\": \"High-performing content is semantically aligned with an 'ideal success archetype' for its specific topic. This archetype represents the central semantic theme of proven, high-performing content (i.e., all competitor posts and top-tier internal posts). This feature measures the cosine similarity of each post to its category's unique success archetype. A higher score signifies stronger alignment with the winning semantic pattern and is expected to positively correlate with performance metrics like `non_brand_inflow`.\",\n  \"python_code\": \"import pandas as pd\\nimport numpy as np\\nfrom sentence_transformers import SentenceTransformer, util\\nimport torch\\n\\n# 1. Lazy-load the model to avoid re-initializing it on every call.\\n_model = None\\n\\ndef get_model():\\n    \\\"\\\"\\\"Initializes and returns a singleton SentenceTransformer model.\\\"\\\"\\\"\\n    global _model\\n    if _model is None:\\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\\n    return _model\\n\\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates a score indicating how semantically aligned a post is with the\\n    'ideal success archetype' for its category.\\n    \\\"\\\"\\\"\\n    # 0. Handle empty DataFrame and preserve original index\\n    if not isinstance(df, pd.DataFrame) or df.empty:\\n        return df\\n    \\n    # Preserve the original index to prevent misalignment issues.\\n    original_index = df.index\\n    df_copy = df.copy()\\n\\n    feature_name = 'semantic_focus_score'\\n    df_copy[feature_name] = np.nan\\n\\n    # 1. Prepare data and generate embeddings for all posts at once\\n    try:\\n        model = get_model()\\n        \\n        # Create an enriched text representation for more robust semantic analysis\\n        df_copy['enriched_text'] = (\\n            df_copy['post_title'].fillna('').astype(str) + ' ' + \\n            df_copy['post_body'].fillna('').astype(str) + ' ' + \\n            df_copy['morpheme_words'].fillna('').astype(str)\\n        ).str.strip()\\n\\n        # Ensure performance metric is numeric for quantile calculations\\n        df_copy['non_brand_inflow_numeric'] = pd.to_numeric(df_copy['non_brand_inflow'], errors='coerce')\\n\\n        texts = df_copy['enriched_text'].tolist()\\n        all_embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=False)\\n\\n    except Exception as e:\\n        # If initial setup fails, return the original DataFrame with an empty feature column\\n        df[feature_name] = 0\\n        return df\\n\\n    # 2. Process each topic category to calculate its unique success archetype\\n    # Use a column that exists in the schema provided.\\n    grouping_col = 'top_category_name'\\n    if grouping_col not in df_copy.columns:\\n        df[feature_name] = 0\\n        return df\\n\\n    for category, group in df_copy.groupby(grouping_col):\\n        # Ensure the group is large enough for meaningful analysis\\n        if len(group) < 3:\\n            continue\\n\\n        # Identify all high-performing ('success') posts in the category\\n        competitor_posts = group[group['source'] == 'competitor']\\n        ours_posts = group[group['source'] == 'ours']\\n        \\n        # High-performing 'ours' are those in the top 30% of inflow\\n        ours_high_idx = pd.Index([])\\n        if not ours_posts.empty and ours_posts['non_brand_inflow_numeric'].notna().sum() > 1:\\n            high_quantile = ours_posts['non_brand_inflow_numeric'].quantile(0.7, interpolation='higher')\\n            # Include posts at or above the high quantile\\n            ours_high_posts = ours_posts[ours_posts['non_brand_inflow_numeric'] >= high_quantile]\\n            if not ours_high_posts.empty:\\n                ours_high_idx = ours_high_posts.index\\n\\n        # The success group comprises all competitors and our best posts\\n        success_group_idx = competitor_posts.index.union(ours_high_idx)\\n\\n        # We need at least 3 posts to create a stable centroid\\n        if len(success_group_idx) < 3:\\n            continue\\n\\n        try:\\n            # Get integer positions to slice the main embeddings tensor\\n            # This is crucial for performance and correctness\\n            success_indices = df_copy.index.get_indexer_for(success_group_idx)\\n            group_member_indices = df_copy.index.get_indexer_for(group.index)\\n\\n            # Filter out -1s which indicate an index was not found\\n            success_indices = success_indices[success_indices != -1]\\n            if len(success_indices) < 3:\\n                continue\\n\\n            # Extract embeddings for the success group and the entire category group\\n            success_embeddings = all_embeddings[success_indices]\\n            group_embeddings = all_embeddings[group_member_indices]\\n\\n            # 3. Calculate the single 'Success Centroid' (archetype)\\n            # This is the semantic average of all successful posts in the category.\\n            success_centroid = torch.mean(success_embeddings, dim=0, keepdim=True)\\n\\n            # 4. Calculate similarity of each post in the group to the success centroid\\n            similarities = util.cos_sim(group_embeddings, success_centroid)\\n            \\n            # Squeeze to remove the last dimension and convert to a flat numpy array\\n            scores = similarities.squeeze().cpu().numpy()\\n\\n            # Assign the calculated scores back to the DataFrame using original indices\\n            df_copy.loc[group.index, feature_name] = scores\\n\\n        except Exception as e:\\n            # If an error occurs for a specific group, skip it and continue\\n            continue\\n\\n    # 5. Clean up temporary columns and handle missing values\\n    df_copy = df_copy.drop(columns=['enriched_text', 'non_brand_inflow_numeric'])\\n    \\n    # For posts in categories that were skipped, fill with the median of calculated scores\\n    # This is a reasonable imputation for posts where a score could not be derived.\\n    median_score = df_copy[feature_name].median()\\n    if pd.notna(median_score):\\n        df_copy[feature_name].fillna(median_score, inplace=True)\\n    else:\\n        # Fallback if no scores were calculated at all (e.g., very small dataset)\\n        df_copy[feature_name].fillna(0, inplace=True)\\n\\n    # Return the new feature column attached to the original DataFrame structure\\n    df[feature_name] = df_copy[feature_name]\\n    return df\"\n}\n```",
        "response_json": {
          "feature_name": "semantic_focus_score",
          "hypothesis": "High-performing content is semantically aligned with an 'ideal success archetype' for its specific topic. This archetype represents the central semantic theme of proven, high-performing content (i.e., all competitor posts and top-tier internal posts). This feature measures the cosine similarity of each post to its category's unique success archetype. A higher score signifies stronger alignment with the winning semantic pattern and is expected to positively correlate with performance metrics like `non_brand_inflow`.",
          "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score indicating how semantically aligned a post is with the\n    'ideal success archetype' for its category.\n    \"\"\"\n    # 0. Handle empty DataFrame and preserve original index\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        return df\n    \n    # Preserve the original index to prevent misalignment issues.\n    original_index = df.index\n    df_copy = df.copy()\n\n    feature_name = 'semantic_focus_score'\n    df_copy[feature_name] = np.nan\n\n    # 1. Prepare data and generate embeddings for all posts at once\n    try:\n        model = get_model()\n        \n        # Create an enriched text representation for more robust semantic analysis\n        df_copy['enriched_text'] = (\n            df_copy['post_title'].fillna('').astype(str) + ' ' + \n            df_copy['post_body'].fillna('').astype(str) + ' ' + \n            df_copy['morpheme_words'].fillna('').astype(str)\n        ).str.strip()\n\n        # Ensure performance metric is numeric for quantile calculations\n        df_copy['non_brand_inflow_numeric'] = pd.to_numeric(df_copy['non_brand_inflow'], errors='coerce')\n\n        texts = df_copy['enriched_text'].tolist()\n        all_embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n\n    except Exception as e:\n        # If initial setup fails, return the original DataFrame with an empty feature column\n        df[feature_name] = 0\n        return df\n\n    # 2. Process each topic category to calculate its unique success archetype\n    # Use a column that exists in the schema provided.\n    grouping_col = 'top_category_name'\n    if grouping_col not in df_copy.columns:\n        df[feature_name] = 0\n        return df\n\n    for category, group in df_copy.groupby(grouping_col):\n        # Ensure the group is large enough for meaningful analysis\n        if len(group) < 3:\n            continue\n\n        # Identify all high-performing ('success') posts in the category\n        competitor_posts = group[group['source'] == 'competitor']\n        ours_posts = group[group['source'] == 'ours']\n        \n        # High-performing 'ours' are those in the top 30% of inflow\n        ours_high_idx = pd.Index([])\n        if not ours_posts.empty and ours_posts['non_brand_inflow_numeric'].notna().sum() > 1:\n            high_quantile = ours_posts['non_brand_inflow_numeric'].quantile(0.7, interpolation='higher')\n            # Include posts at or above the high quantile\n            ours_high_posts = ours_posts[ours_posts['non_brand_inflow_numeric'] >= high_quantile]\n            if not ours_high_posts.empty:\n                ours_high_idx = ours_high_posts.index\n\n        # The success group comprises all competitors and our best posts\n        success_group_idx = competitor_posts.index.union(ours_high_idx)\n\n        # We need at least 3 posts to create a stable centroid\n        if len(success_group_idx) < 3:\n            continue\n\n        try:\n            # Get integer positions to slice the main embeddings tensor\n            # This is crucial for performance and correctness\n            success_indices = df_copy.index.get_indexer_for(success_group_idx)\n            group_member_indices = df_copy.index.get_indexer_for(group.index)\n\n            # Filter out -1s which indicate an index was not found\n            success_indices = success_indices[success_indices != -1]\n            if len(success_indices) < 3:\n                continue\n\n            # Extract embeddings for the success group and the entire category group\n            success_embeddings = all_embeddings[success_indices]\n            group_embeddings = all_embeddings[group_member_indices]\n\n            # 3. Calculate the single 'Success Centroid' (archetype)\n            # This is the semantic average of all successful posts in the category.\n            success_centroid = torch.mean(success_embeddings, dim=0, keepdim=True)\n\n            # 4. Calculate similarity of each post in the group to the success centroid\n            similarities = util.cos_sim(group_embeddings, success_centroid)\n            \n            # Squeeze to remove the last dimension and convert to a flat numpy array\n            scores = similarities.squeeze().cpu().numpy()\n\n            # Assign the calculated scores back to the DataFrame using original indices\n            df_copy.loc[group.index, feature_name] = scores\n\n        except Exception as e:\n            # If an error occurs for a specific group, skip it and continue\n            continue\n\n    # 5. Clean up temporary columns and handle missing values\n    df_copy = df_copy.drop(columns=['enriched_text', 'non_brand_inflow_numeric'])\n    \n    # For posts in categories that were skipped, fill with the median of calculated scores\n    # This is a reasonable imputation for posts where a score could not be derived.\n    median_score = df_copy[feature_name].median()\n    if pd.notna(median_score):\n        df_copy[feature_name].fillna(median_score, inplace=True)\n    else:\n        # Fallback if no scores were calculated at all (e.g., very small dataset)\n        df_copy[feature_name].fillna(0, inplace=True)\n\n    # Return the new feature column attached to the original DataFrame structure\n    df[feature_name] = df_copy[feature_name]\n    return df"
        }
      }
    ],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": -0.07687289829442284,
          "p_value": 0.45903181795911097,
          "interpretation": "약한 음의 상관관계(-0.0769)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.4590), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        },
        "non_brand_average_ctr": {
          "correlation": -0.049795010487837583,
          "p_value": 0.6317863141299299,
          "interpretation": "약한 음의 상관관계(-0.0498)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.6318), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "High-performing content is not only aligned with a success archetype but is also semantically *focused*. Low-performing content is often semantically scattered, showing moderate similarity to multiple, often conflicting, archetypes. This feature quantifies this 'semantic focus' by calculating the ratio of a post's similarity to its single best-fit success archetype versus the sum of its similarities to all other archetypes (both success and failure). A high score, indicating a clear and unambiguous alignment with a single winning pattern, is expected to positively correlate with performance metrics like `non_brand_inflow`.",
      "feature_created": "semantic_focus_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:26:19.605285",
    "iteration": 14,
    "initial_agent_response": {
      "feature_name": "semantic_clarity_score",
      "hypothesis": "Success is predicted not just by aligning with a winning content strategy ('Success Archetype'), but by demonstrating 'semantic clarity'. This clarity is defined as a strong, unambiguous alignment with a single Success Archetype, combined with a consistent and uniform disavowal of all common failure patterns ('Failure Archetypes'). This feature scores a post by taking its maximum similarity to any Success Archetype and subtracting a 'confusion penalty', which is the sum of its average similarity and the standard deviation of its similarities to all Failure Archetypes. A higher score, indicating clear alignment with success and a decisive rejection of failure patterns, is expected to positively correlate with `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\nfrom sklearn.cluster import KMeans\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the SentenceTransformer model, ensuring it's loaded only once.\"\"\"\n    global _model\n    if _model is None:\n        # Using a powerful multilingual model suitable for various content types.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic_clarity_score for each post.\n\n    The score is based on a post's relationship to semantic archetypes derived from\n    high- and low-performing content within its topic category.\n    \"\"\"\n    # 2. Check for empty DataFrame to prevent errors.\n    if df.empty:\n        return df\n\n    # 3. Preserve the original index to ensure correct feature assignment.\n    # This is a critical step to prevent data misalignment.\n    original_index = df.index\n\n    # Ensure required columns are treated as strings and handle missing values.\n    df['post_title'] = df['post_title'].fillna('').astype(str)\n    df['post_body'] = df['post_body'].fillna('').astype(str)\n    df['morpheme_words'] = df['morpheme_words'].fillna('').astype(str)\n\n    # Create an enriched text representation for more accurate semantic analysis.\n    df['enriched_text'] = df['post_title'] + ' ' + df['post_body'] + ' ' + df['morpheme_words']\n\n    # Initialize the feature column with a default value (NaN).\n    feature_name = 'semantic_clarity_score'\n    if feature_name not in df.columns:\n        df[feature_name] = np.nan\n\n    model = get_model()\n\n    # 4. Use efficient batch processing for all texts at once.\n    all_embeddings = model.encode(df['enriched_text'].tolist(), convert_to_tensor=True, show_progress_bar=False)\n\n    # Group by the main topic category to perform analysis within relevant contexts.\n    for category, group_df in df.groupby('top_category_name'):\n        if len(group_df) < 10:  # Skip small categories where analysis is not meaningful.\n            continue\n\n        # Define high and low performance groups within our own posts.\n        our_posts = group_df[group_df['source'] == 'ours']\n        if not our_posts.empty:\n            inflow_quantile_30 = our_posts['non_brand_inflow'].quantile(0.30, interpolation='lower')\n            inflow_quantile_70 = our_posts['non_brand_inflow'].quantile(0.70, interpolation='higher')\n\n            low_perf_mask = (group_df['source'] == 'ours') & (group_df['non_brand_inflow'] <= inflow_quantile_30)\n            high_perf_ours_mask = (group_df['source'] == 'ours') & (group_df['non_brand_inflow'] >= inflow_quantile_70)\n        else:\n            low_perf_mask = pd.Series(False, index=group_df.index)\n            high_perf_ours_mask = pd.Series(False, index=group_df.index)\n\n        competitor_mask = group_df['source'] == 'competitor'\n\n        # Combine competitor posts and our high-performing posts for the 'Success' group.\n        high_perf_group_mask = competitor_mask | high_perf_ours_mask\n\n        # 5. Get integer positions for slicing the embeddings tensor. This is critical.\n        group_indices = df.index.get_indexer_for(group_df.index)\n        high_perf_indices = df.index.get_indexer_for(group_df[high_perf_group_mask].index)\n        low_perf_indices = df.index.get_indexer_for(group_df[low_perf_mask].index)\n\n        group_embeddings = all_embeddings[group_indices]\n\n        # Determine the number of clusters (archetypes), ensuring it's not more than the number of samples.\n        k_success = min(5, len(high_perf_indices))\n        k_failure = min(5, len(low_perf_indices))\n\n        if k_success == 0 or k_failure == 0 or len(group_embeddings) == 0:\n            continue # Not enough data in this category to create both archetypes.\n\n        # --- Calculate Success Archetypes ---\n        high_perf_embeddings = all_embeddings[high_perf_indices]\n        kmeans_success = KMeans(n_clusters=k_success, random_state=42, n_init=10)\n        kmeans_success.fit(high_perf_embeddings.cpu().numpy())\n        success_centroids = torch.tensor(kmeans_success.cluster_centers_, device=model.device)\n\n        # --- Calculate Failure Archetypes ---\n        low_perf_embeddings = all_embeddings[low_perf_indices]\n        kmeans_failure = KMeans(n_clusters=k_failure, random_state=42, n_init=10)\n        kmeans_failure.fit(low_perf_embeddings.cpu().numpy())\n        failure_centroids = torch.tensor(kmeans_failure.cluster_centers_, device=model.device)\n\n        # --- Calculate Scores for all posts in the category ---\n        sim_to_success = util.pytorch_cos_sim(group_embeddings, success_centroids)\n        sim_to_failure = util.pytorch_cos_sim(group_embeddings, failure_centroids)\n\n        max_sim_s, _ = torch.max(sim_to_success, dim=1)\n        \n        mean_sim_f = torch.mean(sim_to_failure, dim=1)\n        \n        # Handle case where std dev is not meaningful (only one failure archetype)\n        if k_failure > 1:\n            std_sim_f = torch.std(sim_to_failure, dim=1)\n        else:\n            std_sim_f = torch.zeros_like(mean_sim_f)\n            \n        # The core logic of the hypothesis: clarity = alignment_with_success - confusion_penalty\n        clarity_scores = max_sim_s - (mean_sim_f + std_sim_f)\n\n        # 6. Assign calculated scores back to the original DataFrame using the correct index.\n        df.loc[group_df.index, feature_name] = clarity_scores.cpu().numpy()\n\n    # Clean up temporary column and fill any remaining NaNs for skipped categories.\n    df.drop(columns=['enriched_text'], inplace=True)\n    df[feature_name] = df[feature_name].fillna(0.0)\n\n    # Ensure the original index is restored if it was altered.\n    df = df.reindex(original_index)\n    \n    return df"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.30596802831612624,
          "p_value": 0.002566443435592381,
          "interpretation": "중간 정도의 양의 상관관계(0.3060)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0026), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.06684745613231097,
          "p_value": 0.519806587817053,
          "interpretation": "약한 양의 상관관계(0.0668)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.5198), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Success is predicted not just by aligning with a winning content strategy ('Success Archetype'), but by demonstrating 'semantic clarity'. This clarity is defined as a strong, unambiguous alignment with a single Success Archetype, combined with a consistent and uniform disavowal of all common failure patterns ('Failure Archetypes'). This feature scores a post by taking its maximum similarity to any Success Archetype and subtracting a 'confusion penalty', which is the sum of its average similarity and the standard deviation of its similarities to all Failure Archetypes. A higher score, indicating clear alignment with success and a decisive rejection of failure patterns, is expected to positively correlate with `non_brand_inflow`.",
      "feature_created": "semantic_clarity_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:27:18.666571",
    "iteration": 15,
    "initial_agent_response": {
      "feature_name": "archetypal_field_influence_score",
      "hypothesis": "The performance of a post is influenced by its aggregate alignment with all identified successful content patterns ('Success Archetypes') and its aggregate repulsion from all identified failure patterns ('Failure Archetypes') within its topic category. This feature is based on an enriched text representation combining 'post_title', 'post_body', and 'morpheme_words'. It calculates a score by summing a post's semantic similarities to all Success Archetypes and subtracting the sum of its similarities to all Failure Archetypes. A high positive score signifies that the post resonates strongly and broadly with the entire 'field' of success while avoiding the 'field' of failure, predicting higher 'non_brand_inflow'.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a model that performs well on Korean and general semantic tasks\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the Archetypal Field Influence Score for each post.\n\n    This score measures a post's total semantic alignment with successful content\n    patterns while simultaneously measuring its repulsion from failure patterns.\n    \"\"\"\n    feature_name = 'archetypal_field_influence_score'\n    df[feature_name] = np.nan\n\n    if df.empty:\n        return df\n\n    # Ensure required columns are present\n    required_cols = ['top_category_name', 'source', 'non_brand_inflow', 'post_title', 'post_body', 'morpheme_words']\n    if not all(col in df.columns for col in required_cols):\n        # Or raise an error, depending on desired behavior\n        return df\n\n    model = get_model()\n\n    # Create an enriched text representation for more robust embeddings\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).str.strip()\n    \n    # Ensure original index is preserved for later mapping\n    original_index = df.index\n\n    # Batch encode all texts at once for efficiency\n    all_embeddings = model.encode(\n        df['enriched_text'].tolist(), \n        convert_to_tensor=True, \n        show_progress_bar=False\n    )\n\n    # Group by topic category to perform analysis within context\n    grouped = df.groupby('top_category_name')\n\n    for name, group in grouped:\n        # Define high and low performing groups based on the hypothesis\n        ours_df = group[group['source'] == 'ours'].dropna(subset=['non_brand_inflow'])\n\n        if len(ours_df) < 10:  # Need enough data to create meaningful quantiles\n            continue\n\n        low_perf_threshold = ours_df['non_brand_inflow'].quantile(0.3, interpolation='lower')\n        high_perf_threshold = ours_df['non_brand_inflow'].quantile(0.7, interpolation='higher')\n\n        high_performers = group[\n            (group['source'] == 'competitor') | \n            ((group['source'] == 'ours') & (group['non_brand_inflow'] >= high_perf_threshold))\n        ]\n        \n        low_performers = group[\n            (group['source'] == 'ours') & (group['non_brand_inflow'] <= low_perf_threshold)\n        ]\n\n        # Need at least k members to form clusters\n        k = 3 \n        if len(high_performers) < k or len(low_performers) < k:\n            continue\n\n        # Safely get embeddings using original index positions\n        high_indices = original_index.get_indexer(high_performers.index)\n        low_indices = original_index.get_indexer(low_performers.index)\n        group_indices = original_index.get_indexer(group.index)\n        \n        high_embeddings = all_embeddings[high_indices]\n        low_embeddings = all_embeddings[low_indices]\n        group_embeddings = all_embeddings[group_indices]\n\n        # --- Create Success Archetypes ---\n        num_high_clusters = min(k, len(high_performers))\n        kmeans_high = KMeans(n_clusters=num_high_clusters, random_state=42, n_init=10)\n        kmeans_high.fit(high_embeddings.cpu().numpy())\n        success_archetypes = torch.tensor(kmeans_high.cluster_centers_, device=all_embeddings.device)\n\n        # --- Create Failure Archetypes ---\n        num_low_clusters = min(k, len(low_performers))\n        kmeans_low = KMeans(n_clusters=num_low_clusters, random_state=42, n_init=10)\n        kmeans_low.fit(low_embeddings.cpu().numpy())\n        failure_archetypes = torch.tensor(kmeans_low.cluster_centers_, device=all_embeddings.device)\n\n        # --- Calculate Field Influence Score ---\n        # Calculate sum of similarities to all success archetypes\n        sim_to_success = util.cos_sim(group_embeddings, success_archetypes)\n        sum_sim_success = sim_to_success.sum(dim=1)\n\n        # Calculate sum of similarities to all failure archetypes\n        sim_to_failure = util.cos_sim(group_embeddings, failure_archetypes)\n        sum_sim_failure = sim_to_failure.sum(dim=1)\n        \n        # The final score is the total influence from the success field minus the failure field\n        field_score = (sum_sim_success - sum_sim_failure).cpu().numpy()\n\n        # Assign the calculated scores back to the original DataFrame\n        df.loc[group.index, feature_name] = field_score\n\n    df = df.drop(columns=['enriched_text'])\n    return df\n"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.11018469623344476,
          "p_value": 0.2877951373899009,
          "interpretation": "약한 양의 상관관계(0.1102)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.2878), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.08681682051071432,
          "p_value": 0.40283725287790845,
          "interpretation": "약한 양의 상관관계(0.0868)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.4028), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "The performance of a post is influenced by its aggregate alignment with all identified successful content patterns ('Success Archetypes') and its aggregate repulsion from all identified failure patterns ('Failure Archetypes') within its topic category. This feature is based on an enriched text representation combining 'post_title', 'post_body', and 'morpheme_words'. It calculates a score by summing a post's semantic similarities to all Success Archetypes and subtracting the sum of its similarities to all Failure Archetypes. A high positive score signifies that the post resonates strongly and broadly with the entire 'field' of success while avoiding the 'field' of failure, predicting higher 'non_brand_inflow'.",
      "feature_created": "archetypal_field_influence_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:28:36.045503",
    "iteration": 16,
    "initial_agent_response": {
      "feature_name": "weighted_archetype_contrast_score",
      "hypothesis": "The performance of a blog post is not just determined by its alignment with successful content patterns ('Success Archetypes') and its distance from failure patterns ('Failure Archetypes'), but also by the internal consistency or 'coherence' of those patterns. A post that aligns with a very tight, well-defined Success Archetype while avoiding a similarly well-defined Failure Archetype will perform best. This feature will quantify this by weighting the similarity to each archetype by the archetype's own internal coherence (average similarity of its members to the centroid). The final score, `(Max Weighted Sim to Success) - (Max Weighted Sim to Failure)`, is expected to have a strong positive correlation with performance metrics like `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a model that's good for semantic search and clustering.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef get_archetypes_and_coherence(embeddings, n_clusters, min_cluster_size=2):\n    \"\"\"\n    Clusters embeddings to find archetypes and calculates their coherence.\n\n    Args:\n        embeddings (np.ndarray): The embeddings to cluster.\n        n_clusters (int): The number of clusters to form.\n        min_cluster_size (int): Minimum number of posts for an archetype to be valid.\n\n    Returns:\n        list: A list of tuples, where each tuple is (centroid_embedding, coherence_score).\n    \"\"\"\n    if len(embeddings) < n_clusters or len(embeddings) < min_cluster_size:\n        # If not enough data, return a single archetype with average coherence.\n        return [(np.mean(embeddings, axis=0), 0.5)] if len(embeddings) > 0 else []\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(embeddings)\n\n    archetypes = []\n    for i in range(n_clusters):\n        cluster_mask = (labels == i)\n        cluster_embeddings = embeddings[cluster_mask]\n        if len(cluster_embeddings) < min_cluster_size:\n            continue\n\n        centroid = kmeans.cluster_centers_[i]\n        # Coherence: average similarity of each point in the cluster to the centroid.\n        similarities = util.cos_sim(cluster_embeddings, [centroid])\n        coherence = similarities.mean().item()\n        archetypes.append((centroid, coherence))\n    \n    return archetypes\n\ndef calculate_max_weighted_similarity(post_embeddings, archetypes):\n    \"\"\"\n    Calculates the maximum weighted similarity of posts to a set of archetypes.\n    \"\"\"\n    if not archetypes or len(post_embeddings) == 0:\n        return np.zeros(len(post_embeddings))\n\n    # Separate centroids and coherences\n    centroids = np.array([arch[0] for arch in archetypes])\n    coherences = np.array([arch[1] for arch in archetypes])\n\n    # Calculate similarity of all posts to all centroids\n    sim_matrix = util.cos_sim(post_embeddings, centroids).numpy()\n\n    # Weight similarities by coherence\n    weighted_sim_matrix = sim_matrix * coherences\n\n    # Find the max weighted similarity for each post\n    max_weighted_sim = np.max(weighted_sim_matrix, axis=1)\n    \n    return max_weighted_sim\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generates the weighted_archetype_contrast_score feature.\n    \"\"\"\n    feature_name = 'weighted_archetype_contrast_score'\n    df[feature_name] = 0.0  # Initialize with a neutral score\n\n    if df.empty:\n        return df\n\n    model = get_model()\n\n    # Create an enriched text representation for better semantic analysis\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).str.strip()\n\n    # Efficiently encode all texts at once\n    all_embeddings = model.encode(df['enriched_text'].tolist(), convert_to_tensor=False, show_progress_bar=False)\n\n    # Process each topic category separately\n    for category, group in df.groupby('top_category_name'):\n        if len(group) < 10:  # Skip small categories\n            continue\n\n        # Define high and low performance groups within the category\n        ours_posts = group[group['source'] == 'ours']\n        competitor_posts = group[group['source'] == 'competitor']\n\n        if ours_posts.empty or ours_posts['non_brand_inflow'].isnull().all():\n            continue # Not enough data to define performance tiers\n\n        # Define performance thresholds\n        low_quantile = ours_posts['non_brand_inflow'].quantile(0.3)\n        high_quantile = ours_posts['non_brand_inflow'].quantile(0.7)\n\n        low_perf_mask = (group['source'] == 'ours') & (group['non_brand_inflow'] <= low_quantile)\n        high_perf_ours_mask = (group['source'] == 'ours') & (group['non_brand_inflow'] >= high_quantile)\n        competitor_mask = group['source'] == 'competitor'\n        high_perf_mask = high_perf_ours_mask | competitor_mask\n\n        # Get integer indices for slicing the master embeddings array\n        group_indices = df.index.get_indexer(group.index)\n        low_perf_indices = df.index.get_indexer(group[low_perf_mask].index)\n        high_perf_indices = df.index.get_indexer(group[high_perf_mask].index)\n        \n        if len(low_perf_indices) < 3 or len(high_perf_indices) < 3:\n            continue\n\n        # Get embeddings for each group\n        group_embeddings = all_embeddings[group_indices]\n        low_perf_embeddings = all_embeddings[low_perf_indices]\n        high_perf_embeddings = all_embeddings[high_perf_indices]\n\n        # Identify archetypes and their coherence for both groups\n        n_archetypes = min(3, len(low_perf_indices)//2, len(high_perf_indices)//2)\n        if n_archetypes < 1:\n            continue\n\n        success_archetypes = get_archetypes_and_coherence(high_perf_embeddings, n_clusters=n_archetypes)\n        failure_archetypes = get_archetypes_and_coherence(low_perf_embeddings, n_clusters=n_archetypes)\n\n        if not success_archetypes or not failure_archetypes:\n            continue\n\n        # Calculate scores for all posts in the current category\n        max_sim_success = calculate_max_weighted_similarity(group_embeddings, success_archetypes)\n        max_sim_failure = calculate_max_weighted_similarity(group_embeddings, failure_archetypes)\n        \n        group_score = max_sim_success - max_sim_failure\n\n        # Safely assign the calculated scores back to the original DataFrame\n        df.loc[group.index, feature_name] = group_score\n\n    # Clean up the temporary column\n    df.drop(columns=['enriched_text'], inplace=True)\n\n    return df"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.3068971390381811,
          "p_value": 0.0024861669852600395,
          "interpretation": "중간 정도의 양의 상관관계(0.3069)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0025), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.11442241435215422,
          "p_value": 0.2695416677202267,
          "interpretation": "약한 양의 상관관계(0.1144)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.2695), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "The performance of a blog post is not just determined by its alignment with successful content patterns ('Success Archetypes') and its distance from failure patterns ('Failure Archetypes'), but also by the internal consistency or 'coherence' of those patterns. A post that aligns with a very tight, well-defined Success Archetype while avoiding a similarly well-defined Failure Archetype will perform best. This feature will quantify this by weighting the similarity to each archetype by the archetype's own internal coherence (average similarity of its members to the centroid). The final score, `(Max Weighted Sim to Success) - (Max Weighted Sim to Failure)`, is expected to have a strong positive correlation with performance metrics like `non_brand_inflow`.",
      "feature_created": "weighted_archetype_contrast_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:29:43.125663",
    "iteration": 17,
    "initial_agent_response": {
      "feature_name": "optimal_semantic_shift_score",
      "hypothesis": "Within each topic category ('top_category_name'), there exists an aggregate 'Center of Success' (the centroid of all success archetype centroids) and a 'Center of Failure' (the centroid of all failure archetype centroids). The vector connecting these two centers defines the 'Optimal Semantic Shift' for that topic. A post's performance is predicted by how well its own semantic position aligns with this optimal shift. The score is calculated by projecting the vector from the 'Center of Failure' to the post onto the 'Optimal Semantic Shift' vector. A higher score, indicating the post has moved in the correct semantic direction away from failure and towards success, is expected to positively correlate with `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean and general text\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the alignment of a post with the optimal semantic shift vector for its category.\n\n    This vector points from the 'center of failure' to the 'center of success'.\n    \"\"\"\n    feature_name = 'optimal_semantic_shift_score'\n    df[feature_name] = np.nan\n\n    if df.empty:\n        return df\n\n    required_columns = ['top_category_name', 'source', 'non_brand_inflow', 'post_title', 'post_body', 'morpheme_words']\n    for col in required_columns:\n        if col not in df.columns:\n            # If a required column is missing, return the original DataFrame with the empty feature column\n            return df\n\n    model = get_model()\n\n    # Create an enriched text representation for more accurate embeddings\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).str.strip()\n\n    # 2. Use efficient batch processing for all texts at once\n    # This is critical for performance\n    all_embeddings = model.encode(\n        df['enriched_text'].tolist(), \n        convert_to_tensor=True, \n        show_progress_bar=False\n    )\n\n    # Ensure we are on CPU to avoid potential device mismatches\n    all_embeddings = all_embeddings.cpu()\n\n    # Process each category separately\n    for category, group_df in df.groupby('top_category_name'):\n        if len(group_df) < 10:  # Skip categories with too few posts for meaningful analysis\n            continue\n\n        # --- Define High and Low performance groups for the category ---\n        our_posts = group_df[group_df['source'] == 'ours']\n        if our_posts.empty or our_posts['non_brand_inflow'].dropna().empty:\n            continue\n\n        # Define performance thresholds\n        inflow_threshold_high = our_posts['non_brand_inflow'].quantile(0.7, interpolation='lower')\n        inflow_threshold_low = our_posts['non_brand_inflow'].quantile(0.3, interpolation='higher')\n\n        # Get indices for high-performing posts (all competitors + our top 30%)\n        high_perf_our_indices = our_posts[our_posts['non_brand_inflow'] >= inflow_threshold_high].index\n        competitor_indices = group_df[group_df['source'] == 'competitor'].index\n        high_perf_indices = competitor_indices.union(high_perf_our_indices)\n\n        # Get indices for low-performing posts (our bottom 30%)\n        low_perf_indices = our_posts[our_posts['non_brand_inflow'] <= inflow_threshold_low].index\n\n        # Map DataFrame indices to integer locations in the embeddings tensor\n        # This preserves the original index and prevents misalignment\n        try:\n            high_perf_locs = [df.index.get_loc(i) for i in high_perf_indices]\n            low_perf_locs = [df.index.get_loc(i) for i in low_perf_indices]\n        except KeyError:\n            # This can happen in rare edge cases; skip the group if an index is not found\n            continue\n\n        high_perf_embeddings = all_embeddings[high_perf_locs]\n        low_perf_embeddings = all_embeddings[low_perf_locs]\n        \n        # We need enough data points in both groups to create archetypes\n        if len(high_perf_embeddings) < 2 or len(low_perf_embeddings) < 2:\n            continue\n\n        # --- Identify Archetypes (Clusters) --- \n        # Determine a dynamic number of clusters for robustness\n        n_clusters_high = max(2, min(5, len(high_perf_embeddings) // 2))\n        n_clusters_low = max(2, min(5, len(low_perf_embeddings) // 2))\n\n        if len(high_perf_embeddings) < n_clusters_high or len(low_perf_embeddings) < n_clusters_low:\n            continue\n\n        kmeans_high = KMeans(n_clusters=n_clusters_high, random_state=42, n_init=10).fit(high_perf_embeddings.numpy())\n        success_archetypes = torch.tensor(kmeans_high.cluster_centers_, dtype=torch.float32)\n\n        kmeans_low = KMeans(n_clusters=n_clusters_low, random_state=42, n_init=10).fit(low_perf_embeddings.numpy())\n        failure_archetypes = torch.tensor(kmeans_low.cluster_centers_, dtype=torch.float32)\n\n        # --- Calculate the Optimal Semantic Shift Vector ---\n        center_of_success = success_archetypes.mean(dim=0)\n        center_of_failure = failure_archetypes.mean(dim=0)\n        \n        optimal_shift_vector = center_of_success - center_of_failure\n        norm = torch.linalg.norm(optimal_shift_vector)\n        if norm > 1e-6: # Avoid division by zero\n             optimal_shift_vector_normalized = optimal_shift_vector / norm\n        else:\n             continue # Cannot define a meaningful direction\n\n        # --- Calculate score for each post in the category ---\n        group_indices = group_df.index\n        group_locs = [df.index.get_loc(i) for i in group_indices]\n        group_embeddings = all_embeddings[group_locs]\n\n        # Vectorize the calculation for the entire group\n        post_vectors = group_embeddings - center_of_failure\n        # Project each post's vector onto the normalized optimal shift vector\n        scores = torch.mv(post_vectors, optimal_shift_vector_normalized)\n        \n        # Assign scores back to the original DataFrame using the correct index\n        df.loc[group_indices, feature_name] = scores.numpy()\n    \n    # Clean up the temporary column\n    df.drop(columns=['enriched_text'], inplace=True)\n    \n    return df"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.11189228480495664,
          "p_value": 0.2803435860522801,
          "interpretation": "약한 양의 상관관계(0.1119)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.2803), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.059094489001019546,
          "p_value": 0.5694549979538048,
          "interpretation": "약한 양의 상관관계(0.0591)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.5695), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic category ('top_category_name'), there exists an aggregate 'Center of Success' (the centroid of all success archetype centroids) and a 'Center of Failure' (the centroid of all failure archetype centroids). The vector connecting these two centers defines the 'Optimal Semantic Shift' for that topic. A post's performance is predicted by how well its own semantic position aligns with this optimal shift. The score is calculated by projecting the vector from the 'Center of Failure' to the post onto the 'Optimal Semantic Shift' vector. A higher score, indicating the post has moved in the correct semantic direction away from failure and towards success, is expected to positively correlate with `non_brand_inflow`.",
      "feature_created": "optimal_semantic_shift_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:31:03.747463",
    "iteration": 18,
    "initial_agent_response": {
      "feature_name": "semantic_trajectory_purity_score",
      "hypothesis": "Within a topic category (`top_category_name`), a post's performance is predicted by the 'purity' of its semantic trajectory away from failure and towards success. This purity is a combination of (1) its directional alignment with the vector from its nearest 'Failure Archetype' to its nearest 'Success Archetype', and (2) its 'commitment' to this path, measured by its high similarity to the target success archetype and low similarity to the source failure archetype. The final score, `(Directional Alignment) * (Commitment Score)`, is expected to positively correlate with `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\nimport torch.nn.functional as F\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for diverse content\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic_trajectory_purity_score for each post.\n\n    This score measures how decisively a post moves from a 'failure' semantic area\n    to a 'success' semantic area within its topic category.\n    \"\"\"\n    if df.empty:\n        df['semantic_trajectory_purity_score'] = []\n        return df\n\n    model = get_model()\n    device = model.device\n\n    # Ensure required columns are present and handle missing values\n    for col in ['post_title', 'post_body', 'morpheme_words', 'top_category_name', 'source', 'non_brand_inflow']:\n        if col not in df.columns:\n            raise ValueError(f\"Input DataFrame is missing required column: {col}\")\n\n    # Create an enriched text representation for more accurate embeddings\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).str.strip()\n    \n    # Initialize the feature column with a default value\n    feature_name = 'semantic_trajectory_purity_score'\n    df[feature_name] = 0.0\n\n    # Efficiently encode all texts at once\n    all_embeddings = model.encode(\n        df['enriched_text'].tolist(), \n        convert_to_tensor=True, \n        show_progress_bar=False,\n        device=device\n    )\n\n    # Process posts group by group based on their topic category\n    for category, group_df in df.groupby('top_category_name'):\n        if len(group_df) < 10:  # Skip small categories\n            continue\n\n        # Define high- and low-performing posts within the category\n        our_posts = group_df[group_df['source'] == 'ours']\n        if our_posts.empty or our_posts['non_brand_inflow'].isnull().all():\n            continue\n\n        low_quantile = our_posts['non_brand_inflow'].quantile(0.3, interpolation='lower')\n        high_quantile = our_posts['non_brand_inflow'].quantile(0.7, interpolation='higher')\n\n        high_perf_mask = ((group_df['source'] == 'competitor') |\n                          ((group_df['source'] == 'ours') & (group_df['non_brand_inflow'] >= high_quantile)))\n        low_perf_mask = ((group_df['source'] == 'ours') & (group_df['non_brand_inflow'] <= low_quantile))\n        \n        high_perf_indices = group_df.index[high_perf_mask]\n        low_perf_indices = group_df.index[low_perf_mask]\n\n        # Get integer positions for slicing the global embeddings tensor\n        # This is a critical step to prevent data misalignment\n        high_perf_ilocs = df.index.get_indexer_for(high_perf_indices)\n        low_perf_ilocs = df.index.get_indexer_for(low_perf_indices)\n        group_ilocs = df.index.get_indexer_for(group_df.index)\n\n        # Determine the number of clusters (archetypes)\n        k_success = min(3, len(high_perf_ilocs)) if len(high_perf_ilocs) > 0 else 0\n        k_failure = min(3, len(low_perf_ilocs)) if len(low_perf_ilocs) > 0 else 0\n\n        if k_success == 0 or k_failure == 0:\n            continue\n\n        # Cluster high-performing posts to find 'Success Archetypes'\n        high_embeddings = all_embeddings[high_perf_ilocs]\n        kmeans_success = KMeans(n_clusters=k_success, random_state=42, n_init='auto').fit(high_embeddings.cpu().numpy())\n        success_centroids = torch.tensor(kmeans_success.cluster_centers_, device=device, dtype=torch.float32)\n\n        # Cluster low-performing posts to find 'Failure Archetypes'\n        low_embeddings = all_embeddings[low_perf_ilocs]\n        kmeans_failure = KMeans(n_clusters=k_failure, random_state=42, n_init='auto').fit(low_embeddings.cpu().numpy())\n        failure_centroids = torch.tensor(kmeans_failure.cluster_centers_, device=device, dtype=torch.float32)\n        \n        # Vectorized calculation for all posts in the current group\n        group_embeddings = all_embeddings[group_ilocs]\n        \n        # Find the nearest success and failure archetype for each post\n        sim_to_success = util.cos_sim(group_embeddings, success_centroids)\n        sim_to_failure = util.cos_sim(group_embeddings, failure_centroids)\n        \n        max_sim_succ, best_succ_idx = torch.max(sim_to_success, dim=1)\n        max_sim_fail, best_fail_idx = torch.max(sim_to_failure, dim=1)\n        \n        # Get the corresponding centroid vectors for each post\n        C_succ_batch = success_centroids[best_succ_idx]\n        C_fail_batch = failure_centroids[best_fail_idx]\n        \n        # 1. Directional Alignment Score\n        V_post_batch = group_embeddings - C_fail_batch\n        V_ideal_batch = C_succ_batch - C_fail_batch\n        # F.cosine_similarity handles normalization and computes score for each row\n        align_scores = F.cosine_similarity(V_post_batch, V_ideal_batch, dim=1)\n        align_scores.clamp_(min=0) # Ignore negative alignment\n        \n        # 2. Commitment Score\n        # Measures how close a post is to its target success archetype and far from its source failure one\n        commitment_scores = max_sim_succ * (1 - torch.clamp(max_sim_fail, 0, 1))\n        \n        # Final Score = Directional Alignment * Commitment\n        final_scores = align_scores * commitment_scores\n        \n        # Assign the calculated scores back to the original DataFrame using correct indices\n        df.loc[group_df.index, feature_name] = final_scores.cpu().numpy()\n\n    # Clean up temporary column\n    df.drop(columns=['enriched_text'], inplace=True)\n\n    return df\n"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.352309859850184,
          "p_value": 0.0004628164027171375,
          "interpretation": "중간 정도의 양의 상관관계(0.3523)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0005), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.07199392898578952,
          "p_value": 0.48810808986007864,
          "interpretation": "약한 양의 상관관계(0.0720)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.4881), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within a topic category (`top_category_name`), a post's performance is predicted by the 'purity' of its semantic trajectory away from failure and towards success. This purity is a combination of (1) its directional alignment with the vector from its nearest 'Failure Archetype' to its nearest 'Success Archetype', and (2) its 'commitment' to this path, measured by its high similarity to the target success archetype and low similarity to the source failure archetype. The final score, `(Directional Alignment) * (Commitment Score)`, is expected to positively correlate with `non_brand_inflow`.",
      "feature_created": "semantic_trajectory_purity_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:32:38.568677",
    "iteration": 19,
    "initial_agent_response": {
      "feature_name": "strategic_safety_score",
      "hypothesis": "Within a topic category ('top_category_name'), a post's performance is predicted not only by its alignment with a 'Success Archetype' (clusters of high-performing content) but also by the 'strategic safety' of that archetype. A 'safe' Success Archetype is one that is semantically distant from all known 'Failure Archetypes' (clusters of low-performing content). This feature calculates a score by multiplying a post's maximum similarity to any Success Archetype by the minimum distance (1 - similarity) from that specific archetype to any Failure Archetype. A higher score signifies alignment with a robust success pattern that is well-defended from common failure modes, and is expected to positively correlate with `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport torch\n\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the strategic_safety_score for each post.\n\n    This score measures how well a post aligns with a 'safe' success archetype,\n    defined as a cluster of high-performing content that is semantically distant\n    from all clusters of low-performing content.\n    \"\"\"\n    feature_name = 'strategic_safety_score'\n\n    if df.empty or not all(c in df.columns for c in ['top_category_name', 'source', 'non_brand_inflow', 'post_title', 'post_body', 'morpheme_words']):\n        df[feature_name] = np.nan\n        return df\n\n    df_copy = df.copy()\n    df_copy[feature_name] = np.nan\n\n    df_copy['enriched_text'] = (\n        df_copy['post_title'].fillna('') + ' ' + \n        df_copy['post_body'].fillna('') + ' ' + \n        df_copy['morpheme_words'].fillna('')\n    ).str.strip()\n\n    if df_copy['enriched_text'].eq('').all():\n        df[feature_name] = np.nan\n        return df\n\n    model = get_model()\n    all_embeddings = model.encode(\n        df_copy['enriched_text'].tolist(), \n        convert_to_tensor=True, \n        show_progress_bar=False\n    )\n\n    for category, group in df_copy.groupby('top_category_name'):\n        if len(group) < 5:\n            continue\n\n        ours_posts = group[group['source'] == 'ours']\n        high_perf_ours_indices = pd.Index([])\n        low_perf_ours_indices = pd.Index([])\n\n        if not ours_posts.empty and ours_posts['non_brand_inflow'].notna().any():\n            inflow_not_na = ours_posts.dropna(subset=['non_brand_inflow'])\n            if len(inflow_not_na) >= 2:\n              low_quantile = inflow_not_na['non_brand_inflow'].quantile(0.3, interpolation='lower')\n              high_quantile = inflow_not_na['non_brand_inflow'].quantile(0.7, interpolation='higher')\n              low_perf_ours_indices = inflow_not_na[inflow_not_na['non_brand_inflow'] <= low_quantile].index\n              high_perf_ours_indices = inflow_not_na[inflow_not_na['non_brand_inflow'] >= high_quantile].index\n\n        competitor_indices = group[group['source'] == 'competitor'].index\n        high_perf_indices = high_perf_ours_indices.union(competitor_indices)\n        low_perf_indices = low_perf_ours_indices\n\n        group_positions = df_copy.index.get_indexer(group.index)\n        group_embeddings = all_embeddings[group_positions]\n        \n        hp_group_positions = group.index.get_indexer_for(high_perf_indices.intersection(group.index))\n        lp_group_positions = group.index.get_indexer_for(low_perf_indices.intersection(group.index))\n\n        if len(hp_group_positions) < 2:\n            continue\n\n        hp_embeddings = group_embeddings[hp_group_positions]\n        lp_embeddings = group_embeddings[lp_group_positions] if len(lp_group_positions) > 0 else torch.tensor([])\n\n        k_success = max(1, min(5, len(hp_group_positions) // 5))\n        if len(hp_group_positions) < k_success:\n            k_success = 1\n\n        if k_success > 1:\n            kmeans_success = KMeans(n_clusters=k_success, random_state=42, n_init='auto').fit(hp_embeddings.cpu().numpy())\n            sa_centroids = torch.tensor(kmeans_success.cluster_centers_, device=all_embeddings.device, dtype=torch.float32)\n        else:\n            sa_centroids = hp_embeddings.mean(dim=0, keepdim=True)\n\n        fa_centroids = torch.tensor([], device=all_embeddings.device)\n        if lp_embeddings.numel() > 0:\n            k_failure = max(1, min(5, lp_embeddings.shape[0] // 5))\n            if lp_embeddings.shape[0] < k_failure:\n                k_failure = 1\n\n            if k_failure > 1:\n                kmeans_failure = KMeans(n_clusters=k_failure, random_state=42, n_init='auto').fit(lp_embeddings.cpu().numpy())\n                fa_centroids = torch.tensor(kmeans_failure.cluster_centers_, device=all_embeddings.device, dtype=torch.float32)\n            else:\n                fa_centroids = lp_embeddings.mean(dim=0, keepdim=True)\n\n        safety_margins = []\n        if fa_centroids.numel() > 0:\n            sa_fa_dists = 1 - util.cos_sim(sa_centroids, fa_centroids)\n            min_dists, _ = torch.min(sa_fa_dists, dim=1)\n            safety_margins = min_dists\n        else:\n            safety_margins = torch.ones(sa_centroids.shape[0], device=all_embeddings.device)\n\n        post_sa_sims = util.cos_sim(group_embeddings, sa_centroids)\n        weighted_sims = post_sa_sims * safety_margins\n        max_weighted_sim, _ = torch.max(weighted_sims, dim=1)\n\n        df_copy.loc[group.index, feature_name] = max_weighted_sim.cpu().numpy()\n\n    df[feature_name] = df_copy[feature_name]\n    return df"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": -0.15280072821637086,
          "p_value": 0.13932823321755816,
          "interpretation": "약한 음의 상관관계(-0.1528)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.1393), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.05222035620867571,
          "p_value": 0.6152552422680053,
          "interpretation": "약한 양의 상관관계(0.0522)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.6153), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within a topic category ('top_category_name'), a post's performance is predicted not only by its alignment with a 'Success Archetype' (clusters of high-performing content) but also by the 'strategic safety' of that archetype. A 'safe' Success Archetype is one that is semantically distant from all known 'Failure Archetypes' (clusters of low-performing content). This feature calculates a score by multiplying a post's maximum similarity to any Success Archetype by the minimum distance (1 - similarity) from that specific archetype to any Failure Archetype. A higher score signifies alignment with a robust success pattern that is well-defended from common failure modes, and is expected to positively correlate with `non_brand_inflow`.",
      "feature_created": "strategic_safety_score"
    }
  },
  {
    "timestamp": "2025-07-21T20:33:50.076478",
    "iteration": 20,
    "initial_agent_response": {
      "feature_name": "semantic_escape_velocity_score",
      "hypothesis": "Within each topic category ('top_category_name'), a post's performance is determined by its ability to achieve 'semantic escape velocity' from the gravitational pull of common failure patterns. This is measured by its relative proximity to its nearest 'Success Archetype' (cluster of high-performers) versus its nearest 'Failure Archetype' (cluster of low-performers), using an enriched text representation ('title' + 'body' + 'morphemes'). The score is calculated as `Sim_to_Nearest_Success / (Sim_to_Nearest_Success + Sim_to_Nearest_Failure)`. A higher score, indicating a decisive move towards a winning pattern and away from a losing one, is expected to positively correlate with performance metrics like `non_brand_inflow`.",
      "python_code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.cluster import KMeans\nimport warnings\n\nwarnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a model that is effective for Korean and general semantic tasks.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the 'semantic escape velocity' score for each post.\n    This score measures a post's semantic similarity to the nearest success archetype\n    relative to its similarity to the nearest failure archetype.\n    \"\"\"\n    feature_name = 'semantic_escape_velocity_score'\n\n    # 0. Handle empty DataFrame\n    if df.empty:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n\n    model = get_model()\n\n    # Ensure the feature column exists with a default value\n    df[feature_name] = np.nan\n\n    # 1. Create an enriched text representation for more accurate embeddings\n    df['enriched_text'] = (\n        df['post_title'].fillna('') + ' ' + \n        df['post_body'].fillna('') + ' ' + \n        df['morpheme_words'].fillna('')\n    ).str.strip()\n\n    # 2. Efficiently encode all texts in a single batch\n    # Preserve the original index to map embeddings back correctly\n    texts_to_encode = df['enriched_text'].tolist()\n    all_embeddings = model.encode(texts_to_encode, convert_to_tensor=True, show_progress_bar=False)\n\n    # Create a mapping from original DataFrame index to embedding tensor index\n    # This is crucial for correctly linking groupby results to the main tensor\n    original_index_to_tensor_pos = {idx: pos for pos, idx in enumerate(df.index)}\n\n    # 3. Process each category group separately\n    for category, group in df.groupby('top_category_name'):\n        if len(group) < 5:  # Skip categories with too few posts for meaningful analysis\n            continue\n\n        # 4. Define High-performing and Low-performing groups within the category\n        ours_group = group[group['source'] == 'ours'].copy()\n        competitor_group = group[group['source'] == 'competitor']\n\n        # Define performance thresholds only on 'ours' posts that have the metric\n        ours_with_metric = ours_group.dropna(subset=['non_brand_inflow'])\n        if len(ours_with_metric) < 2: # Not enough data to define high/low\n            continue\n        \n        low_quantile = ours_with_metric['non_brand_inflow'].quantile(0.3, interpolation='lower')\n        high_quantile = ours_with_metric['non_brand_inflow'].quantile(0.7, interpolation='higher')\n\n        low_perf_ours = ours_with_metric[ours_with_metric['non_brand_inflow'] <= low_quantile]\n        high_perf_ours = ours_with_metric[ours_with_metric['non_brand_inflow'] >= high_quantile]\n\n        # Combine groups to form the final high/low performer sets for the category\n        high_performers = pd.concat([high_perf_ours, competitor_group])\n        low_performers = low_perf_ours\n\n        # Ensure we have posts in both groups to create archetypes\n        if high_performers.empty or low_performers.empty:\n            continue\n\n        # 5. Get embeddings for each group using the pre-computed tensor\n        high_perf_indices = [original_index_to_tensor_pos[i] for i in high_performers.index]\n        low_perf_indices = [original_index_to_tensor_pos[i] for i in low_performers.index]\n        \n        high_perf_embeddings = all_embeddings[high_perf_indices]\n        low_perf_embeddings = all_embeddings[low_perf_indices]\n\n        # 6. Create 'Success' and 'Failure' Archetypes via Clustering\n        # Determine the number of clusters dynamically, capped at 3 for stability\n        k_success = min(3, len(high_perf_embeddings))\n        k_failure = min(3, len(low_perf_embeddings))\n\n        if k_success == 0 or k_failure == 0:\n            continue\n\n        # Find centroids for Success Archetypes\n        kmeans_success = KMeans(n_clusters=k_success, random_state=42, n_init=10)\n        kmeans_success.fit(high_perf_embeddings.cpu().numpy())\n        success_centroids = torch.tensor(kmeans_success.cluster_centers_).to(all_embeddings.device)\n\n        # Find centroids for Failure Archetypes\n        kmeans_failure = KMeans(n_clusters=k_failure, random_state=42, n_init=10)\n        kmeans_failure.fit(low_perf_embeddings.cpu().numpy())\n        failure_centroids = torch.tensor(kmeans_failure.cluster_centers_).to(all_embeddings.device)\n        \n        # 7. Calculate the score for each post in the current category\n        group_indices = [original_index_to_tensor_pos[i] for i in group.index]\n        group_embeddings = all_embeddings[group_indices]\n        \n        # Calculate similarities in batch for efficiency\n        sim_to_success = util.cos_sim(group_embeddings, success_centroids)\n        sim_to_failure = util.cos_sim(group_embeddings, failure_centroids)\n\n        # Find the max similarity to any success/failure archetype\n        max_sim_success, _ = torch.max(sim_to_success, dim=1)\n        max_sim_failure, _ = torch.max(sim_to_failure, dim=1)\n\n        # Calculate the escape velocity score\n        # Add a small epsilon to the denominator to prevent division by zero\n        epsilon = 1e-9\n        scores = max_sim_success / (max_sim_success + max_sim_failure + epsilon)\n        \n        # Assign the calculated scores back to the original DataFrame\n        df.loc[group.index, feature_name] = scores.cpu().numpy()\n\n    # 8. Clean up and finalize\n    # Fill any remaining NaNs (e.g., from small categories) with the median\n    if df[feature_name].isnull().any():\n        median_score = df[feature_name].median()\n        df[feature_name].fillna(median_score, inplace=True)\n    \n    df.drop(columns=['enriched_text'], inplace=True)\n\n    return df"
    },
    "correction_attempts_made": 0,
    "correction_history": [],
    "final_report": {
      "correlation_results": {
        "non_brand_inflow": {
          "correlation": 0.326872817108006,
          "p_value": 0.0012246222447521444,
          "interpretation": "중간 정도의 양의 상관관계(0.3269)를 발견했습니다. 이 결과는 통계적으로 유의미하며(p-value: 0.0012), 이 관계가 우연에 의한 것일 가능성은 낮습니다."
        },
        "non_brand_average_ctr": {
          "correlation": 0.11959903940038655,
          "p_value": 0.24832620842341294,
          "interpretation": "약한 양의 상관관계(0.1196)를 발견했습니다. 하지만 이 결과는 통계적으로 유의미하지 않으므로(p-value: 0.2483), 우연에 의한 결과일 가능성을 배제할 수 없습니다."
        }
      },
      "overall_conclusion": "분석이 완료되었습니다. 각 타겟에 대한 해석을 참고하세요.",
      "hypothesis": "Within each topic category ('top_category_name'), a post's performance is determined by its ability to achieve 'semantic escape velocity' from the gravitational pull of common failure patterns. This is measured by its relative proximity to its nearest 'Success Archetype' (cluster of high-performers) versus its nearest 'Failure Archetype' (cluster of low-performers), using an enriched text representation ('title' + 'body' + 'morphemes'). The score is calculated as `Sim_to_Nearest_Success / (Sim_to_Nearest_Success + Sim_to_Nearest_Failure)`. A higher score, indicating a decisive move towards a winning pattern and away from a losing one, is expected to positively correlate with performance metrics like `non_brand_inflow`.",
      "feature_created": "semantic_escape_velocity_score"
    }
  }
]