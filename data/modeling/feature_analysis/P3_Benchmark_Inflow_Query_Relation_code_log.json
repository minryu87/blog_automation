{
  "timestamp": "2025-07-22T04:15:18.618432",
  "attempt": 1,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'query_intent_satisfaction_ratio'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # A strong multilingual model suitable for Korean text\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the ratio of how well our post satisfies a query's intent\n    compared to the average competitor post for the same query.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with columns 'representative_query',\n                           'post_body', and 'source'.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the new feature column 'query_intent_satisfaction_ratio'.\n    \"\"\"\n    # 2. Always check for empty DataFrame to prevent errors.\n    if df.empty:\n        df['query_intent_satisfaction_ratio'] = pd.Series(dtype=float)\n        return df\n\n    model = get_model()\n\n    # 3. Preserve the original index by working on a copy or being careful.\n    # Create the feature column initialized with NaN.\n    df['query_intent_satisfaction_ratio'] = np.nan\n\n    # 4. Process data in batches for efficiency. Avoid .apply().\n    # Ensure columns are strings and handle potential nulls.\n    queries = df['representative_query'].fillna('').astype(str).tolist()\n    bodies = df['post_body'].fillna('').astype(str).tolist()\n\n    # Encode all queries and bodies in two efficient batch operations.\n    query_embeddings = model.encode(queries, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # 5. Calculate semantic similarity for each row between its query and body.\n    # .diag() correctly extracts the element-wise similarity (query_i vs body_i).\n    relevance_scores = util.cos_sim(query_embeddings, body_embeddings).diag()\n\n    # Temporarily add these scores to the DataFrame for easier grouping.\n    # This is aligned with the original DataFrame's index.\n    df['_temp_relevance'] = relevance_scores.cpu().numpy()\n\n    # 6. Calculate the average competitor relevance for each query group.\n    # This creates a Series where the index is 'representative_query' and the value is the mean score.\n    competitor_avg_relevance = df[df['source'] == 'competitors'].groupby('representative_query')['_temp_relevance'].mean()\n\n    # 7. Use .map() to align the competitor average score with each row in the original DataFrame.\n    # This correctly broadcasts the group's average to all members of that group.\n    our_post_relevance = df['_temp_relevance']\n    competitor_benchmark = df['representative_query'].map(competitor_avg_relevance)\n\n    # Calculate the final feature ratio.\n    # This avoids the logical error of creating a constant feature value by ensuring\n    # the benchmark is specific to each representative_query group.\n    df['query_intent_satisfaction_ratio'] = our_post_relevance / competitor_benchmark\n\n    # 8. Clean up and handle edge cases.\n    # Replace infinite values (division by zero) and NaNs (groups with no competitors)\n    # with a neutral value of 1.0, implying our post is 'on par' with the (non-existent or zero-relevance) competitors.\n    df['query_intent_satisfaction_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n    df['query_intent_satisfaction_ratio'].fillna(1.0, inplace=True)\n\n    # Drop the temporary column.\n    df.drop(columns=['_temp_relevance'], inplace=True)\n\n    # 9. Return the full DataFrame with the new feature.\n    return df"
}{
  "timestamp": "2025-07-22T04:16:18.452000",
  "attempt": 2,
  "status": "success",
  "feature_name": "query_intent_satisfaction_vs_competitors",
  "hypothesis": "우리 포스트의 내용이 '대표 검색어'의 의미와 더 가까울수록(즉, 검색 의도를 더 직접적으로 충족시킬수록), 해당 검색어에 대한 경쟁사 그룹의 평균적인 의도 충족도보다 높게 측정될 것이며, 이 상대적 우위는 'non_brand_inflow'와 양의 상관관계를 가질 것입니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model that is effective for Korean.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well our post satisfies query intent compared to the competitor average.\n\n    The feature is calculated as:\n    (Our Post's Intent Score) / (Competitor Average Intent Score for that Query)\n    \n    This ensures each 'ours' post gets a unique value, fixing the previous error\n    where a single value was broadcast to all rows in a query group.\n    \"\"\"\n    feature_name = 'query_intent_satisfaction_vs_competitors'\n    df[feature_name] = np.nan # Initialize the column\n\n    # 2. Check for an empty DataFrame to prevent errors.\n    if not df.empty:\n        # Ensure we have the necessary columns and data to proceed.\n        required_cols = ['representative_query', 'post_body', 'source']\n        if not all(col in df.columns for col in required_cols):\n            # Return df as is if columns are missing\n            return df\n\n        # Create a working copy to avoid SettingWithCopyWarning\n        temp_df = df.copy()\n\n        # 3. Preprocess text data for encoding.\n        temp_df['representative_query'] = temp_df['representative_query'].fillna('').astype(str)\n        temp_df['post_body'] = temp_df['post_body'].fillna('').astype(str)\n\n        # Separate 'ours' and 'competitor' posts for clear logic\n        ours_df = temp_df[temp_df['source'] == 'ours'].copy()\n        competitors_df = temp_df[temp_df['source'] == 'competitor'].copy()\n\n        if ours_df.empty or competitors_df.empty:\n            # If there's no data for comparison, return the original df\n            return df\n\n        model = get_model()\n\n        # 4. Use efficient batch processing for all text data.\n        queries_list = temp_df['representative_query'].tolist()\n        bodies_list = temp_df['post_body'].tolist()\n\n        query_embeddings = model.encode(queries_list, convert_to_tensor=True, show_progress_bar=False)\n        body_embeddings = model.encode(bodies_list, convert_to_tensor=True, show_progress_bar=False)\n\n        # 5. Calculate individual intent satisfaction score (Similarity(Query, Body)) for every post.\n        # .diag() correctly extracts the similarity of each query with its corresponding body.\n        all_satisfaction_scores = util.cos_sim(query_embeddings, body_embeddings).diag()\n        temp_df['intent_satisfaction'] = all_satisfaction_scores.cpu().numpy()\n\n        # 6. Calculate the competitor benchmark: the average satisfaction score per query.\n        # This creates a Series mapping each query to the mean score of its competitor posts.\n        competitor_avg_by_query = temp_df[temp_df['source'] == 'competitor'].groupby('representative_query')['intent_satisfaction'].mean()\n\n        # 7. Map the competitor benchmark to each row in the DataFrame.\n        temp_df['competitor_avg_satisfaction'] = temp_df['representative_query'].map(competitor_avg_by_query)\n\n        # 8. Calculate the final feature *only for our posts*.\n        # This logic ensures each of our posts gets a unique feature value relative to the competitor average.\n        our_rows_mask = temp_df['source'] == 'ours'\n        our_scores = temp_df.loc[our_rows_mask, 'intent_satisfaction']\n        competitor_benchmark = temp_df.loc[our_rows_mask, 'competitor_avg_satisfaction']\n\n        # Calculate the ratio\n        ratio = our_scores / competitor_benchmark\n\n        # Handle edge cases: division by zero (inf) or no competitors for a query (NaN).\n        # Replace these with 1.0, a neutral value indicating 'on par' with competitors.\n        final_values = ratio.replace([np.inf, -np.inf], np.nan).fillna(1.0)\n\n        # 9. Assign the calculated values back to the original DataFrame using index alignment.\n        df.loc[final_values.index, feature_name] = final_values\n\n    # 10. Return the full DataFrame with the new feature column.\n    return df",
  "analysis": {
    "correlation": 0.09581520409852558,
    "p_value": 0.3556657645219605,
    "interpretation": "약한 양의 상관관계(0.0958)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3557)."
  }
}{
  "timestamp": "2025-07-22T04:32:27.052245",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_query_satisfaction_score",
  "hypothesis": "포스트가 경쟁자 그룹의 평균보다 대표 검색어의 의미(검색 의도)에 더 가까운 내용을 포함할수록, 즉 검색어와 본문 간의 의미론적 유사도가 경쟁자 평균보다 높을수록, 비브랜드 유입량이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean text\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well a post satisfies query intent compared to its competitors.\n\n    For each representative_query, this feature computes:\n    (Similarity of 'our' post to query) / (Average similarity of 'competitor' posts to query)\n\n    A value > 1 means our post is semantically closer to the query than the competitor average.\n    \"\"\"\n    # 2. Check for empty DataFrame to prevent errors.\n    if df.empty:\n        return df\n\n    feature_name = 'relative_query_satisfaction_score'\n    df[feature_name] = np.nan  # Initialize the feature column\n\n    model = get_model()\n\n    # 3. Process data in batches for efficiency. Avoid .apply()\n    # Ensure data types are correct and handle potential NaNs.\n    unique_queries = df['representative_query'].dropna().unique().tolist()\n    post_bodies = df['post_body'].fillna('').astype(str).tolist()\n\n    if not unique_queries:\n        return df # Return if there are no queries to process\n\n    # Encode all unique queries and all post bodies in two efficient batches.\n    query_embeddings = model.encode(unique_queries, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(post_bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # Create a mapping from query text to its embedding tensor for quick lookup.\n    query_to_embedding = {query: emb for query, emb in zip(unique_queries, query_embeddings)}\n\n    # 4. Group by the representative query to analyze each competitive set.\n    # This preserves the original DataFrame index within each group.\n    grouped_by_query = df.groupby('representative_query')\n\n    for query, group_df in grouped_by_query:\n        if query not in query_to_embedding:\n            continue\n\n        # Get the pre-computed embedding for the current query.\n        query_embedding = query_to_embedding[query]\n\n        # 5. [CRITICAL] Correctly map DataFrame indices to tensor positions.\n        # This prevents the silent misalignment of features to the wrong rows.\n        group_original_indices = group_df.index\n        group_tensor_positions = df.index.get_indexer(group_original_indices)\n\n        # Slice the main body_embeddings tensor to get embeddings for this group only.\n        group_body_embeddings = body_embeddings[group_tensor_positions]\n\n        # Calculate cosine similarity between the single query and all bodies in the group.\n        similarities = util.cos_sim(query_embedding, group_body_embeddings).flatten().cpu().numpy()\n\n        # Map similarity scores back to the group's original indices for safe filtering.\n        sim_series = pd.Series(similarities, index=group_original_indices)\n\n        # Identify 'ours' and 'competitor' posts within the group.\n        our_post_mask = (group_df['source'] == 'ours')\n        competitor_post_mask = (group_df['source'] == 'competitor')\n\n        our_indices = group_df[our_post_mask].index\n        competitor_indices = group_df[competitor_post_mask].index\n\n        # Proceed only if there's an 'ours' post to calculate the feature for.\n        if our_indices.empty:\n            continue\n\n        # Safely get the score for our post.\n        our_satisfaction_score = sim_series.loc[our_indices].mean()\n\n        # Calculate the average score for the competitor benchmark.\n        if not competitor_indices.empty:\n            competitor_avg_satisfaction = sim_series.loc[competitor_indices].mean()\n        else:\n            competitor_avg_satisfaction = np.nan # No competitors for this query\n\n        # Calculate the final ratio. Handle division by zero or missing competitors.\n        if pd.notna(competitor_avg_satisfaction) and competitor_avg_satisfaction > 0:\n            final_score = our_satisfaction_score / competitor_avg_satisfaction\n        else:\n            # If no competitors or their avg score is 0, our post is on-par with a non-existent/zero benchmark.\n            final_score = 1.0\n\n        # 6. Assign the result to the correct row(s) in the original DataFrame.\n        df.loc[our_indices, feature_name] = final_score\n\n    return df",
  "analysis": {
    "correlation": 0.09720686672312812,
    "p_value": 0.34868940054060854,
    "interpretation": "약한 양의 상관관계(0.0972)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3487)."
  }
}{
  "timestamp": "2025-07-22T04:56:51.556086",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_query_fulfillment_score",
  "hypothesis": "우리 포스트의 내용이 '대표 검색어'의 의미적 의도를 경쟁자 그룹의 평균보다 더 잘 충족시킬수록(즉, 시맨틱 유사도가 높을수록), 사용자의 검색 만족도가 높아져 비브랜드 유입(non_brand_inflow)이 증가할 것이다. 이 피처는 우리 콘텐츠가 경쟁사와 비교하여 얼마나 더 쿼리에 직접적으로 부합하는지를 정량화한다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Analyzes the relationship between post content and its representative query.\n    Calculates a score for how well a post satisfies its query's intent,\n    relative to the average competitor performance for the same query.\n    \"\"\"\n    feature_name = 'relative_query_fulfillment_score'\n    \n    # 4. ALWAYS use if not df.empty: to check for empty DataFrames.\n    if not df.empty and all(col in df.columns for col in ['post_body', 'representative_query', 'source']):\n        df_copy = df.copy()\n        \n        # Ensure required columns have no NaNs for encoding\n        df_copy['post_body'] = df_copy['post_body'].fillna('').astype(str)\n        df_copy['representative_query'] = df_copy['representative_query'].fillna('').astype(str)\n\n        model = get_model()\n\n        # --- Step 1: Batch encode all post bodies and unique queries --- \n        # 2. Use efficient batch processing\n        all_bodies = df_copy['post_body'].tolist()\n        body_embeddings = model.encode(all_bodies, convert_to_tensor=True, show_progress_bar=False)\n\n        unique_queries = df_copy['representative_query'].unique()\n        query_embeddings = model.encode(unique_queries, convert_to_tensor=True, show_progress_bar=False)\n        query_to_embedding = {query: emb for query, emb in zip(unique_queries, query_embeddings)}\n\n        # --- Step 2: Calculate individual satisfaction score for each post --- \n        # Create a tensor of query embeddings that aligns with each row of the dataframe\n        # This is a critical step for efficient, vectorized calculation.\n        aligned_query_embeddings = torch.stack([query_to_embedding[q] for q in df_copy['representative_query']])\n        \n        # Calculate cosine similarity between each post_body and its corresponding representative_query\n        # .diag() extracts the element-wise similarities from the resulting similarity matrix.\n        similarities = util.cos_sim(body_embeddings, aligned_query_embeddings).diag()\n        df_copy['temp_fulfillment_score'] = similarities.cpu().numpy()\n\n        # --- Step 3: Calculate the competitor benchmark for each query group --- \n        # Create a temporary column for competitor scores only\n        df_copy['competitor_score'] = np.where(df_copy['source'] == 'competitor', df_copy['temp_fulfillment_score'], np.nan)\n        \n        # Calculate the mean competitor score for each query group and broadcast it\n        # This efficiently applies the group's average to every row within that group.\n        group_avg_map = df_copy.groupby('representative_query')['competitor_score'].transform('mean')\n        df_copy['competitor_avg_fulfillment'] = group_avg_map\n        \n        # Handle groups with no competitors. In such cases, the benchmark is the post's own score, making the ratio 1.\n        # This is a neutral and robust fallback.\n        df_copy['competitor_avg_fulfillment'].fillna(df_copy['temp_fulfillment_score'], inplace=True)\n        \n        # --- Step 4: Calculate the final relative feature --- \n        # Calculate the ratio of the post's score to the competitor average\n        # Add a small epsilon to the denominator to prevent division by zero\n        with np.errstate(divide='ignore', invalid='ignore'):\n            df_copy[feature_name] = df_copy['temp_fulfillment_score'] / (df_copy['competitor_avg_fulfillment'])\n        \n        # If the benchmark was 0 and the score was also 0, the result is NaN. Fill with 1 (neutral).\n        # If the benchmark was 0 but the score was > 0, the result is inf. Replace with a large but finite number.\n        df_copy[feature_name].replace([np.inf, -np.inf], 2.0, inplace=True) # Cap the ratio at 2x better\n        df_copy[feature_name].fillna(1.0, inplace=True)\n\n        # 3. Return the full DataFrame with the new column.\n        # Drop temporary columns before returning\n        df[feature_name] = df_copy[feature_name]\n        return df\n\n    # If df is empty or columns are missing, return original df with a default value\n    df[feature_name] = 1.0\n    return df",
  "analysis": {
    "correlation": 0.09581522793548534,
    "p_value": 0.35566564429118275,
    "interpretation": "약한 양의 상관관계(0.0958)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3557)."
  }
}{
  "timestamp": "2025-07-22T05:17:15.891780",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_query_fulfillment_score",
  "hypothesis": "우리 포스트의 내용이 대표 검색어와의 의미적 유사성이 경쟁자 그룹의 평균보다 높을수록, 검색 의도를 더 잘 충족시켜 'non_brand_inflow'가 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # A good multilingual model for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well our post satisfies query intent compared to the competitor average.\n    The score is a ratio: (Our Post's Similarity to Query) / (Avg Competitor Similarity to Query).\n    \"\"\"\n    feature_name = 'relative_query_fulfillment_score'\n\n    # 0. Handle empty or invalid input\n    if df.empty or 'representative_query' not in df.columns or 'post_body' not in df.columns:\n        if feature_name not in df.columns:\n            df[feature_name] = 1.0\n        return df\n\n    # Ensure the feature column exists with a default value\n    if feature_name not in df.columns:\n        df[feature_name] = 1.0\n\n    model = get_model()\n    \n    # Create a working copy to avoid SettingWithCopyWarning\n    temp_df = df[['representative_query', 'post_body', 'source']].copy()\n    temp_df.fillna({'post_body': '', 'representative_query': ''}, inplace=True)\n    \n    # Filter out rows with no query, as they cannot be processed\n    valid_rows = temp_df[temp_df['representative_query'] != '']\n    if valid_rows.empty:\n        return df\n\n    # 1. Encode all unique queries and all post bodies efficiently\n    unique_queries = valid_rows['representative_query'].unique().tolist()\n    query_embeddings = model.encode(unique_queries, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(valid_rows['post_body'].tolist(), convert_to_tensor=True, show_progress_bar=False)\n\n    # 2. Map each post to its corresponding query embedding\n    query_to_idx = {query: i for i, query in enumerate(unique_queries)}\n    query_indices = valid_rows['representative_query'].map(query_to_idx).tolist()\n    corresponding_query_embeddings = query_embeddings[query_indices]\n\n    # 3. Calculate individual fulfillment scores (similarity between body and its query)\n    # cos_sim results in a matrix; the diagonal contains the element-wise similarities\n    fulfillment_scores = util.cos_sim(body_embeddings, corresponding_query_embeddings).diag().cpu().numpy()\n    temp_df.loc[valid_rows.index, 'fulfillment_score'] = fulfillment_scores\n\n    # 4. Calculate the average fulfillment score for competitors for each query\n    competitor_scores = temp_df[temp_df['source'] == 'competitor']\n    competitor_avg_by_query = competitor_scores.groupby('representative_query')['fulfillment_score'].mean()\n\n    # 5. Map the competitor average back to each row\n    temp_df['competitor_avg_score'] = temp_df['representative_query'].map(competitor_avg_by_query)\n\n    # If a query has no competitors, its average will be NaN. \n    # In this case, we use the post's own score as the benchmark, resulting in a neutral ratio of 1.0.\n    temp_df['competitor_avg_score'].fillna(temp_df['fulfillment_score'], inplace=True)\n\n    # 6. Calculate the final relative score\n    # Use a small epsilon to prevent division by zero\n    epsilon = 1e-9\n    relative_scores = temp_df['fulfillment_score'] / (temp_df['competitor_avg_score'] + epsilon)\n\n    # 7. Assign the calculated scores back to the original DataFrame\n    # Replace any potential inf/-inf/nan with a neutral 1.0\n    df.loc[valid_rows.index, feature_name] = relative_scores\n    df[feature_name].replace([np.inf, -np.inf], 1.0, inplace=True)\n    df[feature_name].fillna(1.0, inplace=True)\n\n    return df\n",
  "analysis": {
    "correlation": 0.09581522799992363,
    "p_value": 0.35566564396616374,
    "interpretation": "약한 양의 상관관계(0.0958)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3557)."
  }
}{
  "timestamp": "2025-07-22T05:39:20.627505",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_query_intent_fulfillment",
  "hypothesis": "우리 포스트의 내용이 대표 검색어의 의미와 더 강하게 연결될수록(semantic similarity), 그리고 그 연결성이 동일 검색어에 대한 경쟁자 그룹의 평균적인 연결성보다 높을수록, 검색 의도를 더 잘 충족시켜 'non_brand_inflow'가 증가할 것입니다. 이 피처는 각 검색어별 경쟁 환경을 고려한 상대적인 의미 충족도를 측정합니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a well-regarded multilingual model for general semantic tasks.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well our post fulfills the query's intent relative to the competitor average.\n    \n    The feature is calculated as:\n    (Our Post's Query-Body Similarity) / (Average Competitor Query-Body Similarity)\n    for each representative_query group.\n    \"\"\"\n    feature_name = 'relative_query_intent_fulfillment'\n    \n    # 2. Guard clauses for empty DataFrame or if feature already exists.\n    if df.empty:\n        return df\n    if feature_name in df.columns:\n        return df\n        \n    required_cols = ['representative_query', 'post_body', 'source']\n    if not all(col in df.columns for col in required_cols):\n        # Return original df with an empty feature column if requirements are not met.\n        df[feature_name] = np.nan\n        return df\n\n    # Create a working copy to avoid SettingWithCopyWarning.\n    df_copy = df.copy()\n\n    # 3. Initialize the model.\n    model = get_model()\n    \n    # 4. Prepare data and perform efficient batch encoding.\n    queries = df_copy['representative_query'].fillna('').astype(str).tolist()\n    bodies = df_copy['post_body'].fillna('').astype(str).tolist()\n\n    # Encode everything in batches for performance.\n    query_embeddings = model.encode(queries, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n    \n    # 5. Calculate base intent satisfaction score for every post.\n    # This is the cosine similarity between each post's query and its own body.\n    # The .diag() method efficiently extracts the similarity of query_i to body_i for all i.\n    intent_satisfaction_scores = util.cos_sim(query_embeddings, body_embeddings).diag().cpu().numpy()\n    df_copy['satisfaction_score'] = intent_satisfaction_scores\n\n    # 6. Calculate the average competitor satisfaction score for each query group.\n    # First, create a series containing scores only for competitors, others are NaN.\n    is_competitor = (df_copy['source'] == 'competitors')\n    competitor_scores = df_copy['satisfaction_score'].where(is_competitor)\n\n    # Now, group by query and use transform('mean'). \n    # This computes the mean for each group (ignoring NaNs) and broadcasts the result \n    # to all rows belonging to that group. This is a highly efficient, vectorized operation.\n    df_copy['competitor_avg_satisfaction'] = competitor_scores.groupby(df_copy['representative_query']).transform('mean')\n\n    # 7. Calculate the final feature: the ratio of our score to the competitor average.\n    our_scores = df_copy['satisfaction_score']\n    competitor_avg = df_copy['competitor_avg_satisfaction']\n    \n    # Add a small epsilon to the denominator to prevent division-by-zero errors.\n    relative_fulfillment = our_scores / (competitor_avg + 1e-9)\n    \n    # Handle edge cases:\n    # - If a query group has no competitors, competitor_avg is NaN, so ratio is NaN.\n    # - If competitor_avg is 0, ratio is inf.\n    # We map these cases to 1.0, signifying our post is on par with the (non-existent or zero-score) competition.\n    relative_fulfillment = relative_fulfillment.replace([np.inf, -np.inf], 1.0).fillna(1.0)\n    \n    # 8. Assign the results back to the original DataFrame.\n    # Only 'ours' posts should have this feature value. Others get NaN.\n    is_ours = (df['source'] == 'ours')\n    df[feature_name] = np.nan\n    # Use .loc with the boolean mask to safely assign values to the original DataFrame.\n    df.loc[is_ours, feature_name] = relative_fulfillment[is_ours]\n    \n    return df\n",
  "analysis": {
    "correlation": 0.10356920469306391,
    "p_value": 0.317894858228946,
    "interpretation": "약한 양의 상관관계(0.1036)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3179)."
  }
}{
  "timestamp": "2025-07-22T05:59:48.629552",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_query_body_cohesion",
  "hypothesis": "우리 포스트의 내용이 대표 검색어의 의도와 의미적으로 더 밀접하게 연결될수록(즉, '질의-본문 일관성'이 높을수록), 경쟁사 평균보다 높은 일관성을 보일 때 비브랜드 유입이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Use a reliable multilingual model for Korean and general queries.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the 'Query-Body Cohesion' for each post relative to its competitors.\n    \n    1.  For every post, it calculates a 'cohesion score' which is the cosine similarity\n        between its 'representative_query' and its 'post_body'.\n    2.  For each query group, it calculates the average cohesion score of all competitor posts.\n    3.  The final feature for an 'ours' post is the ratio of its own cohesion score to\n        the competitors' average score for that same query.\n    \"\"\"\n    # Ensure the function can handle empty or near-empty inputs.\n    if df.empty or 'representative_query' not in df.columns or 'post_body' not in df.columns:\n        return df\n\n    feature_name = \"relative_query_body_cohesion\"\n    \n    # Initialize the feature column with a default value of 1.0.\n    # This implies parity with competitors when a comparison isn't possible (e.g., no competitors).\n    df[feature_name] = 1.0\n\n    # Ensure text columns are strings and handle potential missing values.\n    queries = df['representative_query'].fillna('').astype(str).tolist()\n    bodies = df['post_body'].fillna('').astype(str).tolist()\n\n    model = get_model()\n\n    # Batch encode all queries and bodies for maximum efficiency.\n    query_embeddings = model.encode(queries, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # Calculate the cohesion score for each post by getting the diagonal of the similarity matrix.\n    # This efficiently computes sim(query_i, body_i) for all i.\n    cohesion_scores = util.cos_sim(query_embeddings, body_embeddings).diag()\n    df['temp_cohesion_score'] = cohesion_scores.cpu().numpy()\n\n    # Identify competitor posts to create a benchmark.\n    competitor_mask = df['source'] == 'competitor'\n    \n    # Proceed only if there are competitors in the dataframe to create a benchmark from.\n    if competitor_mask.any():\n        # Calculate the average cohesion score for competitors within each query group.\n        # This creates a mapping from each query to its average competitor score.\n        competitor_avg_map = df[competitor_mask].groupby('representative_query')['temp_cohesion_score'].mean()\n    \n        # Map the competitor average score back to each row in the main DataFrame.\n        df['temp_competitor_avg'] = df['representative_query'].map(competitor_avg_map)\n\n        # Define the conditions for a valid calculation:\n        # 1. The post must be one of 'ours'.\n        # 2. The post must have a valid competitor average (not NaN).\n        # 3. The competitor average must be positive to avoid division by zero or nonsensical ratios.\n        ours_mask = df['source'] == 'ours'\n        valid_denominator_mask = df['temp_competitor_avg'].notna() & (df['temp_competitor_avg'] > 0)\n        target_mask = ours_mask & valid_denominator_mask\n        \n        # For the posts that meet our criteria, calculate the relative cohesion score.\n        if target_mask.any():\n            df.loc[target_mask, feature_name] = df.loc[target_mask, 'temp_cohesion_score'] / df.loc[target_mask, 'temp_competitor_avg']\n\n        # Clean up temporary helper columns.\n        df.drop(columns=['temp_cohesion_score', 'temp_competitor_avg'], inplace=True)\n    else:\n        # If there were no competitors at all, we only need to drop the one temporary column.\n        df.drop(columns=['temp_cohesion_score'], inplace=True)\n\n    return df",
  "analysis": {
    "correlation": 0.09581520409852558,
    "p_value": 0.3556657645219605,
    "interpretation": "약한 양의 상관관계(0.0958)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3557)."
  }
}{
  "timestamp": "2025-07-22T06:17:11.039377",
  "attempt": 1,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'relative_query_satisfaction'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton instance of the SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a versatile multilingual model suitable for Korean text.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well a post satisfies the search intent of its representative query,\n    relative to the average competitor for the same query.\n\n    The feature is a ratio: (Our Post's Satisfaction Score) / (Competitors' Avg Score).\n    A value > 1 suggests our post is better at satisfying query intent than the average competitor.\n    \"\"\"\n    feature_name = 'relative_query_satisfaction'\n\n    # 2. Check for empty DataFrame to prevent errors.\n    if not df.empty:\n        # Work on a copy of the relevant subset to ensure original df and index integrity.\n        # This prevents calculation on rows where the feature is not applicable.\n        work_df = df[df['representative_query'].notna()].copy()\n\n        if not work_df.empty:\n            model = get_model()\n\n            # 3. Prepare text for embedding. Combining title and body gives a fuller representation.\n            work_df['text_for_embedding'] = work_df['post_title'].fillna('') + ' ' + work_df['post_body'].fillna('')\n\n            # 4. Efficiently encode all unique queries and all post texts in batches.\n            unique_queries = work_df['representative_query'].unique().tolist()\n            all_post_texts = work_df['text_for_embedding'].tolist()\n\n            query_embeddings = model.encode(unique_queries, convert_to_tensor=True, device='cpu')\n            post_embeddings = model.encode(all_post_texts, convert_to_tensor=True, device='cpu')\n\n            # 5. Create a mapping from query string to its embedding for quick lookup.\n            query_to_embedding_map = {query: emb for query, emb in zip(unique_queries, query_embeddings)}\n\n            # 6. Align query embeddings with each post to perform element-wise similarity calculation.\n            # This avoids a nested loop and is highly efficient.\n            aligned_query_embeddings = torch.stack([query_to_embedding_map[q] for q in work_df['representative_query']])\n\n            # Calculate the cosine similarity between each post and its corresponding query.\n            work_df['satisfaction_score'] = util.cos_sim(post_embeddings, aligned_query_embeddings).diag().tolist()\n\n            # 7. Calculate the competitor benchmark for each query group.\n            competitor_mask = work_df['source'] == 'competitor'\n            competitor_avg_satisfaction = work_df[competitor_mask].groupby('representative_query')['satisfaction_score'].mean()\n\n            # Map the average competitor score back to each row in its group.\n            work_df['competitor_benchmark'] = work_df['representative_query'].map(competitor_avg_satisfaction)\n\n            # 8. Calculate the final feature ratio ONLY for 'ours' posts.\n            ours_mask = work_df['source'] == 'ours'\n            ratio = work_df.loc[ours_mask, 'satisfaction_score'] / work_df.loc[ours_mask, 'competitor_benchmark']\n            work_df.loc[ours_mask, feature_name] = ratio\n\n            # 9. Safely update the original DataFrame with the new feature column.\n            # .update() aligns on the index, which was preserved.\n            df.update(work_df[[feature_name]])\n\n    # 10. Handle edge cases and set a neutral default for missing values.\n    # - Competitors get NaN by default.\n    # - 'ours' posts with no competitors get NaN.\n    # - 'ours' posts with competitor benchmark of 0 get inf.\n    # A value of 1.0 implies performance is on par with the (non-existent or zero-scoring) competition.\n    if feature_name in df.columns:\n        df[feature_name].replace([np.inf, -np.inf], 1.0, inplace=True)\n        df[feature_name].fillna(1.0, inplace=True)\n    else:\n        # If the feature column was never created (e.g., all inputs were invalid), create it with the default.\n        df[feature_name] = 1.0\n\n    return df"
}{
  "timestamp": "2025-07-22T06:18:13.766959",
  "attempt": 2,
  "status": "success",
  "feature_name": "relative_query_fulfillment",
  "hypothesis": "우리 포스트의 내용이 대표 검색어와의 의미적 일치도(검색 의도 충족도로 간주)가 동일 검색어에 대한 경쟁 포스트들의 평균적인 의미적 일치도보다 높을수록, 사용자의 검색 의도를 더 잘 만족시켜 비브랜드 유입량이 증가할 것이다. 이 피처는 개별 포스트의 절대적 점수가 아닌, 경쟁 환경 내에서의 상대적 우위를 측정합니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Use a multilingual model suitable for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how well our post fulfills the query intent compared to the competitor average.\n\n    The \"fulfillment score\" is the semantic similarity between the post body and the representative query.\n    The feature value is the ratio of our post's score to the average score of its competitors for the same query.\n    This introduces variance by comparing each 'ours' post individually against its specific competitive set.\n    \"\"\"\n    # 0. Ensure the model is loaded\n    model = get_model()\n    \n    # Create the feature column, initialized with a neutral value (or NaN to be safe)\n    feature_name = 'relative_query_fulfillment'\n    df[feature_name] = np.nan\n\n    # Check for empty DataFrame to avoid errors\n    if df.empty:\n        return df\n        \n    # Ensure required columns exist and have the correct type\n    required_cols = ['representative_query', 'post_body', 'source']\n    for col in required_cols:\n        if col not in df.columns:\n            # If a required column is missing, return the df with the empty feature column\n            return df\n            \n    df['post_body'] = df['post_body'].fillna('').astype(str)\n    df['representative_query'] = df['representative_query'].fillna('').astype(str)\n\n    # Group by the representative query to process each competitive set\n    grouped = df.groupby('representative_query')\n\n    for query, group_df in grouped:\n        if not query:  # Skip groups with no query\n            continue\n\n        # Separate our posts from competitor posts using their boolean positions\n        ours_mask = (group_df['source'] == 'ours').values\n        competitors_mask = (group_df['source'] == 'competitor').values\n\n        # Proceed only if there's at least one of our posts and one competitor post\n        if not np.any(ours_mask) or not np.any(competitors_mask):\n            continue\n\n        # Combine bodies for a single batch encoding call\n        all_bodies = group_df['post_body'].tolist()\n        \n        # Encode the single query and all post bodies in the group\n        query_embedding = model.encode(query, convert_to_tensor=True)\n        body_embeddings = model.encode(all_bodies, convert_to_tensor=True)\n\n        # Calculate semantic similarity (fulfillment score) for all posts in the group\n        # Result is a tensor of shape [1, N], where N is number of posts in group. Squeeze to [N].\n        all_fulfillment_scores = util.cos_sim(query_embedding, body_embeddings).squeeze(0)\n\n        # Get the scores corresponding to competitor posts using the boolean mask\n        competitor_scores = all_fulfillment_scores[competitors_mask]\n        \n        # Calculate the average fulfillment score for competitors\n        # Add a small epsilon to avoid division by zero\n        if competitor_scores.numel() == 0:\n             competitor_avg_score = 1.0 # Should not happen due to guard clause, but for safety\n        else:\n             competitor_avg_score = competitor_scores.mean().item() + 1e-9\n\n        # Get the scores and original indices for our posts\n        ours_scores = all_fulfillment_scores[ours_mask]\n        ours_indices = group_df.index[ours_mask]\n        \n        # Calculate the relative fulfillment for each of our posts\n        # This is the key step to ensure variance: each 'ours' post gets its own score\n        relative_scores = ours_scores / competitor_avg_score\n\n        # Assign the calculated scores back to the original DataFrame using the preserved indices\n        df.loc[ours_indices, feature_name] = relative_scores.cpu().numpy()\n\n    # Fill any remaining NaNs (e.g., for competitor posts or groups without competition) with a neutral value of 1.0\n    # A value of 1.0 implies parity with competitors.\n    df[feature_name] = df[feature_name].fillna(1.0)\n    \n    return df",
  "analysis": {
    "correlation": 0.0958152759922076,
    "p_value": 0.35566540189889007,
    "interpretation": "약한 양의 상관관계(0.0958)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3557)."
  }
}{
  "timestamp": "2025-07-22T06:33:07.075431",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_query_body_cohesion",
  "hypothesis": "우리 포스트의 본문이 '대표 검색어'와 의미적으로 더 강하게 연결될수록(즉, 검색 의도를 더 잘 충족시킬수록), 경쟁사 그룹의 평균적인 연결성보다 높을 때 비브랜드 유입이 증가할 것이다. 이 피처는 우리 콘텐츠가 동일 검색어에 대한 경쟁 콘텐츠보다 얼마나 더 주제에 집중하고 있는지를 측정한다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Use a model that is effective for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic cohesion between the post body and the representative query,\n    then normalizes this score by the average cohesion of competitors for the same query.\n\n    The feature measures how much better our post satisfies the query's intent\n    compared to the competitor average.\n    \"\"\"\n    feature_name = 'relative_query_body_cohesion'\n    \n    # Ensure the new column exists with a default value.\n    if feature_name not in df.columns:\n        df[feature_name] = 1.0\n\n    # Check for empty DataFrame or necessary columns\n    if not isinstance(df, pd.DataFrame) or df.empty or 'representative_query' not in df.columns or 'post_body' not in df.columns or 'source' not in df.columns:\n        return df\n\n    model = get_model()\n\n    # Preserve the original index to ensure correct final assignment\n    original_index = df.index\n    df_copy = df.copy()\n\n    # Prepare data for encoding\n    queries = df_copy['representative_query'].fillna('').astype(str).tolist()\n    bodies = df_copy['post_body'].fillna('').astype(str).tolist()\n\n    # Encode in batches for efficiency\n    query_embeddings = model.encode(queries, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # Calculate the cosine similarity between each query and its corresponding body\n    # .diag() is used to get the pairwise similarity for each row (query_i, body_i)\n    individual_cohesion_scores = util.cos_sim(query_embeddings, body_embeddings).diag().tolist()\n    df_copy['temp_cohesion_score'] = individual_cohesion_scores\n\n    # Calculate the average cohesion score for competitors, grouped by query\n    competitor_scores = df_copy[df_copy['source'] == 'competitor'].groupby('representative_query')['temp_cohesion_score']\n    competitor_avg_scores = competitor_scores.transform('mean')\n\n    # Assign the calculated competitor average back to the competitor rows\n    df_copy.loc[competitor_avg_scores.index, 'competitor_avg_cohesion'] = competitor_avg_scores\n    \n    # Forward-fill the competitor average within each query group to apply it to 'ours' posts\n    df_copy['competitor_avg_cohesion'] = df_copy.groupby('representative_query')['competitor_avg_cohesion'].transform(lambda x: x.ffill().bfill())\n\n    # Calculate the relative cohesion score (our score / competitor average score)\n    relative_scores = df_copy['temp_cohesion_score'] / df_copy['competitor_avg_cohesion']\n\n    # Handle division by zero or NaN cases\n    # Replace inf/-inf with NaN, then fill NaN with 1.0 (a neutral value indicating our score is same as competitors)\n    relative_scores.replace([np.inf, -np.inf], np.nan, inplace=True)\n    # A query group with no competitors will result in NaN, which we fill with the neutral value 1.0\n    relative_scores.fillna(1.0, inplace=True)\n\n    # Assign the calculated feature values back to the original DataFrame using the preserved index\n    df[feature_name] = pd.Series(relative_scores.values, index=original_index)\n    \n    return df",
  "analysis": {
    "correlation": 0.09581519838640046,
    "p_value": 0.3556657933332428,
    "interpretation": "약한 양의 상관관계(0.0958)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3557)."
  }
}{
  "timestamp": "2025-07-22T06:59:33.082835",
  "attempt": 1,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'relative_intent_satisfaction_distance'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # A good multilingual model suitable for the task.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic distance from our post to the centroid of high-performing competitor posts\n    for the same representative query.\n\n    The feature measures how well a post aligns with the 'ideal' content established by\n    top-performing competitors, with a lower distance indicating better alignment.\n    \"\"\"\n    # Use a clear feature name variable for consistency.\n    feature_name = 'relative_intent_satisfaction_distance'\n\n    # 0. Handle empty DataFrame case\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        if isinstance(df, pd.DataFrame) and feature_name not in df.columns:\n            df[feature_name] = np.nan\n        return df\n\n    model = get_model()\n\n    # 2. Prepare a result DataFrame and the feature column.\n    # Using .copy() is safer and avoids SettingWithCopyWarning.\n    result_df = df.copy()\n    result_df[feature_name] = np.nan\n\n    # 3. Create a combined text column for embedding, handling potential NaN values.\n    df['text_for_embedding'] = df['post_title'].fillna('') + ' ' + df['post_body'].fillna('')\n\n    # 4. Group by the representative query to process each query context separately.\n    grouped = df.groupby('representative_query')\n\n    for _, group in grouped:\n        # Separate our posts from competitor posts within the group.\n        ours_df = group[group['source'] == 'ours']\n        competitors_df = group[group['source'] == 'competitor']\n\n        # Proceed only if we have our own posts and competitor posts to compare against.\n        if ours_df.empty or competitors_df.empty:\n            continue\n\n        # Define high-performing competitors as those above the median inflow for this query.\n        # This creates a more meaningful benchmark.\n        performance_threshold = competitors_df['non_brand_inflow'].quantile(0.5, interpolation='lower')\n        high_perf_competitors_df = competitors_df[competitors_df['non_brand_inflow'] > performance_threshold]\n\n        # If no high-performing competitors exist for this query, we cannot create a benchmark.\n        if high_perf_competitors_df.empty:\n            continue\n\n        # 5. Get texts for batch embedding. The order is important for later slicing.\n        our_texts = ours_df['text_for_embedding'].tolist()\n        competitor_texts = high_perf_competitors_df['text_for_embedding'].tolist()\n        \n        # Efficiently encode all relevant texts in a single batch.\n        all_embeddings = model.encode(our_texts + competitor_texts, convert_to_tensor=True)\n\n        # 6. Slice the embeddings tensor based on the known order.\n        # This is the correct way to handle tensors, using integer positions.\n        our_embeddings = all_embeddings[:len(our_texts)]\n        competitor_embeddings = all_embeddings[len(our_texts):]\n\n        # 7. Calculate the semantic centroid (average embedding) of high-performing competitors.\n        # This represents the 'ideal' content for the query.\n        ideal_centroid = competitor_embeddings.mean(dim=0)\n\n        # 8. Calculate the cosine similarity and convert it to distance (1 - similarity).\n        similarities = util.cos_sim(our_embeddings, ideal_centroid)\n        # .diag() is used to get the similarity of each post to the centroid.\n        # Then convert to a simple list of distances.\n        distances = (1 - similarities.diag()).cpu().tolist()\n        \n        # 9. Assign the calculated feature value back to the correct original indices.\n        # This preserves the original DataFrame's structure and prevents data misalignment.\n        our_indices = ours_df.index\n        result_df.loc[our_indices, feature_name] = distances\n\n    return result_df"
}{
  "timestamp": "2025-07-22T07:00:30.209032",
  "attempt": 2,
  "status": "success",
  "feature_name": "relational_query_fulfillment_score",
  "hypothesis": "우리 포스트가 동일 대표 검색어에 대한 경쟁자 포스트들의 평균적인 의도 충족도보다 더 높은 수준으로 검색 의도를 충족시킬수록(즉, 대표 검색어와 우리 포스트 본문 간의 의미적 유사도가 경쟁자들의 평균 유사도보다 높을수록), 비브랜드 유입량이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean text\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score for each 'ours' post indicating how well it fulfills the query intent\n    relative to the average competitor for the same query.\n\n    A score > 1 means our post is semantically closer to the query than the competitor average.\n    A score < 1 means our post is semantically farther from the query than the competitor average.\n    \"\"\"\n    feature_name = 'relational_query_fulfillment_score'\n\n    # 2. Handle empty DataFrame to avoid errors and ensure column exists.\n    if not df.empty:\n        pass\n    else:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n\n    model = get_model()\n\n    # 3. Initialize the feature column with a default value (NaN).\n    # This ensures all rows have the column, even if they can't be calculated.\n    df[feature_name] = np.nan\n\n    # 4. Group by the representative query to create a benchmark for each query.\n    grouped = df.groupby('representative_query')\n\n    for query, group_df in grouped:\n        # Separate 'ours' from 'competitor' posts within the group.\n        ours_df = group_df[group_df['source'] == 'ours'].copy()\n        competitors_df = group_df[group_df['source'] == 'competitor'].copy()\n\n        # Skip this group if there are no 'ours' posts to score or no 'competitors' to create a benchmark.\n        if ours_df.empty or competitors_df.empty:\n            continue\n\n        # 5. Prepare text data for batch encoding. Fill missing values to prevent errors.\n        query_text = [str(query)]\n        our_bodies = ours_df['post_body'].fillna('').astype(str).tolist()\n        competitor_bodies = competitors_df['post_body'].fillna('').astype(str).tolist()\n\n        # 6. Encode all texts in efficient batches.\n        query_embedding = model.encode(query_text, convert_to_tensor=True)\n        our_embeddings = model.encode(our_bodies, convert_to_tensor=True)\n        competitor_embeddings = model.encode(competitor_bodies, convert_to_tensor=True)\n\n        # 7. Calculate the competitor benchmark: the average similarity to the query.\n        competitor_similarities = util.cos_sim(query_embedding, competitor_embeddings)\n        # The result is a tensor of shape (1, num_competitors), so we calculate the mean.\n        avg_competitor_similarity = competitor_similarities.mean().item()\n\n        # Avoid division by zero if competitors have zero similarity (highly unlikely).\n        if avg_competitor_similarity == 0:\n            continue\n\n        # 8. Calculate the fulfillment score for each 'ours' post.\n        our_similarities = util.cos_sim(query_embedding, our_embeddings)\n        # Squeeze to remove the extra dimension, convert to a simple numpy array.\n        our_individual_scores = our_similarities.squeeze().cpu().numpy()\n\n        # 9. The final feature is the ratio of our score to the competitor average.\n        # This calculation is performed element-wise, creating a unique value for each 'ours' post.\n        relational_scores = our_individual_scores / avg_competitor_similarity\n\n        # 10. Assign the calculated scores back to the original DataFrame using the preserved index.\n        # This is the critical step to avoid misalignment and ensure each row gets its unique value.\n        df.loc[ours_df.index, feature_name] = relational_scores\n\n    return df\n",
  "analysis": {
    "correlation": 0.09581535592796145,
    "p_value": 0.3556649987128454,
    "interpretation": "약한 양의 상관관계(0.0958)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3557)."
  }
}{
  "timestamp": "2025-07-22T07:21:19.170176",
  "attempt": 1,
  "status": "success",
  "feature_name": "relational_query_fulfillment_score",
  "hypothesis": "우리 포스트의 내용이 경쟁자 포스트 그룹의 평균적인 내용보다 '대표 검색어'의 의미에 더 가까울수록(즉, 검색 의도를 더 잘 충족시킬수록), 더 높은 비브랜드 유입량을 보일 것이다. 이 피처는 우리 콘텐츠의 상대적 의미론적 우수성을 측정한다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\ndef get_model():\n    global _model\n    if _model is None:\n        # Using a well-regarded multilingual model suitable for Korean.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score representing how much better our post fulfills the search intent of a query\n    compared to the average competitor for the same query.\n\n    The score is a ratio: (Our Post's Similarity to Query) / (Avg. Competitor Similarity to Query)\n    A score > 1 indicates our post is semantically closer to the query intent than the competition.\n    \"\"\"\n    feature_name = 'relational_query_fulfillment_score'\n\n    # Check for empty DataFrame or required columns\n    if df.empty or 'representative_query' not in df.columns or 'post_body' not in df.columns or 'source' not in df.columns:\n        df[feature_name] = np.nan\n        return df\n\n    model = get_model()\n\n    # Initialize the result column with NaNs. This is a robust way to handle groups that can't be processed.\n    df[feature_name] = np.nan\n\n    # Group by the representative query to analyze each competitive landscape individually.\n    # This loop structure is clear and robust for this relational task, avoiding complex index mapping.\n    for query, group in df.groupby('representative_query'):\n        # Ensure the query is a valid string\n        if not isinstance(query, str) or not query.strip():\n            continue\n\n        our_posts = group[group['source'] == 'ours']\n        competitor_posts = group[group['source'] == 'competitor']\n\n        # The feature can only be calculated if we have our post AND at least one competitor.\n        if our_posts.empty or competitor_posts.empty:\n            continue\n\n        # Prepare texts for encoding. Handle potential NaN values in post bodies.\n        our_bodies = our_posts['post_body'].fillna('').tolist()\n        competitor_bodies = competitor_posts['post_body'].fillna('').tolist()\n\n        # Encode the query and all post bodies in the group.\n        # Batching per group is a good compromise between full vectorization and apply().\n        query_embedding = model.encode([query], convert_to_tensor=True)\n        our_embeddings = model.encode(our_bodies, convert_to_tensor=True)\n        competitor_embeddings = model.encode(competitor_bodies, convert_to_tensor=True)\n\n        # Calculate cosine similarity between the query and all post bodies.\n        # .flatten() converts the resulting tensor to a 1D tensor for easier processing.\n        our_sims = util.cos_sim(query_embedding, our_embeddings).flatten()\n        competitor_sims = util.cos_sim(query_embedding, competitor_embeddings).flatten()\n\n        # Calculate the average similarity for the competitor group.\n        avg_competitor_sim = competitor_sims.mean().item()\n\n        # To avoid division by zero or nonsensical ratios, if avg competitor similarity is 0,\n        # we cannot meaningfully calculate a ratio. We assign NaN.\n        if avg_competitor_sim == 0:\n            df.loc[our_posts.index, feature_name] = np.nan\n            continue\n\n        # Calculate the fulfillment ratio for each of our posts in the group.\n        # The loop handles cases where multiple 'ours' posts exist for the same query.\n        for i, post_index in enumerate(our_posts.index):\n            our_sim = our_sims[i].item()\n            ratio = our_sim / avg_competitor_sim\n            # Use .loc with the original index to ensure correct assignment, preventing data misalignment.\n            df.loc[post_index, feature_name] = ratio\n            \n    # As a final safety check, replace any potential infinite values with NaN.\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    return df\n",
  "analysis": {
    "correlation": 0.0958153606658091,
    "p_value": 0.35566497481573767,
    "interpretation": "약한 양의 상관관계(0.0958)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3557)."
  }
}