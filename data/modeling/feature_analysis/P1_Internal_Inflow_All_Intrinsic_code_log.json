{
  "timestamp": "2025-07-22T03:57:38.399249",
  "attempt": 1,
  "status": "success",
  "feature_name": "lexical_diversity_ratio",
  "hypothesis": "어휘의 다양성, 즉 전체 단어 수 대비 고유 형태소의 비율이 높을수록, 콘텐츠가 더 깊이 있고 전문적이라고 판단될 수 있습니다. 이러한 어휘적 풍부함은 검색 엔진에 의해 더 높은 품질의 콘텐츠로 평가받아, 결과적으로 비브랜드 검색 유입(non_brand_inflow) 증가에 기여할 것이라는 가설을 세웁니다.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    전체 단어 수 대비 고유 형태소 수의 비율을 계산하여 어휘 다양성 지표를 생성합니다.\n\n    이 지표는 콘텐츠의 어휘적 풍부함을 나타냅니다. 비율이 높을수록\n    다양한 어휘를 사용하여 내용을 풍부하게 설명하고 있음을 시사합니다.\n\n    Args:\n        df (pd.DataFrame): 'morpheme_words'와 'word_count' 컬럼을 포함하는 데이터프레임.\n\n    Returns:\n        pd.DataFrame: 'lexical_diversity_ratio' 컬럼이 추가된 데이터프레임.\n    \"\"\"\n    # 1. Check for empty DataFrame to prevent errors on empty input.\n    if df.empty:\n        df['lexical_diversity_ratio'] = pd.Series(dtype='float64')\n        return df\n\n    # 2. Ensure required columns exist.\n    if 'morpheme_words' not in df.columns or 'word_count' not in df.columns:\n        raise ValueError(\"Input DataFrame must contain 'morpheme_words' and 'word_count' columns.\")\n\n    # 3. Process morphemes efficiently without using row-wise apply.\n    # Fill NaN values with empty strings to prevent errors during split.\n    morphemes_series = df['morpheme_words'].fillna('')\n\n    # Use a list comprehension for efficient processing of unique morpheme counts.\n    # It handles empty strings correctly (len(set([''])) is 1, but we need 0 for an empty string).\n    unique_morpheme_counts = [len(set(x.split(','))) if x else 0 for x in morphemes_series]\n\n    # 4. Safely calculate the ratio, handling potential division by zero.\n    word_counts = df['word_count'].astype(float)\n    \n    # Initialize the new feature column with 0.0\n    df['lexical_diversity_ratio'] = 0.0\n    \n    # Create a mask for rows where division is safe (word_count > 0)\n    safe_division_mask = word_counts > 0\n    \n    # Calculate the ratio only for the safe rows.\n    # We convert unique_morpheme_counts to a Pandas Series to align with the mask.\n    unique_counts_series = pd.Series(unique_morpheme_counts, index=df.index)\n    df.loc[safe_division_mask, 'lexical_diversity_ratio'] = unique_counts_series[safe_division_mask] / word_counts[safe_division_mask]\n\n    # Replace any potential inf/NaN values that might arise, although the mask should prevent this.\n    df['lexical_diversity_ratio'].replace([np.inf, -np.inf], 0, inplace=True)\n    df['lexical_diversity_ratio'].fillna(0, inplace=True)\n\n    return df",
  "analysis": {
    "correlation": 0.08620472407316634,
    "p_value": 0.4061745009322977,
    "interpretation": "약한 양의 상관관계(0.0862)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.4062)."
  }
}{
  "timestamp": "2025-07-22T04:18:13.464501",
  "attempt": 1,
  "status": "success",
  "feature_name": "question_answer_score",
  "hypothesis": "포스트의 제목이 질문 형식이거나, 본문에 '방법', '이유', '해결' 등 답변형 형태소를 많이 포함할수록 사용자의 검색 의도를 직접적으로 충족시켜 비브랜드 유입(non_brand_inflow)이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\n\n# This feature does not require a heavy ML model, so lazy-loading is not necessary.\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generates a 'question_answer_score' feature.\n\n    This feature quantifies how well a post is structured as an answer to a user's question.\n    It's a combination of two components:\n    1. Title Score (0.5 if title is a question, 0 otherwise).\n    2. Content Score (0 to 0.5) based on the frequency of \"answer-type\" morphemes.\n    The final score ranges from 0 to 1.\n    \"\"\"\n    feature_name = 'question_answer_score'\n\n    # Check for empty DataFrame to avoid errors\n    if df.empty:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n\n    # Define a list of morphemes that suggest the content is an answer/explanation.\n    # (method, reason, because, result, solution, introduction, find out, curious, cause, process, case)\n    answer_morphemes = ['방법', '이유', '때문', '결과', '해결', '소개', '알아보', '궁금', '원인', '과정', '케이스']\n\n    # --- 1. Calculate Title Score ---\n    # Score is 0.5 if the title is a question, 0 otherwise.\n    # Ensure the column exists and handle potential missing values.\n    if 'title_is_question' in df.columns:\n        title_score = df['title_is_question'].fillna(0).apply(lambda x: 0.5 if x == 1 else 0)\n    else:\n        # If the column is missing, assign a neutral score of 0 to all rows.\n        title_score = pd.Series(0, index=df.index)\n    \n    # --- 2. Calculate Content Score ---\n    # Helper function to count occurrences of specific morphemes in the morpheme string.\n    def count_target_morphemes(morpheme_string: str, target_list: list) -> int:\n        if not isinstance(morpheme_string, str) or morpheme_string == '':\n            return 0\n        # Split the string of morphemes into a list and count matches.\n        morphemes = {m.strip() for m in morpheme_string.split(',')}\n        count = sum(1 for morpheme in target_list if morpheme in morphemes)\n        return count\n\n    # Helper sigmoid function to normalize the count into a 0-1 range with smooth saturation.\n    def scaled_sigmoid(x, scale=0.3, shift=5.0):\n        \"\"\"Scales the count to a 0-1 range. shift=5 means a count of 5 gives a 0.5 score.\"\"\"\n        return 1 / (1 + np.exp(-scale * (x - shift)))\n\n    # Ensure the morpheme column exists\n    if 'morpheme_words' in df.columns:\n        # Count the answer-related morphemes for each post.\n        answer_counts = df['morpheme_words'].fillna('').apply(\n            lambda x: count_target_morphemes(x, answer_morphemes)\n        )\n        \n        # Normalize the count to a score between 0 and 0.5\n        # Using .values ensures the operation is on the underlying numpy array.\n        content_score = 0.5 * scaled_sigmoid(answer_counts.values)\n    else:\n        # If morpheme data is missing, assign a neutral score of 0.\n        content_score = pd.Series(0, index=df.index)\n\n    # --- 3. Combine scores and create the final feature ---\n    # The final score is the sum of the title and content scores.\n    # Using .values ensures alignment is correct even if indices were accidentally shuffled.\n    df[feature_name] = title_score.values + content_score\n    \n    # Ensure the final score does not exceed 1.0\n    df[feature_name] = df[feature_name].clip(0, 1)\n\n    return df\n",
  "analysis": {
    "correlation": -0.0005533141661005206,
    "p_value": 0.995753962038219,
    "interpretation": "약한 음의 상관관계(-0.0006)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.9958)."
  }
}{
  "timestamp": "2025-07-22T04:36:45.509701",
  "attempt": 1,
  "status": "success",
  "feature_name": "category_keyword_density",
  "hypothesis": "포스트 본문 내에서 해당 카테고리의 핵심 키워드가 차지하는 비중(밀도)이 높을수록, 해당 주제에 대한 전문성과 집중도가 높다고 판단되어 비브랜드 유입에 긍정적인 영향을 미칠 것이다.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the density of category-specific keywords within the post body.\n\n    The density is defined as the total count of all category keywords found in the\n    post body, divided by the total word count of the post.\n    \"\"\"\n    feature_name = 'category_keyword_density'\n\n    if df.empty:\n        df[feature_name] = pd.Series(dtype='float64')\n        return df\n\n    # Ensure required columns exist, adding them with default values if they don't\n    if 'post_body' not in df.columns:\n        df['post_body'] = ''\n    if 'category_keywords' not in df.columns:\n        df['category_keywords'] = ''\n    if 'word_count' not in df.columns:\n        df['word_count'] = 0\n    \n    # Fill NaN values to prevent errors during processing\n    df['post_body'] = df['post_body'].fillna('')\n    df['category_keywords'] = df['category_keywords'].fillna('')\n    df['word_count'] = df['word_count'].fillna(0)\n\n    densities = []\n    # Use a loop as keyword lists are unique per row, making vectorization difficult.\n    for index, row in df.iterrows():\n        body = row['post_body']\n        keywords_str = row['category_keywords']\n        word_count = row['word_count']\n\n        if not body or not keywords_str or word_count == 0:\n            densities.append(0.0)\n            continue\n\n        # Parse the comma-separated keyword string into a list of clean keywords\n        keywords = [kw.strip() for kw in keywords_str.split(',') if kw.strip()]\n\n        if not keywords:\n            densities.append(0.0)\n            continue\n\n        # Sum the occurrences of each keyword in the post body\n        total_keyword_count = sum(body.count(kw) for kw in keywords)\n        \n        # Calculate density and handle potential division by zero\n        density = total_keyword_count / word_count\n        densities.append(density)\n\n    # Assign the calculated densities back to the DataFrame\n    df[feature_name] = densities\n\n    return df",
  "analysis": {
    "correlation": -0.012974303750445813,
    "p_value": 0.900690575686882,
    "interpretation": "약한 음의 상관관계(-0.0130)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.9007)."
  }
}{
  "timestamp": "2025-07-22T04:58:23.358763",
  "attempt": 1,
  "status": "success",
  "feature_name": "avg_morpheme_length",
  "hypothesis": "우리 '건강·의학' 포스트의 경우, 평균 형태소 길이가 길수록 '임플란트' 대 '이'와 같이 더 전문적이고 복잡한 용어를 사용했음을 나타냅니다. 이러한 전문적인 언어는 더 깊이 있는 콘텐츠를 시사하며, 이는 전문 정보를 찾는 사용자 및 검색 엔진에 의해 더 높게 평가받아 'non_brand_inflow'를 증가시킬 것이라는 가설을 세웁니다.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    각 포스트의 평균 형태소 길이를 계산합니다.\n\n    이 기능은 긴 형태소가 종종 더 복잡하거나 전문적인 용어(예: '임플란트' 대 '이')를\n    나타낸다는 아이디어에 기반합니다. 평균 길이가 길수록 더 깊이 있고 전문적인 콘텐츠를\n    나타낼 수 있습니다.\n    \"\"\"\n    # 1. 데이터프레임이 비어 있는지 확인합니다.\n    if not df.empty and 'morpheme_words' in df.columns:\n        \n        # 2. 형태소 문자열에서 평균 길이를 계산하는 헬퍼 함수를 정의합니다.\n        def calculate_avg_len(morpheme_string):\n            # 입력값이 문자열이 아니거나 비어 있으면 0.0을 반환합니다.\n            if not isinstance(morpheme_string, str) or not morpheme_string.strip():\n                return 0.0\n\n            # 문자열을 쉼표로 분리하여 형태소 목록을 만듭니다.\n            # 분리 후 각 항목의 양쪽 공백을 제거하고, 비어있는 항목은 제외합니다.\n            morphemes = [m.strip() for m in morpheme_string.split(',') if m.strip()]\n\n            # 유효한 형태소가 없으면 0.0을 반환합니다.\n            if not morphemes:\n                return 0.0\n\n            # 모든 형태소의 총 길이와 개수를 계산합니다.\n            total_length = sum(len(m) for m in morphemes)\n            morpheme_count = len(morphemes)\n\n            # 평균 길이를 계산하여 반환합니다. (0으로 나누기 방지)\n            return total_length / morpheme_count if morpheme_count > 0 else 0.0\n\n        # 3. 'morpheme_words' 열에 함수를 적용하여 새로운 피처를 생성합니다.\n        # NaN 값을 빈 문자열로 채워 오류를 방지합니다.\n        df['avg_morpheme_length'] = df['morpheme_words'].fillna('').apply(calculate_avg_len)\n    \n    # 4. 피처가 생성되지 않은 경우를 대비해 0으로 채웁니다.\n    elif 'avg_morpheme_length' not in df.columns:\n        df['avg_morpheme_length'] = 0.0\n\n    return df",
  "analysis": {
    "correlation": -0.0392152779983079,
    "p_value": 0.7059443730803595,
    "interpretation": "약한 음의 상관관계(-0.0392)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.7059)."
  }
}{
  "timestamp": "2025-07-22T05:18:37.623997",
  "attempt": 1,
  "status": "success",
  "feature_name": "content_engagement_index",
  "hypothesis": "우리 포스트 내에서 단어 수가 많아 내용이 풍부하면서도, 가독성 점수가 높고 이미지가 적절히 사용되어 독자의 이탈을 막는 콘텐츠가 더 높은 사용자 참여를 유도하고, 결과적으로 더 높은 `non_brand_inflow`를 기록할 것이다. 이 복합적인 품질을 '콘텐츠 참여 지수'로 측정할 수 있다.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Creates a 'Content Engagement Index' by combining word count, readability, and image count.\n    The formula is: (word_count * readability_score) / (valid_image_count + 1).\n    This feature hypothesizes that content which is both substantial and easy to digest\n    (due to good readability and visual breaks) correlates with higher user engagement and inflow.\n    \"\"\"\n    # Ensure the DataFrame is not empty\n    if df.empty:\n        # Add the feature column with a default value if it doesn't exist\n        if 'content_engagement_index' not in df.columns:\n            df['content_engagement_index'] = 0\n        return df\n\n    # Check for required columns\n    required_cols = ['word_count', 'readability_score', 'valid_image_count']\n    for col in required_cols:\n        if col not in df.columns:\n            df['content_engagement_index'] = 0\n            return df\n\n    # Impute missing readability scores with the column's mean to avoid penalizing them unfairly.\n    # If the entire column is NaN, the mean will also be NaN, so we fillna(0) as a fallback.\n    readability_imputed = df['readability_score'].fillna(df['readability_score'].mean()).fillna(0)\n\n    # Fill NaN values in word_count and valid_image_count with 0.\n    word_count_filled = df['word_count'].fillna(0)\n    # Add 1 to the image count to prevent division by zero errors.\n    image_count_safe = df['valid_image_count'].fillna(0) + 1\n\n    # Calculate the engagement index\n    # Use .astype(float) to prevent potential integer overflow on large values\n    numerator = word_count_filled.astype(float) * readability_imputed.astype(float)\n    denominator = image_count_safe.astype(float)\n    \n    df['content_engagement_index'] = numerator / denominator\n\n    # Clean up any potential inf or NaN values that might arise from the calculation\n    df['content_engagement_index'].replace([np.inf, -np.inf], np.nan, inplace=True)\n    df['content_engagement_index'].fillna(0, inplace=True)\n\n    return df",
  "analysis": {
    "correlation": 0.06667368868521494,
    "p_value": 0.5208948011793133,
    "interpretation": "약한 양의 상관관계(0.0667)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.5209)."
  }
}{
  "timestamp": "2025-07-22T05:41:44.386186",
  "attempt": 1,
  "status": "success",
  "feature_name": "morpheme_density",
  "hypothesis": "단순히 글자 수가 많은 것보다, 단어 하나에 더 많은 형태소가 포함된, 즉 '형태소 밀도'가 높은 글이 더 구체적이고 전문적인 내용을 담고 있을 가능성이 높다. 이러한 전문성은 검색 엔진과 사용자에게 고품질 콘텐츠로 인식되어, 결과적으로 `non_brand_inflow`를 높이는 핵심 요인이 될 것이다.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the morpheme density, defined as the number of morphemes per word.\n\n    This feature aims to quantify the linguistic complexity or specificity of the content.\n    A higher density might suggest the use of more complex, specific terminology,\n    which could be a proxy for content depth and expertise.\n\n    Args:\n        df (pd.DataFrame): The input dataframe, expected to contain\n                           'morpheme_count' and 'word_count' columns.\n\n    Returns:\n        pd.DataFrame: The dataframe with the new 'morpheme_density' column added.\n    \"\"\"\n    # Ensure the dataframe is not empty to avoid errors on empty slices.\n    if df.empty:\n        return df\n\n    # Ensure required columns exist, adding them with 0 if they don't.\n    if 'morpheme_count' not in df.columns:\n        df['morpheme_count'] = 0\n    if 'word_count' not in df.columns:\n        df['word_count'] = 0\n\n    # Calculate morpheme density. \n    # Use np.divide for safe division, returning 0 where word_count is 0.\n    df['morpheme_density'] = np.divide(\n        df['morpheme_count'].astype(float),\n        df['word_count'].astype(float),\n        out=np.zeros_like(df['morpheme_count'], dtype=float),\n        where=(df['word_count'] != 0)\n    )\n\n    # Replace any potential inf values with 0, although np.divide should prevent this.\n    df['morpheme_density'].replace([np.inf, -np.inf], 0, inplace=True)\n\n    return df",
  "analysis": {
    "correlation": 0.09546119648688892,
    "p_value": 0.35745411748741696,
    "interpretation": "약한 양의 상관관계(0.0955)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3575)."
  }
}{
  "timestamp": "2025-07-22T06:01:32.516254",
  "attempt": 1,
  "status": "success",
  "feature_name": "content_effort_index",
  "hypothesis": "For our own content, posts that demonstrate higher creation 'effort', as measured by a normalized combination of word count and the number of valid images, will provide more value to the user, resulting in higher non-brand inflow.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a 'Content Effort Index' based on the normalized word count and\n    valid image count of a post.\n    \"\"\"\n    # 1. Check for empty DataFrame to avoid errors on empty inputs.\n    if df.empty:\n        df['content_effort_index'] = np.nan\n        return df\n\n    # 2. Safely extract and clean the source columns.\n    # pd.to_numeric handles non-numeric data, and .fillna(0) handles missing values.\n    word_count = pd.to_numeric(df['word_count'], errors='coerce').fillna(0)\n    image_count = pd.to_numeric(df['valid_image_count'], errors='coerce').fillna(0)\n\n    # 3. Normalize features using Min-Max scaling to a common [0, 1] range.\n    # This is crucial because word count (e.g., 1000s) and image count (e.g., 10s)\n    # are on vastly different scales.\n\n    # Normalize word_count\n    min_wc = word_count.min()\n    max_wc = word_count.max()\n    # Ensure we don't divide by zero if all values are the same.\n    if (max_wc - min_wc) > 0:\n        scaled_word_count = (word_count - min_wc) / (max_wc - min_wc)\n    else:\n        scaled_word_count = pd.Series(0.0, index=df.index)\n\n    # Normalize image_count\n    min_ic = image_count.min()\n    max_ic = image_count.max()\n    # Ensure we don't divide by zero if all values are the same.\n    if (max_ic - min_ic) > 0:\n        scaled_image_count = (image_count - min_ic) / (max_ic - min_ic)\n    else:\n        scaled_image_count = pd.Series(0.0, index=df.index)\n\n    # 4. Combine the normalized scores into a single index.\n    # A simple average gives equal weight to text length and visual content effort.\n    df['content_effort_index'] = (scaled_word_count + scaled_image_count) / 2.0\n\n    return df",
  "analysis": {
    "correlation": -0.04212908006375103,
    "p_value": 0.6852073231772129,
    "interpretation": "약한 음의 상관관계(-0.0421)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.6852)."
  }
}{
  "timestamp": "2025-07-22T06:20:19.626316",
  "attempt": 1,
  "status": "success",
  "feature_name": "core_morpheme_title_presence",
  "hypothesis": "포스트 본문의 핵심 형태소(가장 빈도가 높은 N개)가 제목에 포함된 비율이 높을수록, 제목이 내용을 잘 요약하여 사용자의 검색 의도와 일치할 확률이 높아지므로, 결과적으로 비브랜드 유입(non_brand_inflow)이 증가할 것이다.",
  "code": "import pandas as pd\nfrom collections import Counter\nimport re\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the ratio of core body morphemes present in the post title.\n\n    The feature 'core_morpheme_title_presence' represents how well the title\n    summarizes the main topics of the body text. It is calculated by:\n    1. Identifying the top 10 most frequent morphemes (longer than one character)\n       from the post body ('core morphemes').\n    2. Counting how many of these core morphemes appear in the post title.\n    3. Normalizing this count by the total number of core morphemes (usually 10).\n\n    A higher score suggests a strong alignment between the title and the content's\n    key concepts.\n    \"\"\"\n    if not 'morpheme_words' in df.columns or not 'post_title' in df.columns:\n        # If essential columns are missing, return the original DataFrame\n        # with a new column of zeros or NaNs to avoid downstream errors.\n        df['core_morpheme_title_presence'] = 0.0\n        return df\n\n    if df.empty:\n        df['core_morpheme_title_presence'] = pd.Series(dtype=float)\n        return df\n\n    # Use a copy to avoid SettingWithCopyWarning\n    df_copy = df.copy()\n\n    # Pre-process relevant columns to handle potential NaNs\n    df_copy['morpheme_words'] = df_copy['morpheme_words'].fillna('').astype(str)\n    df_copy['post_title'] = df_copy['post_title'].fillna('').astype(str)\n\n    scores = []\n    # Use itertuples for efficient row-wise iteration\n    for row in df_copy.itertuples(index=False):\n        title = row.post_title\n        body_morphemes_str = row.morpheme_words\n\n        if not body_morphemes_str or not title:\n            scores.append(0.0)\n            continue\n\n        # 1. Split morphemes and filter out short, likely grammatical, ones\n        all_morphemes = body_morphemes_str.split(', ')\n        content_morphemes = [m for m in all_morphemes if len(m) > 1 and m]\n        \n        if not content_morphemes:\n            scores.append(0.0)\n            continue\n\n        # 2. Find the 10 most common morphemes as 'core' morphemes\n        counter = Counter(content_morphemes)\n        core_morphemes_with_counts = counter.most_common(10)\n        core_morphemes = {morpheme for morpheme, count in core_morphemes_with_counts}\n\n        if not core_morphemes:\n            scores.append(0.0)\n            continue\n\n        # 3. Count how many core morphemes are present in the title\n        matches = sum(1 for morpheme in core_morphemes if morpheme in title)\n\n        # 4. Calculate the ratio\n        score = matches / len(core_morphemes)\n        scores.append(score)\n\n    # Assign the new feature to the original DataFrame using its index\n    df['core_morpheme_title_presence'] = pd.Series(scores, index=df.index)\n    \n    return df",
  "analysis": {
    "correlation": -0.12859774431168056,
    "p_value": 0.21424136926216208,
    "interpretation": "약한 음의 상관관계(-0.1286)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.2142)."
  }
}{
  "timestamp": "2025-07-22T06:39:07.024587",
  "attempt": 1,
  "status": "success",
  "feature_name": "intrinsic_quality_index",
  "hypothesis": "포스트의 '내재적 품질'은 콘텐츠에 투입된 노력과 비례한다. 단어 수(분량), 유효 이미지 수(시각적 풍부함), 가독성 점수(편집 완성도)를 종합한 '내재적 품질 지수'를 생성하면, 개별 지표보다 `non_brand_inflow`와 더 높은 상관관계를 보일 것이다. 이 지수는 사용자와 검색엔진 모두에게 콘텐츠의 가치를 나타내는 강력한 신호로 작용할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates an 'Intrinsic Quality Index' for 'ours' posts based on word count,\n    valid image count, and readability score. This index quantifies the overall\n    effort and quality invested in a piece of content.\n    \"\"\"\n    feature_name = 'intrinsic_quality_index'\n    # Initialize column to avoid errors on empty or non-matching data\n    df[feature_name] = 0.0\n\n    if df.empty:\n        return df\n\n    # The calculation should only apply to 'ours' posts.\n    # We operate on a view but use the mask to assign back to the original df, preserving the index.\n    ours_mask = df['source'] == 'ours'\n    \n    # Proceed only if there are 'ours' posts to process\n    if ours_mask.any():\n        # --- Define components of the quality index ---\n        word_count = df.loc[ours_mask, 'word_count']\n        image_count = df.loc[ours_mask, 'valid_image_count']\n        readability = df.loc[ours_mask, 'readability_score']\n\n        # --- Z-score normalization for each component to bring them to a comparable scale ---\n        # This handles differing units and distributions (e.g., words vs. score).\n        \n        def safe_zscore(series: pd.Series) -> pd.Series:\n            \"\"\"Safely calculates Z-score, returning a series of zeros if standard deviation is zero.\"\"\"\n            std_dev = series.std()\n            if std_dev > 0:\n                return (series - series.mean()) / std_dev\n            return pd.Series(0.0, index=series.index)\n\n        word_count_z = safe_zscore(word_count)\n        image_count_z = safe_zscore(image_count)\n        readability_z = safe_zscore(readability)\n\n        # --- Combine the normalized scores ---\n        # A simple sum gives equal weight to each normalized component of \"quality\".\n        quality_index = (word_count_z + image_count_z + readability_z).fillna(0)\n\n        # --- Min-Max scale the final index to a more intuitive 0-100 range ---\n        min_val = quality_index.min()\n        max_val = quality_index.max()\n        \n        if max_val > min_val:\n            scaled_index = 100 * (quality_index - min_val) / (max_val - min_val)\n        else:\n            # Handle edge case where all quality_index values are the same\n            scaled_index = pd.Series(50.0, index=quality_index.index)\n\n        # --- Assign the calculated feature back to the original DataFrame ---\n        # .loc with the boolean mask ensures values are assigned to the correct rows.\n        df.loc[ours_mask, feature_name] = scaled_index\n\n    # For any rows that were not processed (e.g., competitors or if no 'ours' posts existed),\n    # the feature column will retain its initial value of 0.0.\n\n    return df",
  "analysis": {
    "correlation": -0.12438138050610092,
    "p_value": 0.22977418888024137,
    "interpretation": "약한 음의 상관관계(-0.1244)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.2298)."
  }
}{
  "timestamp": "2025-07-22T07:03:20.070139",
  "attempt": 1,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'morpheme_noun_proportion'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the proportion of noun morphemes relative to the total number of noun and verb morphemes.\n\n    This feature tests the hypothesis that the linguistic structure, specifically the balance\n    between descriptive nouns (e.g., /NNG, /NNP) and action-oriented verbs (/VV),\n    correlates with content performance. A higher proportion of nouns might indicate more\n    informational content, which could be beneficial for certain query types.\n    \"\"\"\n    # 1. Handle empty DataFrame edge case.\n    if df.empty:\n        df['morpheme_noun_proportion'] = pd.Series(dtype=float)\n        return df\n\n    # 2. Ensure the source column is string type and handle missing values.\n    # The 'morpheme_words' column contains a string representation of morphemes and their tags.\n    morphemes_str = df['morpheme_words'].fillna('').astype(str)\n\n    # 3. Use efficient, vectorized string operations to count morphemes.\n    # We count '/NN' to capture various noun tags (NNG, NNP, etc.).\n    # We count '/VV' to capture verb tags.\n    noun_counts = morphemes_str.str.count('/NN')\n    verb_counts = morphemes_str.str.count('/VV')\n\n    # 4. Calculate the total count of these primary morphemes.\n    total_counts = noun_counts + verb_counts\n\n    # 5. Calculate the proportion of nouns. Use np.divide for safe division.\n    # This will result in NaN for rows where total_counts is 0 (i.e., no nouns or verbs).\n    proportion = np.divide(noun_counts, total_counts)\n\n    # 6. Assign the new feature to the DataFrame.\n    # Fill NaN values with a neutral 0.5, assuming an equal balance when no data is available.\n    df['morpheme_noun_proportion'] = proportion.fillna(0.5)\n\n    return df"
}{
  "timestamp": "2025-07-22T07:03:45.348774",
  "attempt": 2,
  "status": "success",
  "feature_name": "lexical_diversity",
  "hypothesis": "포스트 본문에 사용된 어휘의 다양성(고유 단어 수 / 전체 단어 수)이 높을수록, 콘텐츠가 더 전문적이고 풍부하게 인식되어 비브랜드 유입량이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef calculate_diversity(text):\n    \"\"\"Calculates the lexical diversity for a given text.\"\"\"\n    # Handle non-string or empty/whitespace-only strings\n    if not isinstance(text, str) or not text.strip():\n        return 0.0\n    \n    words = text.split()\n    \n    # Avoid division by zero for empty lists of words\n    if not words:\n        return 0.0\n        \n    unique_words_count = len(set(words))\n    total_words_count = len(words)\n    \n    return unique_words_count / total_words_count\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the lexical diversity for each post body and adds it as a new feature.\n    This feature is calculated row-by-row to ensure variance across the dataset,\n    addressing the previous issue of a constant value for all entries.\n    \"\"\"\n    # Ensure the DataFrame is not empty before processing\n    if not df.empty:\n        # The .apply() method is suitable here as it performs a distinct calculation for each row,\n        # ensuring the resulting feature has variance.\n        df['lexical_diversity'] = df['post_body'].apply(calculate_diversity)\n    \n    return df",
  "analysis": {
    "correlation": 0.17477455467525685,
    "p_value": 0.09026426174184199,
    "interpretation": "약한 양의 상관관계(0.1748)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.0903)."
  }
}{
  "timestamp": "2025-07-22T07:23:01.063500",
  "attempt": 1,
  "status": "success",
  "feature_name": "title_length_optimization_score",
  "hypothesis": "제목의 길이가 검색엔진 결과 페이지(SERP) 노출에 최적화된 특정 범위(예: 50~60자)에 가까울수록, 잘리거나 어색하게 표시될 확률이 낮아져 클릭률(CTR)이 향상되고, 결과적으로 non-brand 유입량이 증가할 것입니다. 이 피처는 최적 길이에 가까울수록 1에 가까운 점수를 부여합니다.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a title length optimization score based on an ideal length for SERPs.\n    The score is highest at the optimal length and decreases for shorter or longer titles,\n    following a Gaussian distribution.\n\n    Args:\n        df (pd.DataFrame): The input dataframe with a 'post_title' column.\n\n    Returns:\n        pd.DataFrame: The dataframe with the new 'title_length_optimization_score' column.\n    \"\"\"\n    # 1. Handle empty DataFrame\n    if df.empty:\n        # Ensure the column is created even for an empty DataFrame to maintain schema consistency\n        df['title_length_optimization_score'] = pd.Series(dtype=float)\n        return df\n\n    # 2. Define parameters for the optimization score\n    # Optimal length for SERP display is often cited as 50-60 characters.\n    optimal_length = 55.0\n    # Sigma controls the steepness of the score drop-off.\n    # A larger sigma means a more gentle penalty for deviating from the optimal length.\n    sigma = 10.0\n\n    # 3. Calculate title lengths safely\n    # Fill potential NaN values with an empty string before calculating length.\n    title_lengths = df['post_title'].fillna('').astype(str).str.len()\n\n    # 4. Calculate the score using a vectorized Gaussian function\n    # The score will be 1.0 at the optimal_length and decay towards 0 as the length deviates.\n    # This is more nuanced than a simple binary in/out of range check.\n    score = np.exp(-np.power(title_lengths - optimal_length, 2.) / (2 * np.power(sigma, 2.)))\n\n    df['title_length_optimization_score'] = score\n    return df",
  "analysis": {
    "correlation": -0.10147939043399831,
    "p_value": 0.32781038811869795,
    "interpretation": "약한 음의 상관관계(-0.1015)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3278)."
  }
}{
  "timestamp": "2025-07-22T07:56:23.796244",
  "attempt": 1,
  "status": "success",
  "feature_name": "image_density_per_1000_chars",
  "hypothesis": "우리 포스트 내에서, 콘텐츠의 텍스트 길이에 비해 유효 이미지 수가 많을수록(이미지 밀도가 높을수록) 시각적 자료가 풍부하여 독자의 이해를 돕고 체류시간을 늘려, 결과적으로 비브랜드 유입에 긍정적인 영향을 미칠 것이다.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    포스트의 텍스트 1000자 당 유효 이미지 개수를 계산하여 '이미지 밀도'를 나타내는 피처를 생성합니다.\n\n    Args:\n        df (pd.DataFrame): 'valid_image_count'와 'char_count_with_space' 열을 포함하는 데이터프레임.\n                          이 함수는 'source'가 'ours'인 데이터에 대해 실행될 것으로 예상됩니다.\n\n    Returns:\n        pd.DataFrame: 'image_density_per_1000_chars' 열이 추가된 데이터프레임.\n    \"\"\"\n    feature_name = 'image_density_per_1000_chars'\n\n    # 0. 입력 데이터프레임이 비어 있는지 확인합니다.\n    if df.empty:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n\n    # 1. 텍스트 길이가 0인 경우 0으로 나누는 것을 방지하기 위해 분모를 처리합니다.\n    # 0을 np.nan으로 대체하여 계산 후 0으로 채울 수 있도록 합니다.\n    denominator = df['char_count_with_space'].replace(0, np.nan)\n\n    # 2. 텍스트 1000자당 유효 이미지 개수를 계산합니다.\n    # 이 방식은 모든 행에 대해 벡터화된 연산을 수행하므로 효율적입니다.\n    image_density = (df['valid_image_count'] / denominator) * 1000\n\n    # 3. 계산 결과가 NaN인 경우(즉, 텍스트 길이가 0이었던 경우) 0으로 채웁니다.\n    df[feature_name] = image_density.fillna(0)\n\n    return df",
  "analysis": {
    "correlation": -0.05858970584552516,
    "p_value": 0.5727631212534067,
    "interpretation": "약한 음의 상관관계(-0.0586)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.5728)."
  }
}{
  "timestamp": "2025-07-22T09:59:14.719931",
  "attempt": 1,
  "status": "success",
  "feature_name": "lexical_diversity_score",
  "hypothesis": "포스트에 사용된 형태소의 어휘 다양성(Lexical Diversity)이 높을수록, 즉 중복된 표현을 적게 쓰고 다양한 어휘를 구사할수록, 콘텐츠의 질이 높게 평가되어 '비브랜드 유입(non_brand_inflow)'이 증가할 것이다. 어휘 다양성은 Type-Token Ratio (고유 형태소 수 / 전체 형태소 수)로 측정한다.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the lexical diversity score (Type-Token Ratio) for each post\n    based on its morphemes from the 'morpheme_words' column.\n\n    Hypothesis: Posts with higher lexical diversity (more unique morphemes relative\n    to the total count) are perceived as higher quality, leading to more non-brand inflow.\n    The score is calculated as (number of unique morphemes) / (total number of morphemes).\n\n    Args:\n        df (pd.DataFrame): The input DataFrame, expected to have a 'morpheme_words' column.\n                           This function is intended for the 'ours' source subset.\n\n    Returns:\n        pd.DataFrame: The DataFrame with an added 'lexical_diversity_score' column.\n    \"\"\"\n    feature_name = 'lexical_diversity_score'\n\n    # Handle empty DataFrame gracefully\n    if df.empty:\n        df[feature_name] = pd.Series(dtype='float64')\n        return df\n\n    # If the necessary column is missing, return a default value to avoid errors\n    if 'morpheme_words' not in df.columns:\n        df[feature_name] = 0.0\n        return df\n\n    def calculate_diversity(morpheme_string: str) -> float:\n        \"\"\"Helper function to calculate TTR from a comma-separated string of morphemes.\"\"\"\n        # Ensure the input is a valid, non-empty string\n        if pd.isna(morpheme_string) or not isinstance(morpheme_string, str) or not morpheme_string.strip():\n            return 0.0\n\n        # The 'morpheme_words' are comma-separated; split into a list of tokens\n        tokens = [token.strip() for token in morpheme_string.split(',') if token.strip()]\n\n        # If no tokens are found after cleaning, diversity is 0\n        if not tokens:\n            return 0.0\n\n        total_tokens = len(tokens)\n        unique_tokens = len(set(tokens))\n\n        # Type-Token Ratio (TTR) calculation, safe from division by zero due to the check above\n        return unique_tokens / total_tokens\n\n    # Apply the function to the 'morpheme_words' column to create the new feature\n    df[feature_name] = df['morpheme_words'].apply(calculate_diversity)\n\n    return df",
  "analysis": {
    "correlation": 0.074694767479413,
    "p_value": 0.47189295557069316,
    "interpretation": "약한 양의 상관관계(0.0747)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.4719)."
  }
}{
  "timestamp": "2025-07-22T10:05:40.773904",
  "attempt": 1,
  "status": "success",
  "feature_name": "numeric_data_density",
  "hypothesis": "포스트 본문에 구체적인 수치(예: 날짜, 개월 수, 치아 번호, 백분율 등)가 많이 포함될수록, 일반적인 정보가 아닌 실제 사례 기반의 상세하고 신뢰도 높은 콘텐츠로 인식됩니다. 이러한 데이터 기반의 구체성은 콘텐츠의 전문성을 높여 사용자 및 검색 엔진에 긍정적으로 평가받고, 결과적으로 `non_brand_inflow`를 증가시킬 것입니다.",
  "code": "import pandas as pd\nimport numpy as np\nimport re\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the density of numeric data within the post body.\n\n    The feature 'numeric_data_density' is computed by finding all numeric tokens \n    (e.g., '11', '46', '6') in the 'post_body' and dividing this count by the 'word_count'.\n    This aims to quantify the specificity and data-driven nature of the content.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame which must contain 'post_body' \n                           and 'word_count' columns.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the new 'numeric_data_density' column added.\n    \"\"\"\n    if df.empty:\n        return df\n\n    feature_name = 'numeric_data_density'\n\n    # Ensure the post_body column is of string type and handle potential NaNs\n    post_bodies = df['post_body'].fillna('').astype(str).tolist()\n\n    # Use a list comprehension for efficient processing, which is faster than .apply()\n    # The regex r'\\d+(?:\\.\\d+)?' finds any integer or float number.\n    numeric_counts = [len(re.findall(r'\\d+(?:\\.\\d+)?', body)) for body in post_bodies]\n\n    # Convert the list of counts to a pandas Series, preserving the original index\n    numeric_counts_series = pd.Series(numeric_counts, index=df.index, dtype=float)\n\n    # Get the word count as the denominator\n    denominator = df['word_count'].astype(float)\n\n    # Initialize the result series with zeros\n    result = pd.Series(0.0, index=df.index)\n\n    # Create a boolean mask to identify rows where the denominator is not zero\n    # This prevents division by zero errors and correctly assigns 0 for posts with no words\n    non_zero_mask = denominator > 0\n\n    # Perform the division only on the rows identified by the mask\n    result.loc[non_zero_mask] = numeric_counts_series[non_zero_mask] / denominator[non_zero_mask]\n\n    # Assign the final calculated series to the new feature column\n    df[feature_name] = result\n\n    return df",
  "analysis": {
    "correlation": -0.23898452868379216,
    "p_value": 0.019679542336223668,
    "interpretation": "약한 음의 상관관계(-0.2390)를 발견했습니다. 이 결과는 통계적으로 유의미합니다(p-value: 0.0197)."
  }
}