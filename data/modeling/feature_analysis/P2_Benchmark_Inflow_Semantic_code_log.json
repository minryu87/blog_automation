{
  "timestamp": "2025-07-22T04:02:08.823620",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_distance_from_ideal",
  "hypothesis": "우리 포스트의 제목과 본문을 결합한 텍스트가, 해당 대표 검색어의 상위 노출 경쟁자(노출 점수 상위 25%)들의 평균적인 의미 중심(semantic centroid)에 더 가까울수록(즉, 의미론적 거리가 짧을수록), 비브랜드 유입량이 높을 것이다. 이는 우리 콘텐츠가 검색 사용자의 의도와 상위 랭킹 콘텐츠의 공통된 의미적 특징을 잘 충족시키고 있음을 의미하기 때문이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic distance of each 'ours' post from the centroid \n    of high-performing competitor posts for the same representative query.\n    \"\"\"\n    # 0. Handle empty or invalid DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        return df\n\n    # Ensure required columns exist\n    required_cols = ['post_title', 'post_body', 'source', 'representative_query', 'exposure_score']\n    if not all(col in df.columns for col in required_cols):\n        df['semantic_distance_from_ideal'] = np.nan\n        return df\n\n    model = get_model()\n    \n    # Initialize the feature column with NaNs\n    df['semantic_distance_from_ideal'] = np.nan\n\n    # 1. Prepare text data and generate embeddings for the entire DataFrame at once\n    # Preserve the original index to ensure correct alignment later\n    original_index = df.index\n    texts = (df['post_title'].fillna('') + ' ' + df['post_body'].fillna('')).tolist()\n    all_embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n\n    # Create a temporary DataFrame with original index for processing\n    temp_df = df[['source', 'representative_query', 'exposure_score']].copy()\n    temp_df['original_pos'] = np.arange(len(df))\n\n    # 2. Group by query and calculate the ideal centroid from top competitors\n    grouped = temp_df.groupby('representative_query')\n\n    for query, group_df in grouped:\n        ours_df = group_df[group_df['source'] == 'ours']\n        competitors_df = group_df[group_df['source'] == 'competitor']\n\n        # Proceed only if both 'ours' and 'competitor' posts exist for the query\n        if ours_df.empty or competitors_df.empty:\n            continue\n\n        # 3. Define high-performing competitors (top 25% by exposure_score)\n        # This avoids skewing the benchmark with low-quality content\n        performance_threshold = competitors_df['exposure_score'].quantile(0.75)\n        \n        # Handle cases where all competitors have the same score (quantile can be tricky)\n        if pd.isna(performance_threshold):\n            continue\n        \n        top_competitors_df = competitors_df[competitors_df['exposure_score'] >= performance_threshold]\n\n        if top_competitors_df.empty:\n            # If no competitors meet the threshold, use all competitors as a fallback\n            top_competitors_df = competitors_df\n\n        # 4. Calculate the semantic centroid for the ideal group\n        # Use original integer positions to slice the embeddings tensor safely\n        competitor_positions = top_competitors_df['original_pos'].values\n        competitor_embeddings = all_embeddings[competitor_positions]\n        \n        # Ensure we have embeddings to process\n        if competitor_embeddings.shape[0] == 0:\n            continue\n\n        ideal_centroid = torch.mean(competitor_embeddings, dim=0)\n\n        # 5. Calculate distance for each 'ours' post from the ideal centroid\n        ours_positions = ours_df['original_pos'].values\n        ours_indices = ours_df.index\n        ours_embeddings = all_embeddings[ours_positions]\n\n        if ours_embeddings.shape[0] > 0:\n            # Calculate cosine similarity, then convert to distance (1 - similarity)\n            similarities = util.cos_sim(ours_embeddings, ideal_centroid).flatten()\n            distances = (1 - similarities).cpu().numpy()\n            \n            # 6. Assign the calculated feature back to the original DataFrame using the correct index\n            df.loc[ours_indices, 'semantic_distance_from_ideal'] = distances\n\n    return df",
  "analysis": {
    "correlation": -0.0339375012692239,
    "p_value": 0.7440483988638961,
    "interpretation": "약한 음의 상관관계(-0.0339)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.7440)."
  }
}{
  "timestamp": "2025-07-22T04:22:39.775373",
  "attempt": 1,
  "status": "execution_error",
  "error": "코드 실행 실패:\nError during script execution: Must have equal len keys and value when setting with an iterable\n",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic distance of our posts from an 'ideal' competitor centroid for each query.\n\n    The 'ideal' centroid is the average semantic vector of the top 50% of competitor posts,\n    ranked by 'exposure_score'. A lower distance suggests our content is semantically closer\n    to what is performing well in search for that specific query.\n    \"\"\"\n    # 0. Handle empty DataFrame\n    if df.empty:\n        return df\n\n    # Initialize the feature column with NaN\n    feature_name = 'semantic_distance_from_ideal'\n    if feature_name not in df.columns:\n        df[feature_name] = np.nan\n\n    model = get_model()\n\n    # 1. Prepare data and encode all post bodies in one batch for efficiency\n    # Ensure the original index is preserved for later mapping\n    df_copy = df.copy()\n    df_copy['post_body'] = df_copy['post_body'].fillna('').astype(str)\n    \n    # Check if a GPU is available, otherwise use CPU\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n\n    all_embeddings = model.encode(\n        df_copy['post_body'].tolist(),\n        convert_to_tensor=True,\n        show_progress_bar=False,\n        device=device\n    )\n\n    # 2. Group by query to perform benchmark calculations\n    # Use the index from the copied dataframe for grouping\n    grouped = df_copy.groupby('representative_query')\n\n    for query, group_df in grouped:\n        # Separate our posts from competitors\n        ours_df = group_df[group_df['source'] == 'ours']\n        competitors_df = group_df[group_df['source'] == 'competitor']\n\n        # Proceed only if we have both our posts and competitor posts for the query\n        if ours_df.empty or competitors_df.empty:\n            continue\n\n        # 3. Define the 'ideal' competitor group\n        # Use the top 50% of competitors based on exposure_score\n        # This avoids benchmarking against low-quality content\n        performance_threshold = competitors_df['exposure_score'].quantile(0.5, interpolation='lower')\n        high_perf_competitors = competitors_df[competitors_df['exposure_score'] >= performance_threshold]\n\n        # If quantile filtering results in an empty set (e.g., only 1 competitor), use all competitors as a fallback\n        if high_perf_competitors.empty:\n            high_perf_competitors = competitors_df\n\n        # 4. Calculate the semantic centroid for the ideal group\n        # IMPORTANT: Use .get_indexer to convert DataFrame index labels to positional integer indices for the tensor\n        competitor_indices = df_copy.index.get_indexer(high_perf_competitors.index)\n        if len(competitor_indices) == 0:\n            continue\n        \n        competitor_embeddings = all_embeddings[competitor_indices]\n        ideal_centroid = torch.mean(competitor_embeddings, dim=0)\n\n        # 5. Calculate the distance for our posts from the ideal centroid\n        our_indices = df_copy.index.get_indexer(ours_df.index)\n        if len(our_indices) == 0:\n            continue\n\n        our_embeddings = all_embeddings[our_indices]\n        \n        # Calculate cosine similarity and convert to distance (1 - similarity)\n        similarities = util.cos_sim(our_embeddings, ideal_centroid)\n        distances = 1 - similarities.diag() # Use diag() as we compare each of our posts to the single centroid\n        \n        # 6. Assign the calculated values back to the original DataFrame using the correct index\n        df.loc[ours_df.index, feature_name] = distances.cpu().numpy()\n\n    return df\n"
}{
  "timestamp": "2025-07-22T04:23:32.140353",
  "attempt": 2,
  "status": "success",
  "feature_name": "title_body_cohesion_vs_competitors",
  "hypothesis": "게시물의 제목과 본문이 의미적으로 더 일관성 있을수록(높은 코사인 유사도) 사용자 의도를 더 잘 충족시킵니다. 이러한 자체적인 의미적 일관성을 동일 대표 검색어의 경쟁자 평균과 비교했을 때, 우리 포스트의 일관성이 경쟁자 평균보다 높을수록 주제에 집중하고 잘 구성된 콘텐츠로 판단되어 더 높은 비브랜드 유입(non_brand_inflow)을 유도할 것입니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # A good multilingual model suitable for various topics.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the ratio of title-body semantic cohesion for our posts compared to the competitor average for the same query.\n\n    The feature 'title_body_cohesion_vs_competitors' is calculated as:\n    (Cohesion of 'ours' post) / (Average cohesion of 'competitor' posts)\n    \n    This addresses the previous error 'Must have equal len keys and value when setting with an iterable'\n    by calculating results for each 'ours' post and assigning them back to the original DataFrame\n    using the unique index, ensuring perfect alignment and avoiding length mismatches.\n    \"\"\"\n    # 2. Check for empty DataFrame to prevent errors.\n    if df.empty:\n        return df\n\n    # Ensure the new column exists with a default value (NaN)\n    feature_name = 'title_body_cohesion_vs_competitors'\n    if feature_name not in df.columns:\n        df[feature_name] = np.nan\n\n    model = get_model()\n\n    # 3. Efficient batch processing: Prepare data and encode all at once.\n    # Using .fillna('') to handle potential missing titles or bodies.\n    titles = df['post_title'].fillna('').astype(str).tolist()\n    bodies = df['post_body'].fillna('').astype(str).tolist()\n\n    # Encode all titles and bodies in batches for performance.\n    title_embeddings = model.encode(titles, convert_to_tensor=True, show_progress_bar=False)\n    body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # 4. Calculate self-cohesion (similarity between title and body) for every post.\n    # .diag() efficiently extracts the diagonal from the similarity matrix, giving a 1-to-1 comparison.\n    cohesion_scores = util.cos_sim(title_embeddings, body_embeddings).diag()\n\n    # Assign cohesion scores to a temporary column for easy grouping and access.\n    # This alignment is safe because we haven't altered the DataFrame's index or order.\n    df['temp_self_cohesion'] = cohesion_scores.cpu().numpy()\n\n    # 5. Group by representative query to perform the comparison.\n    # We preserve the original index to assign the final feature value correctly.\n    grouped = df.groupby('representative_query')\n\n    # Use a list to collect results before assigning to the DataFrame to avoid direct modification issues in loops.\n    results = []\n\n    for name, group in grouped:\n        ours_posts = group[group['source'] == 'ours']\n        competitor_posts = group[group['source'] == 'competitor']\n\n        # Skip groups that don't have both our posts and competitor posts.\n        if ours_posts.empty or competitor_posts.empty:\n            continue\n\n        # Calculate the average cohesion for all competitor posts in the group.\n        competitor_avg_cohesion = competitor_posts['temp_self_cohesion'].mean()\n\n        # Iterate through each of our posts in the group (usually just one).\n        for idx, our_post in ours_posts.iterrows():\n            our_cohesion = our_post['temp_self_cohesion']\n            \n            # Calculate the ratio. Handle division by zero or near-zero.\n            if competitor_avg_cohesion > 1e-6:\n                ratio = our_cohesion / competitor_avg_cohesion\n            else:\n                # If competitors have no cohesion, our post is infinitely better (or set to a high capped value).\n                # Setting to 1.0 implies parity if both are 0, which is reasonable.\n                ratio = 1.0\n            \n            # Store the result with its original index for safe assignment later.\n            results.append({'index': idx, 'value': ratio})\n\n    # 6. Safely assign the calculated feature values back to the main DataFrame.\n    # This approach correctly handles assignment even if not all 'ours' posts get a score.\n    if results:\n        result_df = pd.DataFrame(results).set_index('index')\n        df.loc[result_df.index, feature_name] = result_df['value']\n\n    # 7. Clean up the temporary column and return the full DataFrame.\n    df.drop(columns=['temp_self_cohesion'], inplace=True)\n\n    return df",
  "analysis": {
    "correlation": -0.06492749438793932,
    "p_value": 0.5318939953130282,
    "interpretation": "약한 음의 상관관계(-0.0649)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.5319)."
  }
}{
  "timestamp": "2025-07-22T04:41:05.860123",
  "attempt": 1,
  "status": "execution_error",
  "error": "코드 실행 실패:\nError during script execution: module 'nltk.downloader' has no attribute 'DownloadError'\n",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\nimport nltk\n\n# 1. To avoid re-initializing on every call, use a global variable for the model.\n_model = None\n# Flag to ensure nltk data is downloaded only once.\n_nltk_punkt_downloaded = False\n\ndef get_model_and_resources():\n    \"\"\"Lazy-loads the model and ensures NLTK 'punkt' is available.\"\"\"\n    global _model, _nltk_punkt_downloaded\n    if _model is None:\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    \n    if not _nltk_punkt_downloaded:\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except nltk.downloader.DownloadError:\n            nltk.download('punkt', quiet=True)\n        _nltk_punkt_downloaded = True\n        \n    return _model\n\ndef _calculate_internal_spread(text: str, model: SentenceTransformer) -> float:\n    \"\"\"\n    Calculates the semantic spread of a single text document.\n    The spread is the average cosine distance of sentence embeddings to their centroid.\n    \"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return 0.0\n\n    sentences = nltk.sent_tokenize(text)\n    \n    # If there's only one sentence or none, semantic spread is zero.\n    if len(sentences) < 2:\n        return 0.0\n\n    try:\n        # Encode sentences into embeddings\n        embeddings = model.encode(sentences, convert_to_tensor=True, show_progress_bar=False)\n        \n        # Handle potential empty embeddings if sentences are just whitespace/special chars\n        if embeddings.shape[0] < 2:\n            return 0.0\n\n        # Calculate the centroid (mean embedding)\n        centroid = torch.mean(embeddings, dim=0, keepdim=True)\n        \n        # Calculate cosine similarity of each sentence to the centroid\n        # Convert similarities to distances (1 - sim)\n        distances = 1 - util.cos_sim(embeddings, centroid)\n        \n        # The spread is the mean of these distances\n        return distances.mean().item()\n    except Exception:\n        # In case of any unexpected error during processing a single text\n        return 0.0\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generates the 'relative_semantic_spread' feature.\n    This feature is the ratio of a post's semantic spread to the average\n    spread of its competitors for the same representative query.\n    \"\"\"\n    feature_name = 'relative_semantic_spread'\n\n    if df.empty:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n\n    # Preserve original index for safe final assignment\n    original_index = df.index\n    df_copy = df.copy()\n\n    # 1. Get model and ensure necessary resources are downloaded\n    model = get_model_and_resources()\n\n    # 2. Calculate the internal semantic spread for every post\n    # Using a list comprehension is more efficient than .apply for this task\n    spreads = [_calculate_internal_spread(body, model) for body in df_copy['post_body'].fillna('')]\n    df_copy['semantic_spread'] = spreads\n\n    # 3. Separate 'ours' and 'competitor' data\n    ours_df = df_copy[df_copy['source'] == 'ours'].copy()\n    competitors_df = df_copy[df_copy['source'] == 'competitor'].copy()\n\n    # Initialize the feature column in the original dataframe with NaN\n    df[feature_name] = np.nan\n\n    # Proceed only if there are both our posts and competitor posts to compare\n    if not ours_df.empty and not competitors_df.empty:\n        # 4. Calculate the average spread for each competitor group\n        competitor_avg_spreads = competitors_df.groupby('representative_query')['semantic_spread'].mean()\n        \n        # 5. Map the competitor average to our posts\n        ours_df['competitor_avg_spread'] = ours_df['representative_query'].map(competitor_avg_spreads)\n\n        # 6. Calculate the relative spread ratio\n        # Use np.divide for safe division, handling division by zero\n        ours_df[feature_name] = np.divide(\n            ours_df['semantic_spread'], \n            ours_df['competitor_avg_spread']\n        )\n        \n        # Replace inf values (from dividing by zero) with NaN, then fill with 1.0 (neutral ratio)\n        ours_df[feature_name].replace([np.inf, -np.inf], np.nan, inplace=True)\n        ours_df[feature_name].fillna(1.0, inplace=True)\n\n        # 7. Safely assign the calculated feature back to the original DataFrame using its index\n        df.loc[ours_df.index, feature_name] = ours_df[feature_name]\n\n    return df"
}{
  "timestamp": "2025-07-22T04:41:55.502697",
  "attempt": 2,
  "status": "success",
  "feature_name": "semantic_actionability_vs_competitors",
  "hypothesis": "우리 포스트의 본문이 경쟁자 그룹의 평균적인 본문보다 '방법', '해결', '팁' 등 실행 가능한 정보를 나타내는 의미를 더 많이 포함할수록, 사용자의 문제 해결 의도를 충족시켜 비브랜드 유입량이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport nltk\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # Use a multilingual model suitable for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef _ensure_nltk_punkt():\n    \"\"\"Ensures the 'punkt' tokenizer is downloaded. Handles potential errors gracefully.\"\"\"\n    try:\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        # Standard error if resource not found\n        nltk.download('punkt', quiet=True)\n    # The previous error was `module 'nltk.downloader' has no attribute 'DownloadError'`\n    # This indicates an old NLTK version. The try/except LookupError is the standard, safe way\n    # to handle this, as it works across versions.\n\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a semantic actionability score for each post and benchmarks it against competitors.\n    1. Defines 'actionability' keywords.\n    2. Calculates a score for each post based on its semantic similarity to these keywords.\n    3. For each 'ours' post, calculates the ratio of its score to the average score of its competitors.\n    \"\"\"\n    if df.empty:\n        df['semantic_actionability_vs_competitors'] = pd.Series(dtype=float)\n        return df\n\n    # Ensure NLTK data is available before any potential use.\n    _ensure_nltk_punkt()\n\n    model = get_model()\n\n    # Create a copy to avoid SettingWithCopyWarning and preserve original df\n    df_copy = df.copy()\n\n    # --- Step 1: Define Actionability Keywords and Calculate Score for ALL posts ---\n    actionability_keywords = [\n        '방법', '해결', '팁', '가이드', '직접', '쉽게', '하는 법', '절차', '순서',\n        'how to', 'solution', 'tip', 'guide', 'step-by-step'\n    ]\n\n    # Encode keywords and post bodies\n    keyword_embeddings = model.encode(actionability_keywords, convert_to_tensor=True)\n    post_bodies = df_copy['post_body'].fillna('').astype(str).tolist()\n    body_embeddings = model.encode(post_bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # Calculate cosine similarity between each post body and all actionability keywords\n    cos_scores = util.cos_sim(body_embeddings, keyword_embeddings)\n\n    # The 'actionability_score' for each post is the *maximum* similarity to any of the keywords\n    actionability_scores = cos_scores.max(axis=1).values.cpu().numpy()\n    df_copy['actionability_score'] = actionability_scores\n\n    # --- Step 2: Calculate Competitor Average Score per Query Group ---\n    # This will calculate the mean score for competitors within each query group\n    # and broadcast it to all members of the group (both 'ours' and 'competitor' rows)\n    # This is an efficient way to make the competitor average available for calculation.\n    competitor_scores = df_copy.loc[df_copy['source'] == 'competitor', ['representative_query', 'actionability_score']]\n    \n    # Use groupby().transform('mean') to calculate group-wise average and align it back to the original index\n    # This avoids manual merging and potential index misalignment issues.\n    competitor_avg_series = competitor_scores.groupby('representative_query')['actionability_score'].transform('mean')\n    \n    # Map the calculated averages back to the main DataFrame copy\n    df_copy['competitor_avg_actionability'] = df_copy['representative_query'].map(competitor_avg_series.groupby(df_copy['representative_query']).first())\n\n    # --- Step 3: Calculate the Final Ratio for 'ours' posts ---\n    # Isolate 'ours' posts for the final calculation\n    our_posts_mask = df_copy['source'] == 'ours'\n\n    our_scores = df_copy.loc[our_posts_mask, 'actionability_score']\n    competitor_avg_scores = df_copy.loc[our_posts_mask, 'competitor_avg_actionability']\n\n    # Calculate the ratio. Handle division by zero or cases where competitors have no score.\n    # A ratio of 1.0 implies parity with competitors.\n    ratio = our_scores / competitor_avg_scores\n    final_ratio = ratio.replace([np.inf, -np.inf], 1.0).fillna(1.0).astype(float)\n\n    # --- Step 4: Assign the new feature to the original DataFrame ---\n    # Initialize column with NaN for all rows\n    df['semantic_actionability_vs_competitors'] = np.nan\n    # Assign the calculated values only to the 'ours' rows using the original index\n    df.loc[our_posts_mask, 'semantic_actionability_vs_competitors'] = final_ratio\n\n    return df",
  "analysis": {
    "correlation": 0.00039714506721615604,
    "p_value": 0.9969523700435227,
    "interpretation": "약한 양의 상관관계(0.0004)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.9970)."
  }
}{
  "timestamp": "2025-07-22T05:01:52.149465",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_actionability_vs_competitors",
  "hypothesis": "우리 포스트의 내용(제목+본문)이 상위 경쟁자 그룹의 평균적인 내용보다 '방법', '해결', '팁', '후기'와 같은 실용적/해결책 지향적 의미에 더 가까울수록(즉, 의미론적 실행 가능성 점수 비율이 높을수록), 사용자의 정보 탐색 의도를 더 잘 충족시켜 비브랜드 유입량이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a model that performs well on Korean and general semantic tasks.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a feature that benchmarks the 'actionability' of our posts against competitors.\n    \n    The feature is the ratio of our post's semantic actionability score to the average \n    score of high-performing competitor posts for the same representative query.\n    \"\"\"\n    # 0. Handle empty DataFrame\n    if df.empty:\n        df['semantic_actionability_vs_competitors'] = pd.Series(dtype=float)\n        return df\n\n    # Initialize the feature column with NaN\n    df['semantic_actionability_vs_competitors'] = np.nan\n\n    model = get_model()\n\n    # 1. Define 'actionability' concept with semantically rich phrases\n    actionability_phrases = [\n        '방법', '해결책', '하는 법', '팁', '노하우',\n        '가이드', '절차', '순서', '단계별 설명',\n        '성공 후기', '실제 경험', '사용 후기', '비교 분석',\n        '문제 해결', '추천', '가장 좋은 방법'\n    ]\n\n    # 2. Create a semantic centroid for 'actionability'\n    with torch.no_grad():\n        action_embeddings = model.encode(actionability_phrases, convert_to_tensor=True)\n        actionability_centroid = torch.mean(action_embeddings, dim=0)\n\n    # 3. Prepare text and compute embeddings for all posts in a single batch\n    df_copy = df.copy()\n    df_copy['text_for_embedding'] = df_copy['post_title'].fillna('') + ' ' + df_copy['post_body'].fillna('')\n    texts_to_encode = df_copy['text_for_embedding'].tolist()\n\n    with torch.no_grad():\n        all_embeddings = model.encode(texts_to_encode, convert_to_tensor=True, show_progress_bar=False)\n\n    # 4. Calculate actionability score for every post\n    # Use unsqueeze and squeeze to manage tensor dimensions for broadcasting\n    cosine_scores = util.cos_sim(all_embeddings, actionability_centroid.unsqueeze(0))\n    df_copy['actionability_score'] = cosine_scores.squeeze().cpu().numpy()\n\n    # 5. Group by query and calculate the benchmark ratio\n    grouped = df_copy.groupby('representative_query')\n\n    for _, group_df in grouped:\n        ours_posts = group_df[group_df['source'] == 'ours']\n        competitor_posts = group_df[group_df['source'] == 'competitor']\n\n        if ours_posts.empty or competitor_posts.empty:\n            continue\n\n        # Filter for high-performing competitors to create a meaningful benchmark\n        # This prevents low-quality posts from skewing the competitor average\n        median_exposure = competitor_posts['exposure_score'].median()\n        if pd.isna(median_exposure):\n            # Fallback if exposure scores are all NaN\n            high_perf_competitors = competitor_posts\n        else:\n            high_perf_competitors = competitor_posts[competitor_posts['exposure_score'] >= median_exposure]\n        \n        # If filtering results in an empty set, fall back to all competitors\n        if high_perf_competitors.empty:\n            high_perf_competitors = competitor_posts\n\n        avg_competitor_score = high_perf_competitors['actionability_score'].mean()\n        \n        if pd.notna(avg_competitor_score) and avg_competitor_score > 0:\n            our_scores = ours_posts['actionability_score']\n            # Add a small epsilon to prevent division by zero\n            ratio = our_scores / (avg_competitor_score + 1e-9)\n            # Assign the calculated ratio back to the original DataFrame using the index\n            df.loc[ours_posts.index, 'semantic_actionability_vs_competitors'] = ratio\n\n    return df",
  "analysis": {
    "correlation": 0.09703895366291311,
    "p_value": 0.3495265766042259,
    "interpretation": "약한 양의 상관관계(0.0970)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.3495)."
  }
}{
  "timestamp": "2025-07-22T05:23:22.134947",
  "attempt": 1,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'semantic_actionability_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a versatile multilingual model suitable for Korean content.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic actionability of 'our' posts relative to the competitor average for each query.\n    \n    The feature measures how strongly a post's text aligns with solution-oriented concepts \n    (e.g., 'how to', 'solution', 'guide') compared to the average competitor post for the same query.\n    \"\"\"\n    # 0. Handle empty DataFrame edge case\n    if df.empty:\n        df['semantic_actionability_vs_competitors'] = pd.Series(dtype=float)\n        return df\n\n    # 1. Initialize model and set a default neutral value for the feature column.\n    model = get_model()\n    # A default of 1.0 indicates parity with competitors or no basis for comparison.\n    df['semantic_actionability_vs_competitors'] = 1.0\n\n    # 2. Define the 'actionability' concept and prepare text data for embedding.\n    actionability_keywords = [\n        '방법', '해결', '순서', '팁', '가이드', '하는 법', '해결책',\n        'how to', 'step by step', 'solution', 'guide', 'tutorial', 'fix'\n    ]\n    \n    # Combine title and body, handling potential NaN values.\n    df_temp = df.copy() # Work on a copy to avoid modifying the original df until the final assignment\n    df_temp['text_for_embedding'] = df_temp['post_title'].fillna('') + ' ' + df_temp['post_body'].fillna('')\n\n    # 3. Generate embeddings for the concept and all posts in efficient batches.\n    # The concept's embedding is the average of its keyword embeddings.\n    concept_embedding = model.encode(actionability_keywords, convert_to_tensor=True).mean(dim=0)\n    post_embeddings = model.encode(df_temp['text_for_embedding'].tolist(), convert_to_tensor=True, show_progress_bar=False)\n\n    # 4. Calculate similarity of each post to the actionability concept.\n    all_similarities = util.cos_sim(post_embeddings, concept_embedding).flatten().cpu().numpy()\n    df_temp['actionability_score'] = all_similarities\n\n    # 5. Group by query to calculate the benchmark and the final ratio.\n    # Ensure the grouping key exists and is not all NaN.\n    if df_temp['representative_query'].notna().any():\n        # Calculate the average actionability score for competitors in each query group.\n        competitor_avg_scores = df_temp[df_temp['source'] == 'competitors'].groupby('representative_query')['actionability_score'].mean()\n        \n        # Map the competitor average back to each row based on its query.\n        df_temp['competitor_benchmark'] = df_temp['representative_query'].map(competitor_avg_scores)\n\n        # Isolate 'ours' posts to calculate their feature value.\n        ours_mask = df_temp['source'] == 'ours'\n        our_scores = df_temp.loc[ours_mask, 'actionability_score']\n        competitor_benchmarks = df_temp.loc[ours_mask, 'competitor_benchmark']\n\n        # Calculate the ratio. Handle division by zero or missing benchmarks.\n        # A missing benchmark (NaN) results in a neutral 1.0.\n        # A zero benchmark (np.inf) also results in a neutral 1.0, assuming our score is non-zero.\n        ratio = our_scores.divide(competitor_benchmarks).fillna(1.0)\n        ratio.replace([np.inf, -np.inf], 1.0, inplace=True)\n\n        # 6. Assign the calculated feature values to the original DataFrame, preserving its index.\n        df.loc[ours_mask, 'semantic_actionability_vs_competitors'] = ratio\n\n    return df"
}{
  "timestamp": "2025-07-22T05:24:29.236864",
  "attempt": 2,
  "status": "success",
  "feature_name": "relative_semantic_cohesion",
  "hypothesis": "콘텐츠의 제목과 본문 간의 의미론적 일관성(cohesion)은 주제에 대한 깊이와 전문성을 나타내는 지표입니다. 우리 포스트의 의미론적 일관성을 해당 대표 검색어의 '상위 성과 경쟁자' 그룹의 평균 일관성과 비교했을 때, 우리 포스트가 상대적으로 더 높은 일관성을 보일수록(즉, 제목과 본문이 더 긴밀하게 연결될수록) 주제에 대한 집중도가 높다고 판단할 수 있으며, 이는 사용자의 검색 의도를 더 잘 만족시켜 'non_brand_inflow'에 긍정적인 영향을 미칠 것입니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport sys\n\n# 1. Safe, lazy-loading model initialization\n_model = None\n\ndef get_model():\n    \"\"\"Lazily loads and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a widely recognized multilingual model suitable for Korean.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the relative semantic cohesion of a post compared to its high-performing competitors.\n\n    The feature measures the internal semantic alignment (title vs. body) of our post\n    and compares it to the average internal alignment of top competitors for the same query.\n    A higher value suggests our post is more thematically focused than the competition.\n    \"\"\"\n    feature_name = 'relative_semantic_cohesion'\n    df[feature_name] = np.nan\n\n    if df.empty:\n        return df\n\n    try:\n        model = get_model()\n    except Exception as e:\n        print(f\"Warning: Could not load SentenceTransformer model. Skipping feature '{feature_name}'. Error: {e}\", file=sys.stderr)\n        return df\n\n    results = {}\n\n    for query, group in df.groupby('representative_query'):\n        ours_posts = group[group['source'] == 'ours']\n        competitor_posts = group[group['source'] == 'competitor']\n\n        if ours_posts.empty or competitor_posts.empty:\n            continue\n\n        # LOGICAL ERROR CORRECTION: Use only HIGH-PERFORMING competitors for a meaningful benchmark.\n        # This prevents skew from low-quality content and ensures the benchmark represents excellence.\n        performance_threshold = competitor_posts['exposure_score'].quantile(0.75)\n        high_performing_competitors = competitor_posts[competitor_posts['exposure_score'] >= performance_threshold]\n\n        if high_performing_competitors.empty:\n            for idx in ours_posts.index:\n                results[idx] = 1.0\n            continue\n\n        # Efficiently process all relevant posts for the group in one batch.\n        combined_posts = pd.concat([ours_posts, high_performing_competitors])\n\n        titles = combined_posts['post_title'].fillna('').astype(str).tolist()\n        bodies = combined_posts['post_body'].fillna('').astype(str).tolist()\n\n        title_embeddings = model.encode(titles, convert_to_tensor=True, show_progress_bar=False)\n        body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n        # .diag() efficiently calculates the element-wise similarity (title_i vs body_i).\n        cohesion_scores = util.cos_sim(title_embeddings, body_embeddings).diag()\n\n        cohesion_map = pd.Series(cohesion_scores.cpu().numpy(), index=combined_posts.index)\n\n        our_cohesion_scores = cohesion_map.loc[ours_posts.index]\n        competitor_cohesion_scores = cohesion_map.loc[high_performing_competitors.index]\n\n        # Add a small epsilon to prevent division by zero.\n        benchmark_cohesion = competitor_cohesion_scores.mean() + 1e-9\n\n        # LOGICAL ERROR CORRECTION: This calculation produces a unique value per 'ours' post\n        # relative to its specific, high-performing competitor set, ensuring variance.\n        relative_cohesion = our_cohesion_scores / benchmark_cohesion\n\n        # LOGICAL ERROR CORRECTION: Store results in a dictionary keyed by the original index\n        # to prevent silent misalignment when assigning back to the DataFrame.\n        results.update(relative_cohesion.to_dict())\n\n    if results:\n        feature_series = pd.Series(results, name=feature_name)\n        df.update(feature_series)\n\n    # Fill any remaining NaNs (e.g., for groups with no competitors) with a neutral value of 1.0.\n    df[feature_name].fillna(1.0, inplace=True)\n\n    return df",
  "analysis": {
    "correlation": -0.018009830401595404,
    "p_value": 0.8624719574708121,
    "interpretation": "약한 음의 상관관계(-0.0180)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.8625)."
  }
}{
  "timestamp": "2025-07-22T05:44:38.871970",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_actionability_vs_competitors",
  "hypothesis": "우리 포스트의 본문이 경쟁사 포스트 그룹의 평균보다 '방법', '해결', '가이드'와 같은 실행 가능한 주제에 대해 의미적으로 더 가깝게 정렬될수록, 사용자의 문제 해결 의도를 더 잘 충족시켜 비브랜드 유입량이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a versatile multilingual model suitable for Korean.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a semantic actionability score for each post and benchmarks\n    'ours' posts against the competitor average for the same representative query.\n    \"\"\"\n    feature_name = 'semantic_actionability_vs_competitors'\n    df[feature_name] = np.nan\n\n    if df.empty:\n        return df\n\n    # Make a copy to safely add temporary columns\n    df_copy = df.copy()\n\n    try:\n        model = get_model()\n\n        # 2. Define a concept vector for 'actionability' or 'how-to' content.\n        actionability_keywords = [\n            \"방법\", \"해결\", \"순서\", \"가이드\", \"팁\", \"따라하기\", \"직접 해보기\",\n            \"how to\", \"단계별\", \"체크리스트\", \"프로세스\"\n        ]\n        actionability_embedding = model.encode(\n            actionability_keywords, convert_to_tensor=True\n        ).mean(dim=0)\n\n        # 3. Prepare body text for efficient batch processing.\n        bodies = df_copy['post_body'].fillna('').astype(str).tolist()\n        \n        # 4. Encode all post bodies in a single batch for efficiency.\n        body_embeddings = model.encode(\n            bodies, convert_to_tensor=True, show_progress_bar=False\n        )\n\n        # 5. Calculate similarity of each post to the 'actionability' concept.\n        # This results in a (N, 1) tensor, which we flatten.\n        actionability_scores = util.cos_sim(\n            body_embeddings, actionability_embedding\n        ).flatten().tolist()\n        \n        df_copy['temp_actionability_score'] = actionability_scores\n\n        # 6. Group by query and calculate the benchmark ratio.\n        grouped = df_copy.groupby('representative_query')\n\n        for name, group in grouped:\n            ours_post = group[group['source'] == 'ours']\n            competitor_posts = group[group['source'] == 'competitor']\n\n            # Proceed only if both our post and competitor posts exist for the query\n            if ours_post.empty or competitor_posts.empty:\n                continue\n\n            # Calculate the average actionability score for competitors\n            competitor_avg_score = competitor_posts['temp_actionability_score'].mean()\n\n            # Get our post's score\n            # .iloc[0] is safe because we assume one 'ours' post per query group\n            our_score = ours_post['temp_actionability_score'].iloc[0]\n\n            # Calculate the ratio. Handle division by zero or NaN denominator.\n            if pd.notna(competitor_avg_score) and competitor_avg_score > 0:\n                ratio = our_score / competitor_avg_score\n            else:\n                # Assign a neutral value or NaN if benchmark is invalid\n                ratio = 1.0 \n\n            # 7. Use the original index to assign the feature value back to the main DataFrame.\n            # This prevents any potential misalignment from sorting or grouping.\n            our_post_index = ours_post.index\n            df.loc[our_post_index, feature_name] = ratio\n\n    except Exception as e:\n        # In case of any error during processing, the column will have NaNs.\n        print(f\"Error generating {feature_name}: {e}\")\n        # Ensure the column exists even if the process fails midway.\n        if feature_name not in df.columns:\n            df[feature_name] = np.nan\n\n    return df\n",
  "analysis": {
    "correlation": -0.00019390504078362764,
    "p_value": 0.9985119999304111,
    "interpretation": "약한 음의 상관관계(-0.0002)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.9985)."
  }
}{
  "timestamp": "2025-07-22T06:04:40.663115",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_actionability_vs_competitors",
  "hypothesis": "우리 포스트의 내용(제목+본문)이 경쟁자 그룹의 평균적인 내용보다 '해결 방법', '단계별 가이드', '하는 법' 등 실용적이고 실행 가능한 정보를 나타내는 의미에 더 가까울수록, 사용자의 실질적인 문제 해결 의도를 더 잘 충족시켜 'non_brand_inflow'가 높을 것이라고 예상합니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # 한국어를 포함한 다국어 지원 모델 사용\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    각 'ours' 포스트에 대해, '실행 가능성'이라는 의미론적 벡터와의 유사도를 계산하고,\n    이를 해당 대표 검색어의 경쟁자 그룹 평균 유사도와 비교하여 비율을 계산합니다.\n    \"\"\"\n    feature_name = 'semantic_actionability_vs_competitors'\n    \n    if not df.empty:\n        try:\n            model = get_model()\n\n            # 1. '실행 가능성/문제 해결'을 대표하는 키워드 벡터 생성\n            actionability_keywords = [\n                \"해결 방법\", \"단계별 가이드\", \"하는 법\", \"팁\", \"전략\", \"쉽게 따라하기\",\n                \"자세히 알아보기\", \"원인과 해결\", \"예방 수칙\", \"치료 과정\", \"어떻게 하나요\", \"셀프 케어\"\n            ]\n            # 키워드 임베딩의 평균을 계산하여 '실행 가능성' 대표 벡터 생성\n            actionability_embedding = model.encode(actionability_keywords, convert_to_tensor=True).mean(axis=0)\n\n            # 2. 각 포스트의 텍스트를 일괄적으로 임베딩\n            # 제목과 본문을 합쳐 전체적인 의미를 파악\n            df['temp_text_for_semantic'] = df['post_title'].fillna('') + ' ' + df['post_body'].fillna('')\n            post_embeddings = model.encode(df['temp_text_for_semantic'].tolist(), convert_to_tensor=True, show_progress_bar=False)\n\n            # 3. 모든 포스트에 대해 '실행 가능성' 점수 계산\n            actionability_scores = util.cos_sim(post_embeddings, actionability_embedding).flatten().cpu().numpy()\n            df['temp_actionability_score'] = actionability_scores\n\n            # 4. 대표 검색어 그룹별로 'ours' 포스트의 상대적 실행 가능성 점수 계산\n            results = {}\n            # 원본 인덱스를 보존하며 그룹별 연산 수행\n            for name, group in df.groupby('representative_query'):\n                # 해당 그룹의 경쟁자 포스트 점수 추출\n                competitor_scores = group.loc[group['source'] == 'competitors', 'temp_actionability_score']\n                \n                # 경쟁자가 없거나 점수가 없는 경우를 대비한 기본값 설정\n                avg_competitor_score = competitor_scores.mean() if not competitor_scores.empty else np.nan\n\n                # 'ours' 포스트에 대해 순회하며 비율 계산\n                our_posts = group[group['source'] == 'ours']\n                for idx, row in our_posts.iterrows():\n                    our_score = row['temp_actionability_score']\n                    \n                    # 경쟁자 평균이 유효하지 않거나 0일 경우, 비율은 1(중립)로 처리하여 에러 방지\n                    if pd.isna(avg_competitor_score) or avg_competitor_score == 0:\n                        ratio = 1.0\n                    else:\n                        ratio = our_score / avg_competitor_score\n                    results[idx] = ratio # 원본 인덱스를 key로 결과 저장\n            \n            # 5. 계산된 결과를 원본 DataFrame에 정확히 매핑\n            df[feature_name] = pd.Series(results)\n\n            # 임시 컬럼 제거\n            df.drop(columns=['temp_text_for_semantic', 'temp_actionability_score'], inplace=True)\n\n        except Exception as e:\n            # 예외 발생 시 피처 컬럼을 NaN으로 채워넣음\n            print(f\"Error generating {feature_name}: {e}\")\n            df[feature_name] = np.nan\n    else:\n        df[feature_name] = np.nan\n\n    return df",
  "analysis": {
    "correlation": null,
    "p_value": null,
    "interpretation": "상관관계를 계산할 수 없습니다 (예: 피처의 분산이 0)."
  }
}{
  "timestamp": "2025-07-22T06:23:47.530751",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_actionability_vs_competitors",
  "hypothesis": "포스트 본문이 경쟁자 그룹의 평균보다 '해결책 제시', '방법 안내'와 같은 실행 가능한 정보를 의미적으로 더 많이 포함할수록, 사용자의 문제 해결 의도를 더 잘 충족시켜 비브랜드 유입량이 증가할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n_ideal_vector = None\n\ndef get_model_and_vector():\n    \"\"\"Initializes the model and computes the ideal actionability vector, memoizing both.\"\"\"\n    global _model, _ideal_vector\n    if _model is None:\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    \n    if _ideal_vector is None:\n        # Define sentences that represent the concept of 'actionability' and 'how-to'.\n        actionability_sentences = [\n            '문제를 해결하는 구체적인 방법', # A specific method for solving the problem\n            '단계별 실행 가이드', # Step-by-step execution guide\n            '이 문제를 해결하기 위한 절차는 다음과 같습니다', # The procedure to solve this problem is as follows\n            '다음 단계를 순서대로 따라 하세요', # Follow the next steps in order\n            '실용적인 해결책', # A practical solution\n            '빠르게 적용할 수 있는 팁', # Tips that can be applied quickly\n            '어떻게 해결해야 하나요?', # How can I solve this?\n            '자세한 해결 과정' # Detailed solution process\n        ]\n        # Encode these sentences and find their average semantic vector (centroid).\n        ideal_embeddings = _model.encode(actionability_sentences, convert_to_tensor=True)\n        _ideal_vector = torch.mean(ideal_embeddings, dim=0, keepdim=True)\n\n    return _model, _ideal_vector\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score representing how 'actionable' a post's body is compared to its competitors.\n\n    1. Defines an 'ideal actionability' vector by averaging embeddings of how-to phrases.\n    2. Calculates a semantic similarity score for each post against this ideal vector.\n    3. For each 'ours' post, computes the ratio of its score to the average score of its competitors\n       for the same representative query.\n    \"\"\"\n    # 0. Handle empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        if isinstance(df, pd.DataFrame):\n            df['semantic_actionability_vs_competitors'] = np.nan\n        return df\n\n    # 1. Get the model and the ideal actionability vector\n    model, ideal_vector = get_model_and_vector()\n\n    # 2. Prepare data for batch processing and calculate actionability scores\n    # Ensure we handle potential NaNs and different data types.\n    bodies = df['post_body'].fillna('').astype(str).tolist()\n    \n    # Efficiently encode all bodies in a single batch\n    body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # Calculate cosine similarity of each post body to the ideal 'actionability' vector\n    # The result is a (N, 1) tensor, which we flatten to a 1D list for the DataFrame column.\n    actionability_scores = util.cos_sim(body_embeddings, ideal_vector).flatten().cpu().numpy()\n    \n    # Use a temporary column for calculation. Preserves original DataFrame structure.\n    df_temp = df.copy()\n    df_temp['temp_actionability_score'] = actionability_scores\n\n    # 3. Calculate the benchmarked ratio per query group\n    # Initialize the target column with a default value (1.0 means 'same as competitors')\n    df['semantic_actionability_vs_competitors'] = 1.0\n    \n    # Group by the representative query to compare 'ours' vs 'competitor' posts\n    for query, group in df_temp.groupby('representative_query'):\n        competitor_posts = group[group['source'] == 'competitor']\n        our_posts = group[group['source'] == 'ours']\n\n        # Proceed only if both our posts and competitor posts exist for the query\n        if not competitor_posts.empty and not our_posts.empty:\n            # Calculate the average actionability score for competitors\n            competitor_avg_score = competitor_posts['temp_actionability_score'].mean()\n\n            # Avoid division by zero. If competitors have no actionability, ratio is undefined.\n            # We set it to 1, implying no relative advantage or disadvantage.\n            if competitor_avg_score > 1e-9:\n                # Calculate the ratio for each of our posts against the competitor average\n                our_scores = our_posts['temp_actionability_score']\n                ratio = our_scores / competitor_avg_score\n                \n                # **CRITICAL**: Assign the calculated ratio back to the original DataFrame\n                # using the correct index from the 'our_posts' slice.\n                df.loc[our_posts.index, 'semantic_actionability_vs_competitors'] = ratio\n            else:\n                # If competitor avg score is near zero, set our posts' ratio to 1.0\n                df.loc[our_posts.index, 'semantic_actionability_vs_competitors'] = 1.0\n    \n    # Fill any remaining NaNs that might have occurred from groups without competitors\n    df['semantic_actionability_vs_competitors'].fillna(1.0, inplace=True)\n\n    return df",
  "analysis": {
    "correlation": 0.08641639561352286,
    "p_value": 0.40501860120961436,
    "interpretation": "약한 양의 상관관계(0.0864)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.4050)."
  }
}{
  "timestamp": "2025-07-22T06:46:08.034203",
  "attempt": 1,
  "status": "success",
  "feature_name": "semantic_superiority_vs_top_competitors",
  "hypothesis": "우리 포스트의 본문 내용이 동일 대표 검색어에 대한 '상위 성과 경쟁자' 그룹의 평균적인 의미 중심(semantic centroid)에 가까울수록(즉, 코사인 유사도가 높을수록) 비브랜드 유입 성과가 높을 것이다. 이는 성공적인 경쟁자들의 핵심 주제 및 소구 포인트를 우리 포스트가 효과적으로 반영하고 있음을 의미하기 때문이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns the SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a well-regarded Korean-centric model\n        _model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic similarity of our posts to the centroid of \n    top-performing competitor posts for the same representative query.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with post data.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the new feature column added.\n    \"\"\"\n    # 2. Handle empty or invalid input DataFrame\n    if df.empty or 'post_body' not in df.columns or 'source' not in df.columns or 'representative_query' not in df.columns:\n        df['semantic_superiority_vs_top_competitors'] = np.nan\n        return df\n\n    model = get_model()\n    feature_name = 'semantic_superiority_vs_top_competitors'\n    df[feature_name] = np.nan # Initialize the feature column\n\n    # 3. Preserve original index for safe assignment later\n    # This is critical to prevent data misalignment.\n    df_processed = df.copy()\n    df_processed['original_index'] = df.index\n    df_processed = df_processed.dropna(subset=['post_body', 'representative_query'])\n\n    if df_processed.empty:\n        return df\n\n    # 4. Efficiently encode all relevant post bodies in a single batch\n    # We get integer locations for precise tensor slicing later.\n    body_texts = df_processed['post_body'].fillna('').astype(str).tolist()\n    with torch.no_grad():\n        all_embeddings = model.encode(body_texts, convert_to_tensor=True, show_progress_bar=False)\n\n    # Map original df index to the tensor's row index for quick lookup\n    # This mapping is essential for correctly linking DataFrame rows to their embeddings.\n    index_to_tensor_pos = {idx: i for i, idx in enumerate(df_processed['original_index'])}\n\n    # 5. Group by query and calculate the feature for each group\n    for query, group in df_processed.groupby('representative_query'):\n        ours_df = group[group['source'] == 'ours']\n        competitors_df = group[group['source'] == 'competitor']\n\n        # Skip if there's nothing to compare\n        if ours_df.empty or competitors_df.empty:\n            continue\n\n        # 6. Define 'top-performing' competitors to create a meaningful benchmark\n        # Using median inflow as the threshold for 'top performance'.\n        # This avoids skewing the benchmark with low-quality content.\n        median_inflow = competitors_df['non_brand_inflow'].median()\n        if pd.isna(median_inflow) or median_inflow == 0:\n             # Fallback if inflow data is sparse or all zero, use all competitors\n             top_competitors_df = competitors_df\n        else:\n             top_competitors_df = competitors_df[competitors_df['non_brand_inflow'] >= median_inflow]\n        \n        # If filtering results in an empty set, fall back to all competitors in the group\n        if top_competitors_df.empty:\n            top_competitors_df = competitors_df\n\n        # 7. Get integer positions for tensor slicing to calculate centroid\n        # This correctly uses integer positions, not DataFrame labels, for the tensor.\n        competitor_indices = [index_to_tensor_pos[idx] for idx in top_competitors_df['original_index'] if idx in index_to_tensor_pos]\n        \n        if not competitor_indices:\n            continue\n\n        competitor_embeddings = all_embeddings[competitor_indices]\n        centroid = torch.mean(competitor_embeddings, dim=0, keepdim=True)\n\n        # 8. Calculate similarity for our posts against the competitor centroid\n        our_indices = [index_to_tensor_pos[idx] for idx in ours_df['original_index'] if idx in index_to_tensor_pos]\n\n        if not our_indices:\n            continue\n\n        our_embeddings = all_embeddings[our_indices]\n\n        # Calculate cosine similarity. The result is a tensor of shape (num_our_posts, 1)\n        similarities = util.cos_sim(our_embeddings, centroid).squeeze().cpu().tolist()\n        \n        # 9. Safely assign the calculated feature values back to the original DataFrame\n        # Use the original index of 'our' posts to ensure correct assignment.\n        original_our_indices = ours_df['original_index']\n        df.loc[original_our_indices, feature_name] = similarities\n\n    return df\n",
  "analysis": {
    "correlation": 0.10787440781995641,
    "p_value": 0.2980841324929682,
    "interpretation": "약한 양의 상관관계(0.1079)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.2981)."
  }
}{
  "timestamp": "2025-07-22T07:07:17.847324",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_semantic_actionability_score",
  "hypothesis": "우리 포스트의 내용(제목+본문)이 '방법', '해결', '가이드'와 같은 실행 가능한 정보를 담고 있는 의미론적 개념에 가까울수록, 경쟁 포스트 그룹 대비 더 높은 비브랜드 유입을 유도할 것입니다. 이 피처는 우리 포스트의 '실용성 점수'를 경쟁사 평균 '실용성 점수'로 나눈 값으로, 1보다 클수록 경쟁 우위를 나타냅니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model that is effective for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a score representing how 'actionable' our content is compared to competitors.\n\n    The score is the ratio of our post's semantic similarity to an 'actionability' concept\n    versus the average competitor's similarity for the same query.\n    \"\"\"\n    feature_name = \"relative_semantic_actionability_score\"\n    \n    # 2. Handle empty DataFrame and initialize the feature column with a neutral value.\n    if df.empty:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n    df[feature_name] = 1.0\n\n    model = get_model()\n\n    # 3. Define the semantic concept of 'actionability' in Korean.\n    # Using a single descriptive string is often more robust than averaging word embeddings.\n    actionability_concept_text = \"방법 해결 가이드 팁 단계별 노하우 하는 법 문제 해결 과정 따라하기\"\n    actionability_embedding = model.encode(actionability_concept_text, convert_to_tensor=True)\n\n    # 4. Ensure text columns are string type to prevent errors during concatenation.\n    df['post_title'] = df['post_title'].astype(str).fillna('')\n    df['post_body'] = df['post_body'].astype(str).fillna('')\n\n    # 5. Store results before assigning to the main DataFrame to ensure correct alignment.\n    # This avoids potential SettingWithCopyWarning and maintains index integrity.\n    results = {}\n\n    # 6. Group by query to create a benchmark for each topic.\n    for query, group_df in df.groupby('representative_query'):\n        ours_posts = group_df[group_df['source'] == 'ours']\n        competitor_posts = group_df[group_df['source'] == 'competitor']\n\n        # Skip groups where a comparison isn't possible.\n        if ours_posts.empty or competitor_posts.empty:\n            continue\n\n        # 7. Create combined text for encoding and process the entire group in one batch.\n        group_texts = (group_df['post_title'] + \" \" + group_df['post_body']).tolist()\n        group_embeddings = model.encode(group_texts, convert_to_tensor=True, show_progress_bar=False)\n        \n        # 8. Calculate the 'actionability score' for every post in the group.\n        # This is the cosine similarity between each post and the actionability concept.\n        actionability_scores = util.cos_sim(group_embeddings, actionability_embedding).flatten()\n        \n        # Use a temporary Series with the group's index for easy slicing.\n        group_scores = pd.Series(actionability_scores.cpu().numpy(), index=group_df.index)\n        \n        # 9. Calculate the average competitor score.\n        competitor_indices = competitor_posts.index\n        avg_competitor_score = group_scores.loc[competitor_indices].mean()\n\n        # Avoid division by zero. If competitors have no actionability, any score from our post is infinitely better.\n        # A small epsilon is used for stability, but we cap the result later.\n        if pd.isna(avg_competitor_score) or avg_competitor_score == 0:\n            avg_competitor_score = 1e-9\n\n        # 10. Calculate the relative score for each of 'our' posts.\n        for idx, row in ours_posts.iterrows():\n            our_score = group_scores.loc[idx]\n            relative_score = our_score / avg_competitor_score\n            # Store result with its original index.\n            results[idx] = relative_score\n\n    # 11. Safely assign the calculated scores back to the original DataFrame.\n    if results:\n        score_series = pd.Series(results)\n        df.loc[score_series.index, feature_name] = score_series\n\n    # Cap the feature value to prevent extreme outliers from dominating the analysis.\n    df[feature_name] = df[feature_name].clip(upper=5)\n\n    return df",
  "analysis": {
    "correlation": 0.017430143614630664,
    "p_value": 0.8668572772165799,
    "interpretation": "약한 양의 상관관계(0.0174)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.8669)."
  }
}{
  "timestamp": "2025-07-22T08:14:16.326008",
  "attempt": 1,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'semantic_practicality_score_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a SentenceTransformer model using a global singleton pattern.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model that is effective for Korean.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a semantic practicality score for each post and compares 'ours' posts\n    to the competitor average for the same representative query.\n\n    The practicality score is the maximum semantic similarity of a post's body to a \n    predefined list of practical/solution-oriented keywords.\n    \"\"\"\n    # Handle empty DataFrame gracefully.\n    if df.empty:\n        df['semantic_practicality_score_vs_competitors'] = pd.Series(dtype=float)\n        return df\n\n    feature_name = 'semantic_practicality_score_vs_competitors'\n    df[feature_name] = 1.0  # Default to a neutral value of 1.0\n\n    try:\n        model = get_model()\n    except Exception as e:\n        # If the model fails to load, return the DataFrame with the default column.\n        # This prevents crashes in environments without model caching/internet.\n        print(f\"Error loading SentenceTransformer model: {e}\")\n        return df\n\n    # Define keywords representing practicality and solutions.\n    PRACTICALITY_KEYWORDS = [\n        '해결 방법', '팁', '가이드', '자세히 알아보기', '단계별 설명', \n        '문제 해결', '꿀팁', '원인과 해결책', 'how to', '튜토리얼'\n    ]\n\n    # Encode keywords and post bodies in efficient batches.\n    keyword_embeddings = model.encode(PRACTICALITY_KEYWORDS, convert_to_tensor=True)\n    \n    # Ensure post_body is string type and handle missing values.\n    post_bodies = df['post_body'].fillna('').astype(str).tolist()\n    body_embeddings = model.encode(post_bodies, convert_to_tensor=True, show_progress_bar=False)\n\n    # Calculate cosine similarity between each body and all keywords.\n    cos_sim = util.cos_sim(body_embeddings, keyword_embeddings)\n\n    # For each post, the practicality score is its max similarity to any keyword.\n    if cos_sim.shape[0] > 0:\n        practicality_scores = cos_sim.max(axis=1).values.cpu().numpy()\n    else:\n        practicality_scores = np.array([])\n\n    # Add a temporary column for the raw practicality score.\n    # Check for length mismatch before assignment.\n    if len(practicality_scores) == len(df):\n        df['temp_practicality_score'] = practicality_scores\n    else:\n        # In case of an unexpected error, fill with a neutral value and proceed.\n        df['temp_practicality_score'] = 0.5 \n\n    # Group by query to compare 'ours' vs 'competitors'.\n    # Preserving the original index is crucial for correct assignment.\n    grouped = df.groupby('representative_query')\n\n    for _, group in grouped:\n        ours_df = group[group['source'] == 'ours']\n        competitors_df = group[group['source'] == 'competitors']\n\n        # Proceed only if there are both 'ours' and 'competitors' posts to compare.\n        if ours_df.empty or competitors_df.empty:\n            continue\n\n        # Calculate the average practicality score for the competitor group.\n        competitor_avg_score = competitors_df['temp_practicality_score'].mean()\n\n        # Avoid division by zero or by a negligible value.\n        if competitor_avg_score > 1e-6:\n            # Calculate the relative score for each 'ours' post.\n            # This creates variance if 'ours' posts have different scores.\n            relative_scores = ours_df['temp_practicality_score'] / competitor_avg_score\n            \n            # Safely assign the calculated values back to the original DataFrame using the index.\n            df.loc[ours_df.index, feature_name] = relative_scores\n        # If competitors have no practicality, 'ours' posts are left at the default 1.0.\n\n    # Clean up the temporary column.\n    df.drop(columns=['temp_practicality_score'], inplace=True)\n\n    return df"
}{
  "timestamp": "2025-07-22T08:15:43.559845",
  "attempt": 2,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'title_body_focus_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport sys\n\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a reliable multilingual model suitable for Korean text.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a 'topic focus score' (semantic similarity between title and body) for each post,\n    and then computes a relative score by comparing it to the average focus score of the competitor group\n    for the same representative query.\n\n    This feature quantifies how focused a post is on its topic compared to its competitors, addressing\n    the previous issue of generating a constant value by ensuring variance across different posts and query groups.\n    \"\"\"\n    feature_name = 'title_body_focus_vs_competitors'\n    \n    if df.empty:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n\n    # 1. Load the model using the lazy-loading pattern.\n    try:\n        model = get_model()\n    except Exception as e:\n        # If model loading fails for any reason, return a default column.\n        df[feature_name] = 1.0\n        return df\n\n    # 2. Prepare data for batch processing, ensuring no NaN values.\n    titles = df['post_title'].fillna('').astype(str).tolist()\n    bodies = df['post_body'].fillna('').astype(str).tolist()\n\n    # 3. Generate embeddings for all titles and bodies in efficient batches.\n    try:\n        title_embeddings = model.encode(titles, convert_to_tensor=True, show_progress_bar=False)\n        body_embeddings = model.encode(bodies, convert_to_tensor=True, show_progress_bar=False)\n    except Exception as e:\n        # In case of an encoding error, gracefully exit with a default value.\n        df[feature_name] = 1.0\n        return df\n\n    # 4. Calculate the cosine similarity for each post (title vs. body).\n    # .diag() extracts the diagonal elements, corresponding to sim(A_i, B_i) for each post i.\n    focus_scores = util.cos_sim(title_embeddings, body_embeddings).diag()\n    df['temp_focus_score'] = focus_scores.cpu().numpy()\n\n    # 5. Calculate the competitor benchmark for each query group.\n    competitors = df[df['source'] == 'competitors']\n    \n    if not competitors.empty:\n        # Calculate the mean focus score per query, only using competitor data.\n        competitor_avg_focus = competitors.groupby('representative_query')['temp_focus_score'].mean()\n        \n        # Map the calculated group average back to the original dataframe.\n        # Posts in a query group without competitors will get NaN here.\n        df['competitor_benchmark'] = df['representative_query'].map(competitor_avg_focus)\n    else:\n        # If there are no competitors in the entire dataframe, initialize the benchmark column with NaNs.\n        df['competitor_benchmark'] = np.nan\n\n    # 6. Handle posts in groups that had no competitors (benchmark is NaN).\n    # The fallback is to use the post's own score, which will result in a neutral ratio of 1.0.\n    df['competitor_benchmark'].fillna(df['temp_focus_score'], inplace=True)\n\n    # 7. Prevent division by zero.\n    # If the benchmark is 0 (e.g., all competitor titles/bodies were empty), replace with a tiny number.\n    df['competitor_benchmark'].replace(0, 1e-9, inplace=True)\n\n    # 8. Calculate the final feature: the ratio of the post's focus to the competitor average.\n    df[feature_name] = df['temp_focus_score'] / df['competitor_benchmark']\n\n    # 9. Clean up temporary columns.\n    df.drop(columns=['temp_focus_score', 'competitor_benchmark'], inplace=True)\n\n    # 10. Final cleanup: Replace any potential inf/NaN values (e.g., from 0/0) with a neutral 1.0.\n    df[feature_name].replace([np.inf, -np.inf], np.nan, inplace=True)\n    df[feature_name].fillna(1.0, inplace=True)\n\n    return df\n"
}{
  "timestamp": "2025-07-22T08:17:39.882050",
  "attempt": 3,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'semantic_alignment_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Lazy-loads and caches the sentence transformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Use a well-regarded multilingual model suitable for Korean.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic alignment of each 'our' post with its representative query,\n    relative to the average alignment of competitor posts for the same query.\n\n    The core logic is to generate a score that varies for each individual 'our' post,\n    thus avoiding the previous error of creating a constant value per query group.\n\n    Hypothesis: The more semantically aligned our post's title and body are with the\n    representative query's topic, compared to the competitor group's average alignment,\n    the higher the non-brand inflow will be.\n    \"\"\"\n    feature_name = 'semantic_alignment_vs_competitors'\n    # Initialize with a default neutral value.\n    df[feature_name] = 1.0\n\n    if df.empty:\n        return df\n\n    # The feature requires both 'ours' and 'competitors' data to compute a benchmark.\n    if 'ours' not in df['source'].unique() or 'competitors' not in df['source'].unique():\n        return df\n\n    # Use a temporary copy to avoid SettingWithCopyWarning\n    temp_df = df.copy()\n\n    try:\n        model = get_model()\n\n        # --- Step 1: Prepare data and embeddings in batches for efficiency --- #\n        temp_df['post_title_cleaned'] = temp_df['post_title'].fillna('').astype(str)\n        temp_df['post_body_cleaned'] = temp_df['post_body'].fillna('').astype(str)\n        temp_df['query_cleaned'] = temp_df['representative_query'].fillna('').astype(str)\n\n        # Encode all unique queries to create a reusable map.\n        unique_queries = temp_df['query_cleaned'].unique()\n        query_embeddings = model.encode(unique_queries, convert_to_tensor=True, show_progress_bar=False)\n        query_embedding_map = {query: emb for query, emb in zip(unique_queries, query_embeddings)}\n\n        # Encode all titles and bodies in a single batch.\n        title_embeddings = model.encode(temp_df['post_title_cleaned'].tolist(), convert_to_tensor=True, show_progress_bar=False)\n        body_embeddings = model.encode(temp_df['post_body_cleaned'].tolist(), convert_to_tensor=True, show_progress_bar=False)\n\n        # --- Step 2: Calculate individual alignment score for EVERY post --- #\n        # Map each row's query string to its pre-computed embedding tensor.\n        device = title_embeddings.device\n        df_query_embeddings_list = [query_embedding_map[q] for q in temp_df['query_cleaned']]\n        \n        if not df_query_embeddings_list:\n             return df # Safeguard for edge cases\n\n        df_query_embeddings = torch.stack(df_query_embeddings_list).to(device)\n\n        # Calculate cosine similarity for all posts against their respective queries.\n        # .diag() efficiently selects the one-to-one comparisons from the similarity matrix.\n        title_sim = util.cos_sim(title_embeddings, df_query_embeddings).diag()\n        body_sim = util.cos_sim(body_embeddings, df_query_embeddings).diag()\n\n        # The alignment score is the average of title and body similarity to the query.\n        temp_df['alignment_score'] = (title_sim.cpu().numpy() + body_sim.cpu().numpy()) / 2\n\n        # --- Step 3: Calculate competitor benchmark and the final feature --- #\n        # The benchmark is the average alignment score of competitors for each query.\n        competitor_benchmarks = temp_df[temp_df['source'] == 'competitors'].groupby('representative_query')['alignment_score'].mean()\n        \n        if competitor_benchmarks.empty:\n            return df # No competitor data to create benchmarks.\n\n        # Map the benchmarks to each 'our' post based on its query.\n        ours_indices = temp_df.index[temp_df['source'] == 'ours']\n        our_queries = temp_df.loc[ours_indices, 'representative_query']\n        our_benchmarks = our_queries.map(competitor_benchmarks)\n        \n        # Get the individual alignment score for each 'our' post.\n        our_scores = temp_df.loc[ours_indices, 'alignment_score']\n\n        # Calculate the relative score. This will vary for each 'our' post, solving the constant value problem.\n        relative_scores = our_scores / our_benchmarks\n\n        # Handle cases where a query has 'our' posts but no competitors (benchmark is NaN)\n        # or where benchmark is 0 (division -> inf). Replace these with the neutral value 1.0.\n        relative_scores.replace([np.inf, -np.inf], np.nan, inplace=True)\n        relative_scores.fillna(1.0, inplace=True)\n\n        # Assign the calculated variable scores to the feature column in the original DataFrame.\n        df.loc[ours_indices, feature_name] = relative_scores\n\n    except Exception:\n        # In case of any error, return the original dataframe with default values.\n        return df\n\n    return df"
}{
  "timestamp": "2025-07-22T08:19:04.293237",
  "attempt": 4,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'solution_oriented_score_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a SentenceTransformer model, loading it only once.\"\"\"\n    global _model\n    if _model is None:\n        # Using a reliable multilingual model suitable for Korean\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates a 'solution-oriented' score for each post and compares it to the competitor average.\n\n    The score measures how much closer a post's semantics are to 'solution' keywords than to 'problem' keywords.\n    This individual post score is then normalized by the average score of competitors within the same query group.\n    This approach ensures variance by generating a unique value for each 'our' post based on its specific content.\n    \"\"\"\n    feature_name = 'solution_oriented_score_vs_competitors'\n    # Initialize with a neutral default value of 1.0.\n    df[feature_name] = 1.0\n\n    if df.empty:\n        return df\n\n    # Ensure required columns exist\n    if 'source' not in df.columns or 'representative_query' not in df.columns or 'post_body' not in df.columns:\n        return df\n\n    # Make copies to avoid SettingWithCopyWarning\n    ours_df = df[df['source'] == 'ours'].copy()\n    competitors_df = df[df['source'] == 'competitors'].copy()\n\n    # If there are no 'ours' posts to calculate for, or no competitors to compare against, return with defaults.\n    if ours_df.empty or competitors_df.empty:\n        return df\n\n    try:\n        model = get_model()\n\n        # 1. Define semantic poles (Problem vs. Solution) to create a semantic axis.\n        problem_keywords = ['통증', '증상', '문제', '깨짐', '부러짐', '충치', '아픔', '질환', '불편']\n        solution_keywords = ['치료', '방법', '해결', '과정', '수복', '개선', '회복', '시술', '진료']\n\n        # 2. Encode keywords and create pole vectors by averaging their embeddings.\n        problem_embeddings = model.encode(problem_keywords, convert_to_tensor=True)\n        solution_embeddings = model.encode(solution_keywords, convert_to_tensor=True)\n        problem_vector = util.normalize_embeddings(problem_embeddings.mean(dim=0).unsqueeze(0))\n        solution_vector = util.normalize_embeddings(solution_embeddings.mean(dim=0).unsqueeze(0))\n\n        # 3. Batch encode all post bodies for efficiency.\n        all_texts = df['post_body'].fillna('').astype(str).tolist()\n        all_embeddings = model.encode(all_texts, convert_to_tensor=True, show_progress_bar=False)\n        all_embeddings = util.normalize_embeddings(all_embeddings)\n\n        # 4. Calculate \"solution focus score\" for every post.\n        # .squeeze() converts the (N, 1) tensor to a 1D tensor of shape (N,)\n        sim_to_solution = util.cos_sim(all_embeddings, solution_vector).squeeze()\n        sim_to_problem = util.cos_sim(all_embeddings, problem_vector).squeeze()\n\n        # The score projects the post onto the 'problem-to-solution' axis.\n        # A small constant is added to shift scores away from zero, preventing division issues.\n        df['solution_focus_score'] = (sim_to_solution - sim_to_problem).cpu().numpy() + 1.0\n\n        # 5. Calculate the competitor benchmark (mean score) for each query group.\n        competitor_benchmarks = df[df['source'] == 'competitors'].groupby('representative_query')['solution_focus_score'].mean()\n\n        # 6. Map the benchmark to each 'ours' post based on its query.\n        # This creates the denominator for our relative score calculation.\n        # Posts in query groups without competitors will have NaN here.\n        ours_indices = df.index[df['source'] == 'ours']\n        competitor_avg_scores = df.loc[ours_indices, 'representative_query'].map(competitor_benchmarks)\n\n        # 7. Calculate the final feature value for 'ours' posts.\n        # The feature is the ratio of an individual post's score to the competitor average for that query.\n        # This design generates variance between 'our' posts, even within the same query group.\n        our_scores = df.loc[ours_indices, 'solution_focus_score']\n        relative_scores = our_scores / competitor_avg_scores\n\n        # 8. Handle edge cases: replace inf (division by ~0) and NaN (no competitors) with the neutral default 1.0.\n        relative_scores.replace([np.inf, -np.inf], np.nan, inplace=True)\n        relative_scores.fillna(1.0, inplace=True)\n\n        # 9. Assign the calculated scores back to the original DataFrame using .loc for safe assignment.\n        df.loc[ours_indices, feature_name] = relative_scores\n\n        # Clean up temporary column\n        df.drop(columns=['solution_focus_score'], inplace=True)\n\n    except Exception as e:\n        # A robust error handling block to ensure the pipeline doesn't crash.\n        print(f\"An error occurred in feature 'solution_oriented_score_vs_competitors': {e}\")\n        # Return the DataFrame with default values already set.\n        return df\n\n    return df"
}{
  "timestamp": "2025-07-22T08:20:45.312362",
  "attempt": 5,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'semantic_richness_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\n\ndef parse_morphemes(morpheme_str):\n    \"\"\"Helper function to parse a comma-separated morpheme string into a set.\"\"\"\n    if not isinstance(morpheme_str, str) or not morpheme_str.strip():\n        return set()\n    # Split by comma and space, and strip extra whitespace from each morpheme\n    return set(m.strip() for m in morpheme_str.split(',') if m.strip())\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the semantic richness of 'our' posts relative to competitors for the same query.\n\n    Hypothesis:\n    The more our posts cover a broader range of related concepts (based on morphemes)\n    than the average competitor post for the same query, i.e., the higher the 'semantic richness',\n    the higher the non-brand inflow will be. This is because it increases the likelihood that\n    the post will be perceived as more comprehensive and expert.\n\n    Method:\n    1. For each 'representative_query' group, define a 'competitor semantic field' by aggregating all unique morphemes from competitor posts.\n    2. For each post in the group, calculate a 'semantic_coverage_score' (Jaccard similarity) between its morphemes and the 'competitor semantic field'. This measures how well the post covers the topics discussed by competitors.\n    3. For each 'ours' post, calculate the final feature by dividing its 'semantic_coverage_score' by the average score of its competitors. This creates a relative richness score.\n    4. A value of 1.0 is used as a default for 'ours' posts in groups without competitors and for all 'competitor' posts.\n    \"\"\"\n    feature_name = 'semantic_richness_vs_competitors'\n    \n    if 'morpheme_words' not in df.columns or df.empty:\n        df[feature_name] = 1.0\n        return df\n\n    # Initialize the feature column with a default neutral value.\n    df[feature_name] = 1.0\n\n    # Create a copy to avoid SettingWithCopyWarning\n    df_copy = df.copy()\n\n    # Ensure 'morpheme_words' is string type and handle NaNs\n    df_copy['morpheme_words'] = df_copy['morpheme_words'].fillna('').astype(str)\n\n    # Pre-parse all morphemes into sets for efficiency\n    df_copy['morpheme_sets'] = df_copy['morpheme_words'].apply(parse_morphemes)\n\n    # Group by representative query\n    grouped = df_copy.groupby('representative_query')\n\n    all_group_scores = []\n\n    for name, group in grouped:\n        ours_posts_mask = group['source'] == 'ours'\n        competitor_posts_mask = group['source'] == 'competitors'\n        \n        ours_posts = group[ours_posts_mask]\n        competitor_posts = group[competitor_posts_mask]\n\n        # If no competitors or no 'ours' posts in the group, we can't compare.\n        # The default value of 1.0 for this group's posts remains.\n        if competitor_posts.empty or ours_posts.empty:\n            all_group_scores.append(group[[feature_name]])\n            continue\n\n        # 1. Define the 'competitor semantic field' for this query group\n        competitor_morphemes = set().union(*competitor_posts['morpheme_sets'])\n        \n        # If competitors have no morphemes, also skip comparison.\n        if not competitor_morphemes:\n            all_group_scores.append(group[[feature_name]])\n            continue\n\n        # 2. Calculate 'semantic_coverage_score' for every post in the group\n        def calculate_jaccard(post_morphemes):\n            intersection_size = len(post_morphemes.intersection(competitor_morphemes))\n            union_size = len(post_morphemes.union(competitor_morphemes))\n            return intersection_size / union_size if union_size > 0 else 0.0\n\n        group['semantic_coverage_score'] = group['morpheme_sets'].apply(calculate_jaccard)\n\n        # 3. Calculate the average score for competitors\n        competitor_avg_score = group.loc[competitor_posts_mask, 'semantic_coverage_score'].mean()\n        \n        group_feature_col = pd.Series(1.0, index=group.index, name=feature_name)\n        \n        # Handle case where competitor average score is zero\n        if competitor_avg_score > 0:\n            # Calculate the relative score for each 'ours' post individually\n            # This ensures variance if 'ours' posts have different morpheme sets\n            our_scores = group.loc[ours_posts_mask, 'semantic_coverage_score']\n            relative_scores = our_scores / competitor_avg_score\n            group_feature_col.loc[ours_posts_mask] = relative_scores\n\n        all_group_scores.append(group_feature_col.to_frame())\n\n    # Concatenate results from all groups and assign back to the original DataFrame\n    if all_group_scores:\n        final_scores = pd.concat(all_group_scores)\n        df[feature_name] = final_scores\n\n    return df"
}{
  "timestamp": "2025-07-22T08:23:34.611112",
  "attempt": 6,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'semantic_diversity_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport re\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a SentenceTransformer model using a global singleton pattern.\"\"\"\n    global _model\n    if _model is None:\n        # Using a well-regarded Korean-capable multilingual model\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef calculate_semantic_diversity(text: str, model: SentenceTransformer) -> float:\n    \"\"\"\n    Calculates the semantic diversity of a text.\n    Diversity is measured as the average cosine distance of sentence embeddings from their centroid.\n    A higher value indicates greater semantic spread.\n    \"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return 0.0\n\n    # Split text into sentences. A simple split on periods and newlines is robust enough.\n    sentences = [s.strip() for s in re.split(r'[.\\n]', text) if s.strip()]\n\n    # If there are fewer than 2 sentences, diversity cannot be measured meaningfully.\n    if len(sentences) < 2:\n        return 0.0\n\n    try:\n        # Encode sentences into embeddings\n        embeddings = model.encode(sentences, convert_to_tensor=True)\n\n        # Calculate the centroid (mean of all sentence embeddings)\n        centroid = torch.mean(embeddings, dim=0)\n\n        # Calculate cosine similarity of each sentence to the centroid\n        similarities = util.cos_sim(embeddings, centroid.unsqueeze(0))\n\n        # Convert similarities to distances (1 - similarity) and calculate the mean.\n        # The average distance represents the semantic diversity.\n        avg_distance = torch.mean(1 - similarities).item()\n\n        return avg_distance\n\n    except Exception:\n        # Catch any unexpected errors during encoding or calculation\n        return 0.0\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generates the 'semantic_diversity_vs_competitors' feature.\n\n    This feature compares the semantic diversity of an 'ours' post against the average\n    semantic diversity of competitor posts for the same representative query.\n    \"\"\"\n    # Ensure torch is imported for tensor operations within the function\n    import torch\n\n    if df.empty:\n        df['semantic_diversity_vs_competitors'] = 1.0\n        return df\n\n    model = get_model()\n\n    # Calculate the semantic diversity score for every post in the dataframe\n    df['semantic_diversity_score'] = df['post_body'].apply(lambda x: calculate_semantic_diversity(x, model))\n\n    # Separate 'ours' and 'competitor' data\n    ours_df = df[df['source'] == 'ours'].copy()\n    competitors_df = df[df['source'] == 'competitors'].copy()\n\n    # Initialize the feature column with a default value of 1.0\n    df['semantic_diversity_vs_competitors'] = 1.0\n\n    # --- CRITICAL FIX for Constant Value Error ---\n    # If there are no competitors, we cannot create a benchmark. Return default.\n    if competitors_df.empty:\n        return df\n\n    # Calculate the average diversity for each query group among competitors\n    competitor_query_avg = competitors_df.groupby('representative_query')['semantic_diversity_score'].mean()\n\n    # Create a mapping from query to competitor average diversity\n    competitor_avg_map = competitor_query_avg.to_dict()\n\n    # Calculate a global average competitor diversity as a fallback for queries with no competitors\n    global_competitor_avg = competitors_df['semantic_diversity_score'].mean()\n    if pd.isna(global_competitor_avg) or global_competitor_avg == 0:\n        global_competitor_avg = 1.0 # Ultimate fallback to prevent division by zero\n\n    # --- Logic to generate varied feature values for 'ours' posts ---\n    # Get the diversity score for our posts\n    our_scores = ours_df['semantic_diversity_score']\n\n    # Map the competitor average to our posts based on their query.\n    # Use the global average as a fallback if a specific query has no competitors.\n    competitor_benchmarks = ours_df['representative_query'].map(competitor_avg_map).fillna(global_competitor_avg)\n\n    # Avoid division by zero for benchmarks\n    competitor_benchmarks[competitor_benchmarks == 0] = global_competitor_avg\n\n    # Calculate the relative diversity score\n    # This operation is performed element-wise and preserves variance\n    relative_scores = our_scores / competitor_benchmarks\n\n    # Update the main dataframe for 'ours' posts\n    df.loc[df['source'] == 'ours', 'semantic_diversity_vs_competitors'] = relative_scores\n    \n    # Fill any potential NaNs that might have slipped through with the default\n    df['semantic_diversity_vs_competitors'].fillna(1.0, inplace=True)\n\n    # Clean up temporary column\n    df.drop(columns=['semantic_diversity_score'], inplace=True)\n\n    return df"
}{
  "timestamp": "2025-07-22T08:25:18.198069",
  "attempt": 7,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'semantic_distance_from_ideal'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 1. 지연 로딩을 사용하여 모델 재초기화를 방지합니다.\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        # 널리 사용되는 다국어 모델을 사용합니다.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    각 '우리' 포스트의 의미론적 거리를 경쟁자들의 '이상적인' 의미 중심(centroid)으로부터 계산합니다.\n\n    가설: 특정 검색어에 대해, 이상적인 경쟁자 벤치마크에 의미적으로 더 가까운(즉, 유사도가 높은) 우리 포스트가\n    더 높은 비브랜드 유입을 가질 것이다. 이상적인 벤치마크는 해당 검색어에 대한 모든 경쟁자 포스트의\n    평균 의미 벡터(centroid)로 정의됩니다.\n\n    Args:\n        df (pd.DataFrame): 포스트 데이터가 포함된 입력 데이터프레임.\n\n    Returns:\n        pd.DataFrame: 새로운 피처 열 'semantic_distance_from_ideal'이 추가된 데이터프레임.\n    \"\"\"\n    feature_name = 'semantic_distance_from_ideal'\n\n    # 피처 열을 중립적인 기본값(0.0)으로 초기화합니다.\n    # 이는 경쟁사 행 및 계산할 수 없는 '우리' 행을 처리합니다.\n    df[feature_name] = 0.0\n\n    if df.empty:\n        return df\n\n    try:\n        model = get_model()\n    except Exception as e:\n        # 모델 로드에 실패하면 피처를 생성할 수 없습니다.\n        # 기본 열 값이 있는 df를 반환합니다.\n        # print(f\"Error loading SentenceTransformer model: {e}\") # In production, this might be logged.\n        return df\n\n    # 재현성 및 다른 장치에서의 오류 방지를 위해 device 설정\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n\n    # 더 풍부한 의미 표현을 위해 제목과 본문을 결합합니다.\n    df['text_for_embedding'] = df['post_title'].fillna('') + ' ' + df['post_body'].fillna('')\n\n    # 결과를 여기에 저장합니다: {original_index: score}\n    # 이는 정렬 불일치 문제를 방지하는 데 매우 중요합니다.\n    scores = {}\n\n    # 동일한 검색 컨텍스트 내에서 비교를 수행하기 위해 쿼리별로 그룹화합니다.\n    if 'representative_query' in df.columns:\n        for query, group in df.groupby('representative_query'):\n            # 나중에 올바르게 할당하기 위해 원본 인덱스를 보존합니다.\n            ours_posts = group[group['source'] == 'ours']\n            competitor_posts = group[group['source'] == 'competitors']\n\n            # 중요 확인: 점수를 계산하려면 우리 포스트와 경쟁사 포스트가 모두 필요합니다.\n            if ours_posts.empty or competitor_posts.empty:\n                continue  # 이 그룹에는 기본값 0.0이 사용됩니다.\n\n            # 효율성을 위해 텍스트를 배치로 인코딩합니다.\n            our_texts = ours_posts['text_for_embedding'].tolist()\n            competitor_texts = competitor_posts['text_for_embedding'].tolist()\n\n            # 임베딩 생성\n            our_embeddings = model.encode(our_texts, convert_to_tensor=True, device=device)\n            competitor_embeddings = model.encode(competitor_texts, convert_to_tensor=True, device=device)\n\n            # 경쟁자들의 '이상적인' 의미 중심(centroid)을 계산합니다.\n            # .mean(dim=0)을 사용하여 모든 경쟁자 임베딩에 대한 평균을 구합니다.\n            competitor_centroid = torch.mean(competitor_embeddings, dim=0)\n\n            # 각 우리 포스트와 경쟁자 중심 간의 코사인 유사도를 계산합니다.\n            # 결과는 (num_our_posts, 1) 모양의 텐서입니다.\n            cosine_scores = util.cos_sim(our_embeddings, competitor_centroid)\n\n            # 점수를 원래 데이터프레임 인덱스에 다시 매핑합니다.\n            # .flatten().cpu().numpy()는 텐서를 1D numpy 배열로 변환합니다.\n            # ours_posts.index는 점수를 올바른 행에 일치시키는 것을 보장합니다.\n            for idx, score in zip(ours_posts.index, cosine_scores.flatten().cpu().numpy()):\n                scores[idx] = score\n\n    # 계산된 점수를 사전을 사용하여 데이터프레임에 할당합니다.\n    # 이는 인덱스를 기반으로 값을 안전하게 업데이트하는 방법입니다.\n    if scores:\n        # Ensure we only try to locate indices that exist in the dataframe\n        valid_indices = [idx for idx in scores.keys() if idx in df.index]\n        if valid_indices:\n            df.loc[valid_indices, feature_name] = pd.Series(scores)[valid_indices].values\n\n    # 임시 열을 정리합니다.\n    df = df.drop(columns=['text_for_embedding'])\n\n    return df"
}{
  "timestamp": "2025-07-22T08:26:43.183967",
  "attempt": 8,
  "status": "logical_error",
  "error": "논리적 오류: 생성된 피처 'solution_oriented_score_vs_competitors'의 값이 모두 동일하여 상관관계를 계산할 수 없습니다. 모든 행에 대해 다른 값을 생성하도록 코드를 수정해주세요.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a model that is effective for Korean and general semantic tasks.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates how 'solution-oriented' our post is compared to competitors for the same query.\n\n    The feature is calculated as follows:\n    1. A 'solution concept' vector is created by averaging the embeddings of practical, solution-oriented phrases.\n    2. Each post's body is scored based on its semantic similarity to this solution concept.\n    3. For each 'ours' post, its score is divided by the average score of all competitor posts for the same representative query.\n    4. This creates a relative score indicating how much more solution-focused our content is.\n\n    Args:\n        df (pd.DataFrame): The input dataframe with post data.\n\n    Returns:\n        pd.DataFrame: The dataframe with the new 'solution_oriented_score_vs_competitors' feature column.\n    \"\"\"\n    # 2. Check for empty DataFrame to prevent errors.\n    if df.empty:\n        df['solution_oriented_score_vs_competitors'] = pd.Series(dtype=float)\n        return df\n\n    feature_name = 'solution_oriented_score_vs_competitors'\n    df[feature_name] = 1.0  # Default value for posts with no competitors or other issues.\n\n    try:\n        model = get_model()\n\n        # 3. Define solution-oriented phrases and create a target semantic vector.\n        solution_phrases = [\n            \"해결 방법\", \"치료 과정\", \"개선 방법\", \"어떻게 해결하나요\",\n            \"진료 과정\", \"단계별 설명\", \"문제를 해결하는 법\", \"대처 방안\"\n        ]\n        solution_embeddings = model.encode(solution_phrases, convert_to_tensor=True)\n        solution_vector = torch.mean(solution_embeddings, dim=0, keepdim=True)\n\n        # 4. Efficiently process all post bodies in a single batch.\n        post_bodies = df['post_body'].fillna('').tolist()\n        body_embeddings = model.encode(post_bodies, convert_to_tensor=True)\n\n        # 5. Calculate the solution-oriented score for every post.\n        # The result is a (num_posts x 1) tensor, so we flatten it.\n        similarities = util.cos_sim(body_embeddings, solution_vector)\n        df['temp_solution_score'] = similarities.cpu().numpy().flatten()\n\n        # 6. Group by query to calculate relative scores.\n        # This ensures each 'ours' post is compared only against competitors for the same query.\n        for query, group in df.groupby('representative_query'):\n            ours_posts = group[group['source'] == 'ours']\n            competitor_posts = group[group['source'] == 'competitors']\n\n            if not ours_posts.empty and not competitor_posts.empty:\n                # Calculate the benchmark: the average score of competitors in this group.\n                competitor_benchmark_score = competitor_posts['temp_solution_score'].mean()\n\n                if competitor_benchmark_score > 0:\n                    # Get the individual scores for our posts.\n                    our_scores = ours_posts['temp_solution_score']\n                    # Calculate the relative score for each of our posts.\n                    relative_scores = our_scores / competitor_benchmark_score\n                    \n                    # 7. CRITICAL: Assign the calculated individual scores back to the original DataFrame using the correct index.\n                    # This correctly handles multiple 'ours' posts in a group and prevents the constant value error.\n                    df.loc[ours_posts.index, feature_name] = relative_scores\n\n        # 8. Clean up the temporary column.\n        df.drop(columns=['temp_solution_score'], inplace=True)\n\n    except Exception as e:\n        # In case of any error (e.g., model loading), ensure the column exists with default values.\n        print(f\"An error occurred during feature generation: {e}\")\n        if feature_name not in df.columns:\n            df[feature_name] = 1.0\n        # Fill any potential NaNs that might have arisen from failed calculations.\n        df[feature_name].fillna(1.0, inplace=True)\n\n    return df\n"
}{
  "timestamp": "2025-07-22T08:28:09.486104",
  "attempt": 9,
  "status": "success",
  "feature_name": "solution_oriented_score_vs_competitors",
  "hypothesis": "우리 포스트 본문이 '해결 방법', '가이드', '팁'과 같은 실용적/해결 지향적 주제 벡터에 의미론적으로 더 가까울수록, 해당 검색어의 경쟁사 그룹 평균보다 더 높은 비브랜드 유입(non_brand_inflow)을 유도할 것이다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# 1. 모델을 전역 변수로 선언하여 불필요한 재초기화를 방지합니다.\n_model = None\n\ndef get_model():\n    \"\"\"SentenceTransformer 모델을 초기화하고 반환합니다.\"\"\"\n    global _model\n    if _model is None:\n        # 한국어 처리에 적합한 다국어 모델을 사용합니다.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    우리 포스트의 '해결 지향성'을 경쟁사와 비교하는 피처를 생성합니다.\n\n    'solution_oriented_score_vs_competitors' 피처는 각 포스트가 가진 실용적, \n    해결책 중심의 의미론적 특성을 동일 검색어에 대한 경쟁사 평균과 비교하여 측정합니다.\n    이전 시도에서 발생한 상수 값 오류를 해결하기 위해, 벤치마크가 없는 경우에 대한\n    강력한 대체 로직을 구현하여 항상 분산이 있는 유의미한 값을 생성하도록 수정했습니다.\n    \"\"\"\n    feature_name = 'solution_oriented_score_vs_competitors'\n    # 2. 피처 열을 기본값 1.0으로 초기화합니다. 이는 비교 불가 또는 차이 없음을 의미합니다.\n    df[feature_name] = 1.0\n\n    # 3. 데이터프레임이 비어 있거나 필수 열이 없는 경우, 그대로 반환합니다.\n    if df.empty or any(col not in df.columns for col in ['post_body', 'source', 'representative_query']):\n        return df\n\n    try:\n        model = get_model()\n    except Exception as e:\n        # 모델 로딩 실패 시 (예: 오프라인 환경), 기본값으로 채워진 df를 반환합니다.\n        print(f\"SentenceTransformer 모델 로딩 오류: {e}\")\n        return df\n\n    # 4. '해결 지향성'을 정의하는 핵심 키워드 목록을 정의합니다.\n    solution_keywords = [\n        \"해결 방법\", \"가이드\", \"팁\", \"단계별 설명\", \"원인과 해결\",\n        \"하는 법\", \"자세히 알아보기\", \"솔루션\", \"대처법\", \"매뉴얼\"\n    ]\n    # 키워드 임베딩의 평균으로 '해결 지향성' 대표 벡터를 생성합니다.\n    solution_vector = model.encode(solution_keywords, convert_to_tensor=True).mean(axis=0)\n\n    # 5. 모든 포스트 본문을 효율적인 배치 방식으로 인코딩합니다.\n    post_bodies = df['post_body'].fillna('').astype(str).tolist()\n    post_embeddings = model.encode(post_bodies, convert_to_tensor=True)\n\n    # 6. 각 포스트와 '해결 지향성' 벡터 간의 코사인 유사도를 계산하여 '실용성 점수'를 부여합니다.\n    practicality_scores = util.cos_sim(post_embeddings, solution_vector).squeeze().cpu().numpy()\n    df['practicality_score'] = practicality_scores\n\n    # 7. 'ours'와 'competitors' 데이터를 분리합니다.\n    ours_df = df[df['source'] == 'ours'].copy()\n    competitors_df = df[df['source'] == 'competitors'].copy()\n\n    # 'ours' 포스트가 없으면 계산할 필요가 없습니다.\n    if ours_df.empty:\n        df.drop(columns=['practicality_score'], inplace=True, errors='ignore')\n        return df\n\n    # 8. 벤치마크 점수를 계산합니다.\n    #    - 1순위: 검색어별 경쟁사 평균 점수\n    #    - 2순위: 전체 경쟁사 평균 점수 (해당 검색어에 경쟁사가 없을 경우)\n    #    - 3순위: 'ours' 포스트 간의 상대 비교 (경쟁사 데이터가 전혀 없을 경우)\n    if not competitors_df.empty:\n        benchmark_per_query = competitors_df.groupby('representative_query')['practicality_score'].mean()\n        global_benchmark = competitors_df['practicality_score'].mean()\n        \n        # 분모가 0이 되는 것을 방지합니다.\n        if global_benchmark == 0 or np.isnan(global_benchmark):\n            global_benchmark = 1.0\n\n        # 'ours' 포스트에 벤치마크 점수를 매핑합니다. 매칭되는 쿼리가 없으면 전역 벤치마크를 사용합니다.\n        ours_benchmarks = ours_df['representative_query'].map(benchmark_per_query).fillna(global_benchmark)\n    else:\n        # 데이터셋에 경쟁사가 전혀 없는 경우, 'ours' 포스트들을 서로 비교하여 벤치마크를 생성합니다.\n        # 이는 피처 값이 상수가 되는 것을 방지하는 핵심 로직입니다.\n        if len(ours_df) > 1:\n            total_sum = ours_df['practicality_score'].sum()\n            # 각 포스트의 벤치마크 = (전체 합 - 자기 점수) / (전체 개수 - 1)\n            ours_benchmarks = ours_df.apply(\n                lambda row: (total_sum - row['practicality_score']) / (len(ours_df) - 1), axis=1\n            )\n        else:\n            # 'ours' 포스트가 하나만 있고 경쟁사가 없으면, 상대 점수는 1.0이 됩니다.\n            ours_benchmarks = pd.Series([1.0], index=ours_df.index)\n\n    # 9. 0으로 나누기 오류를 방지합니다. 벤치마크가 0이면 1.0으로 대체하여 극단적인 값 출현을 막습니다.\n    ours_benchmarks.replace(0, 1.0, inplace=True)\n    \n    # 10. 'ours' 포스트에 대한 최종 상대 점수를 계산합니다.\n    relative_scores = ours_df['practicality_score'] / ours_benchmarks\n    \n    # 11. 계산된 상대 점수를 원본 데이터프레임의 해당 위치에 업데이트합니다.\n    df.loc[df['source'] == 'ours', feature_name] = relative_scores\n    \n    # 12. 임시로 사용한 점수 열을 삭제합니다.\n    df.drop(columns=['practicality_score'], inplace=True, errors='ignore')\n\n    return df\n",
  "analysis": {
    "correlation": 0.0020435955285167144,
    "p_value": 0.9843187190417606,
    "interpretation": "약한 양의 상관관계(0.0020)를 발견했습니다. 하지만 통계적으로 유의미하지 않습니다(p-value: 0.9843)."
  }
}{
  "timestamp": "2025-07-22T10:18:17.984012",
  "attempt": 1,
  "status": "success",
  "feature_name": "relative_semantic_richness",
  "hypothesis": "콘텐츠의 의미론적 풍부함(semantic richness)은 주제에 대한 포괄적인 이해와 깊이를 나타냅니다. 본문 내 문장들 간의 평균적인 의미론적 거리(average pairwise semantic distance)를 풍부함의 척도로 삼을 때, 우리 포스트의 풍부함이 상위 경쟁자 그룹의 평균 풍부함보다 높을수록, 더 깊이 있고 포괄적인 정보를 제공하는 것으로 인식되어 'non_brand_inflow'에 긍정적인 영향을 미칠 것입니다.",
  "code": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\nimport re\n\n# 1. Lazy-load the model to avoid re-initializing it on every call.\n_model = None\n\ndef get_model():\n    \"\"\"Initializes and returns a singleton SentenceTransformer model.\"\"\"\n    global _model\n    if _model is None:\n        # Using a multilingual model suitable for Korean text.\n        _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    return _model\n\ndef _calculate_richness(text: str, model: SentenceTransformer) -> float:\n    \"\"\"\n    Calculates the semantic richness of a given text.\n    Richness is defined as 1 minus the average pairwise cosine similarity of its sentences.\n    A higher value (closer to 1) indicates more semantically diverse sentences.\n    A lower value (closer to 0) indicates sentences are very similar to each other.\n\n    Args:\n        text: The input string (post_body).\n        model: The SentenceTransformer model instance.\n\n    Returns:\n        A float representing the semantic richness score.\n    \"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return 0.0\n\n    # Split text into sentences using a regex that handles common terminators.\n    sentences = [s.strip() for s in re.split(r'[.!?]\\s+', text) if s.strip()]\n    if len(sentences) < 2:\n        return 0.0\n\n    try:\n        # Encode all sentences into embeddings.\n        embeddings = model.encode(sentences, convert_to_tensor=True)\n        \n        # Calculate the pairwise cosine similarity matrix.\n        cos_sim_matrix = util.cos_sim(embeddings, embeddings)\n        \n        # To get unique pairwise similarities, we take the upper triangle of the matrix,\n        # excluding the diagonal (which is always 1.0).\n        upper_triangle_indices = torch.triu(torch.ones_like(cos_sim_matrix), diagonal=1).bool()\n        \n        # If there are no pairs (e.g., only one unique sentence), return 0.\n        if not upper_triangle_indices.any():\n            return 0.0\n            \n        pairwise_sims = cos_sim_matrix[upper_triangle_indices]\n        \n        # Calculate the average similarity.\n        mean_similarity = torch.mean(pairwise_sims).item()\n        \n        # Richness is the average semantic distance (1 - similarity).\n        richness = 1.0 - mean_similarity\n        \n        return richness if not np.isnan(richness) else 0.0\n\n    except Exception:\n        # In case of any unexpected errors during embedding or calculation.\n        return 0.0\n\ndef generate_feature(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculates the relative semantic richness of 'ours' posts compared to top competitors.\n    \n    The feature is the ratio of our post's richness to the average richness of top-performing\n    (top 25% by exposure_score) competitor posts for the same representative query.\n    A value > 1 suggests our content is semantically richer than the benchmark.\n    A value < 1 suggests it is less rich.\n    \"\"\"\n    feature_name = 'relative_semantic_richness'\n\n    if df.empty:\n        df[feature_name] = pd.Series(dtype=float)\n        return df\n\n    model = get_model()\n\n    # Create a working copy to avoid SettingWithCopyWarning.\n    df_copy = df.copy()\n\n    # 1. Calculate the richness score for every post in the dataframe.\n    # This is more efficient than calculating it repeatedly inside a loop.\n    df_copy['semantic_richness'] = df_copy['post_body'].fillna('').apply(\n        lambda x: _calculate_richness(x, model)\n    )\n\n    # 2. Calculate a global fallback benchmark.\n    # This is crucial for handling query groups that have no competitor posts,\n    # preventing the feature from becoming a constant 1.0 in sparse data scenarios.\n    all_competitors = df_copy[df_copy['source'] == 'competitors']\n    if not all_competitors.empty and all_competitors['semantic_richness'].notna().any():\n        fallback_benchmark = all_competitors['semantic_richness'].mean()\n    else:\n        # If there are no competitors at all in the dataset, no comparison is possible.\n        # The ratio defaults to 1, but this will apply to all 'ours' posts.\n        fallback_benchmark = 1.0\n\n    # 3. Group by query and calculate the relative score for 'ours' posts.\n    results = pd.Series(index=df.index, dtype=float)\n    if 'representative_query' in df_copy.columns:\n        for query, group in df_copy.groupby('representative_query'):\n            ours_group = group[group['source'] == 'ours']\n            competitors_group = group[group['source'] == 'competitors']\n\n            if ours_group.empty:\n                continue\n\n            benchmark_richness = 0.0\n            if not competitors_group.empty:\n                # Define top competitors as those in the 75th percentile of exposure_score.\n                try:\n                    threshold = competitors_group['exposure_score'].quantile(0.75)\n                    top_competitors = competitors_group[competitors_group['exposure_score'] >= threshold]\n                    if not top_competitors.empty:\n                        benchmark_richness = top_competitors['semantic_richness'].mean()\n                except Exception:\n                    # Fallback in case quantile calculation fails.\n                    benchmark_richness = competitors_group['semantic_richness'].mean()\n            \n            # If no competitors were found in the group or they had 0 richness, use the global fallback.\n            if benchmark_richness <= 0:\n                benchmark_richness = fallback_benchmark\n\n            # Avoid division by zero. If fallback is also 0, the score will be NaN and filled later.\n            if benchmark_richness > 0:\n                our_richness_scores = ours_group['semantic_richness']\n                relative_scores = our_richness_scores / benchmark_richness\n                results.update(relative_scores)\n\n    # 4. Assign the calculated scores back to the original DataFrame.\n    df[feature_name] = results\n    # Competitor posts get a neutral score of 1.0.\n    df.loc[df['source'] == 'competitors', feature_name] = 1.0\n    # Any 'ours' post that failed calculation (is NaN) also gets a neutral 1.0.\n    df[feature_name].fillna(1.0, inplace=True)\n\n    return df",
  "analysis": {
    "correlation": -0.23556418473599197,
    "p_value": 0.02155944437304108,
    "interpretation": "약한 음의 상관관계(-0.2356)를 발견했습니다. 이 결과는 통계적으로 유의미합니다(p-value: 0.0216)."
  }
}